{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd53f5e-8059-4338-bbb4-88abc6202208",
   "metadata": {},
   "source": [
    "## Reft + Vision (VLBart) experiments\n",
    "\n",
    "Does ReFT works with Vision-language? Let's find out with the VQA Task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3002ccd0-1095-4a01-8395-70f059d425c6",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb906f7-b8ba-42ee-a114-bc1dee0378ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer_base import TrainerBase\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.multiprocessing as mp\n",
    "import os\n",
    "import collections\n",
    "from pathlib import Path\n",
    "from packaging import version\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "from copy import deepcopy\n",
    "\n",
    "from param import parse_args\n",
    "\n",
    "from utils import LossMeter\n",
    "from dist_utils import reduce_dict\n",
    "import wandb\n",
    "\n",
    "proj_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "\n",
    "\n",
    "_use_native_amp = False\n",
    "_use_apex = False\n",
    "\n",
    "# Check if Pytorch version >= 1.6 to switch between Native AMP and Apex\n",
    "if version.parse(torch.__version__) < version.parse(\"1.6\"):\n",
    "    from transormers.file_utils import is_apex_available\n",
    "    if is_apex_available():\n",
    "        from apex import amp\n",
    "    _use_apex = True\n",
    "else:\n",
    "    _use_native_amp = True\n",
    "    from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27b0012c-e0e5-47b1-8f0f-1d0c7debf32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from transformers.models.bart.modeling_bart import (\n",
    "    BartConfig,\n",
    "    ACT2FN,\n",
    "    shift_tokens_right, _make_causal_mask, _expand_mask\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple\n",
    "import copy\n",
    "\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a9fd4-0059-422a-8d8d-511cf821a8fd",
   "metadata": {},
   "source": [
    "### 0. Prerequisites\n",
    "- Install Pyvene (peterwz-llava branch) and Pyreft with Python 3.8. Main branch of Pyvene does not work, because to support transformers 4.39, it fails to support the Bart Models used by this notebook.\n",
    "- Download all the training datasets using the following command:\n",
    "    ```\n",
    "    gdrive download 1O_RU1iFh_sbItZCTkOHUrbVIQQ_89Djj\n",
    "    ```\n",
    "    Here is the [google drive link](https://drive.google.com/file/d/1O_RU1iFh_sbItZCTkOHUrbVIQQ_89Djj/view), we suggest you use [gdrive](https://github.com/prasmussen/gdrive) to download it. Put the `datasets` folder directly under `image_video_text_understanding`.\n",
    "- Download the image annotations as well by running `wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip`. Unzip and put the files in `image_video_text_understanding/datasets/COCO/images/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0089a652-7948-4cea-94ff-6e74b974ac2f",
   "metadata": {},
   "source": [
    "### 1. Reft Data (intervention locations)\n",
    "\n",
    "Reft needs to populate `intervention_locations` together with the text input IDs and image embeddings. Here we calcluate the intervention locations.\n",
    "\n",
    "The function `get_image_intervention_locations()` creates separate interventions for texts and image inputs. As a reminder, Vision-language Bart concatenates these location tokens together. However, image tokens are not visible to the VLBart model at the time of input (only text `input_ids` are visible). So the image interventions start at the end of `input_ids` and last for `args.n_boxes` long (image features are of length `args.n_boxes`). The number of image and text interventions match the number of interventions defined in the `ReftConfig` in section 1.3.1. We intervene on all encoder layers, and Bart encoder has 6 layers, so there will be 6 text and 6 image interventions if we share weights between the first and the last tokens. If we do not share weights, there will be 12 text and 12 image interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e99be92-5b36-451f-947f-a4b6f67b184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import torch\n",
    "\n",
    "def parse_positions(positions: str):\n",
    "    # parse position\n",
    "    first_n, last_n = 0, 0\n",
    "    if \"+\" in positions:\n",
    "        first_n = int(positions.split(\"+\")[0].strip(\"f\"))\n",
    "        last_n = int(positions.split(\"+\")[1].strip(\"l\"))\n",
    "    else:\n",
    "        if \"f\" in positions:\n",
    "            first_n = int(positions.strip(\"f\"))\n",
    "        elif \"l\" in positions:\n",
    "            last_n = int(positions.strip(\"l\"))\n",
    "    return first_n, last_n\n",
    "\n",
    "def get_image_intervention_locations(**kwargs):\n",
    "    \"\"\"\n",
    "    This function generates separate intervention locations for images and texts.\n",
    "    \"\"\"\n",
    "    # parse kwargs\n",
    "    share_weights = kwargs[\"share_weights\"] if \"share_weights\" in kwargs else False\n",
    "    last_text_position = kwargs[\"last_position\"]\n",
    "    assert \"image_positions\" in kwargs, \"Image positions must be provided\"\n",
    "    assert \"positions\" in kwargs, \"Text positions must be provided\"\n",
    "    first_n, last_n = parse_positions(kwargs[\"positions\"])\n",
    "    first_image_n, last_image_n = parse_positions(kwargs[\"image_positions\"])\n",
    "\n",
    "    num_interventions = kwargs[\"num_interventions\"]\n",
    "    # `last_offset` is the length of the images (n_boxes).\n",
    "    # Image tokens are concatenated to the end of the text tokens, i.e. after `last_position` tokens.\n",
    "    # The true last position of the input is `last_position + last_offset`\n",
    "    image_offset = kwargs[\"last_offset\"] if \"last_offset\" in kwargs else 0\n",
    "\n",
    "    pad_mode = kwargs[\"pad_mode\"] if \"pad_mode\" in kwargs else \"first\"\n",
    "    pad_position = -1 if pad_mode == \"first\" else last_text_position + image_offset\n",
    "    if pad_mode != \"first\" and \"nlvr\" in kwargs[\"tasks\"]:\n",
    "        pad_position = last_text_position + 2 * image_offset\n",
    "\n",
    "    if share_weights or ((first_n == 0 or last_n == 0) and (first_image_n == 0 or last_image_n == 0)):\n",
    "        position_list = [i for i in range(first_n)] + \\\n",
    "            [i for i in range(last_text_position - last_n, last_text_position)]\n",
    "        image_position_list = [i for i in range(last_text_position, last_text_position + first_image_n)] + \\\n",
    "            [i for i in range(last_text_position + image_offset - last_image_n, last_text_position + image_offset)]\n",
    "        # There are 2 images in nlvr, so performing special treatment\n",
    "        # For this notebook however, we only use vqa\n",
    "        if \"nlvr\" in kwargs[\"tasks\"]:\n",
    "            image_position_list += [i for i in range(last_text_position + image_offset, last_text_position + image_offset + first_image_n)] + \\\n",
    "            [i for i in range(last_text_position + 2 * image_offset - last_image_n, last_text_position + 2 * image_offset)]\n",
    "        text_len = len(position_list)\n",
    "        image_len = len(image_position_list)\n",
    "        if text_len > image_len:\n",
    "            image_position_list += [pad_position for _ in range(text_len-image_len)]\n",
    "        else:\n",
    "            position_list += [pad_position for _ in range(image_len-text_len)]\n",
    "        intervention_locations = [position_list]*(num_interventions//2) + \\\n",
    "            [image_position_list]*(num_interventions//2)\n",
    "    else:\n",
    "        assert first_n == last_n, \"For now, we only support same first and last positions\"\n",
    "        left_intervention_locations = [i for i in range(first_n)]\n",
    "        right_intervention_locations = [i for i in range(last_text_position - last_n, last_text_position)]\n",
    "        left_image_intervention_locations = [i for i in range(last_text_position, last_text_position + first_image_n)]\n",
    "        right_image_intervention_locations = [i for i in range(last_text_position + image_offset - last_image_n, last_text_position + image_offset)]\n",
    "        if \"nlvr\" in kwargs[\"tasks\"]:\n",
    "            left_image_intervention_locations += [i for i in range(last_text_position + image_offset, last_text_position + image_offset + first_image_n)]\n",
    "            right_image_intervention_locations += [i for i in range(last_text_position + 2 * image_offset - last_image_n, last_text_position + 2 * image_offset)]\n",
    "        text_len = len(left_intervention_locations)\n",
    "        image_len = len(left_image_intervention_locations)\n",
    "        if text_len > image_len:\n",
    "            left_image_intervention_locations += [pad_position for _ in range(text_len-image_len)]\n",
    "            right_image_intervention_locations += [pad_position for _ in range(text_len-image_len)]\n",
    "        else:\n",
    "            left_intervention_locations += [pad_position for _ in range(image_len-text_len)]\n",
    "            right_intervention_locations += [pad_position for _ in range(image_len-text_len)]\n",
    "\n",
    "        intervention_locations = [left_intervention_locations]*(num_interventions//4) + \\\n",
    "            [right_intervention_locations]*(num_interventions//4) + \\\n",
    "            [left_image_intervention_locations]*(num_interventions//4) + \\\n",
    "            [right_image_intervention_locations]*(num_interventions//4)\n",
    "    return intervention_locations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1626ef3-000d-48d9-b9f6-c7f5958fda7e",
   "metadata": {},
   "source": [
    "Here we also process intervention padding. To collate multiple interventions of different lengths, we create padding interventions that only intervene on padded locations. So these interventions do not impact Reft output. We only use `pad_mode = first`, so you can see that (1) a `1` is prepended to the input IDs at position 0 (2) padding interventions intervene on the position 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94da9380-74b5-4eb4-af3a-67bcd86e4464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intervention(\n",
    "    id: int, \n",
    "    result: dict, \n",
    "    tokenizer,\n",
    "    fields_to_pad = [],\n",
    "    fields_to_mask = [],\n",
    "    **kwargs):\n",
    "    pad_mode = kwargs[\"pad_mode\"]\n",
    "    # compute intervention locs\n",
    "    assert \"positions\" in kwargs and \"image_positions\" in kwargs\n",
    "    intervention_locations = get_image_intervention_locations(**kwargs)\n",
    "    result[\"intervention_locations\"] = intervention_locations\n",
    "    result[\"id\"] = id\n",
    "\n",
    "    # add a single padding token BEFORE input_ids and fix everything\n",
    "    if fields_to_pad is not None:\n",
    "        if pad_mode == \"first\":\n",
    "            for field in fields_to_pad:\n",
    "                if field not in result:\n",
    "                    continue\n",
    "                if field == \"labels\":\n",
    "                    result[field] = torch.cat((torch.tensor([IGNORE_INDEX,]), result[field]))\n",
    "                else:\n",
    "                    result[field] = torch.cat((torch.tensor([tokenizer.pad_token_id,]), result[field]))\n",
    "            result[\"intervention_locations\"] = (torch.IntTensor(result[\"intervention_locations\"]) + 1).tolist()\n",
    "            result[\"input_length\"] += 1\n",
    "        elif pad_mode == \"last\":\n",
    "            for field in fields_to_pad:\n",
    "                if field not in result:\n",
    "                    continue\n",
    "                if field == \"labels\":\n",
    "                    result[field] = torch.cat((result[field], torch.tensor([IGNORE_INDEX,])))\n",
    "                else:\n",
    "                    result[field] = torch.cat((result[field], torch.tensor([tokenizer.pad_token_id,])))\n",
    "            result[\"input_length\"] += 1\n",
    "        \n",
    "    # attention masks\n",
    "    if len(fields_to_mask) == 1:\n",
    "        result[\"attention_mask\"] = (result[fields_to_mask[0]] != tokenizer.pad_token_id).int()\n",
    "    else:\n",
    "        for field in fields_to_mask:\n",
    "            result[f\"{field}_mask\"] = (result[field] != tokenizer.pad_token_id).int()\n",
    "\n",
    "    # does not handle subspaces for now\n",
    "    # print(\"Intervention Locations\", result[\"intervention_locations\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18de0d1b-7bac-4cb1-99ad-3ee470158d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reft_post_process(\n",
    "    out_dict,\n",
    "    tokenizer,\n",
    "    idx: int, \n",
    "    last_position: int, \n",
    "    args = None,\n",
    "    pad_mode = \"none\",\n",
    "    fields_to_pad = [],\n",
    "    fields_to_mask = []\n",
    "):\n",
    "    # print(\"Out_dict keys:\", out_dict.keys())\n",
    "    out_dict[\"instruction\"] = tokenizer.decode(\n",
    "        out_dict[\"input_ids\"], skip_special_tokens=True)\n",
    "    kwargs = {}\n",
    "    if args is not None:\n",
    "        if args.reft_rank != -1:\n",
    "            kwargs[\"positions\"] = args.positions\n",
    "        if args.reft_image_rank != -1:\n",
    "            kwargs[\"image_positions\"] = args.image_positions\n",
    "        kwargs[\"share_weights\"] = args.share_weights\n",
    "        layers = [int(l) for l in args.layers.split(\";\")]\n",
    "        kwargs[\"num_interventions\"] = len(layers) if args.share_weights else 2 * len(layers)\n",
    "        # Double interventions if creating separate interventions for texts and images\n",
    "        if args.reft_image_rank != -1 and args.reft_rank != -1:\n",
    "            kwargs[\"num_interventions\"] *= 2\n",
    "        # `n_boxes` is the seq length of the image embeddings\n",
    "        kwargs[\"last_offset\"] = args.n_boxes\n",
    "        # Only tested `first` \n",
    "        kwargs[\"pad_mode\"] = pad_mode\n",
    "        kwargs[\"last_position\"] = last_position\n",
    "        kwargs[\"tasks\"] = args.prompt\n",
    "    # print(kwargs)\n",
    "\n",
    "    # print(\"BEFORE:\", out_dict[\"input_ids\"].shape, kwargs[\"last_position\"])\n",
    "    tokenized = compute_intervention(\n",
    "            idx, \n",
    "            out_dict, \n",
    "            tokenizer,\n",
    "            fields_to_pad,\n",
    "            fields_to_mask,\n",
    "            **kwargs)\n",
    "    # print(\"AFTER:\", tokenized[\"input_ids\"].shape, tokenized[\"intervention_locations\"])\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f4ad91-4798-4f8f-abcf-590b5fb680db",
   "metadata": {},
   "source": [
    "Here we collate the `intervention_locations` together with other collating fields, such as the image features, image positions (boxes - for some reason they are all zero tensors in both ReFT and DoRA), and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c60d24ea-aac4-431b-bce4-3c215045831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_intervention_locations(datum):\n",
    "    new_data = {}\n",
    "    new_data[\"input_ids\"] = datum[\"input_ids\"]\n",
    "    new_data[\"intervention_locations\"] = datum[\"intervention_locations\"]\n",
    "    new_data[\"attention_mask\"] = datum[\"attention_mask\"]\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def reft_supplemental_data_collator(batch, tokenizer):\n",
    "    # Create padded `intervention_locations`\n",
    "    intervene_batch = [keep_intervention_locations(item) for item in batch]\n",
    "    # The normal data collator for collating other VLBart fields\n",
    "    intervention_loc_collate_fn = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=None,\n",
    "        label_pad_token_id=-100,\n",
    "        padding=\"longest\"\n",
    "    )\n",
    "    \n",
    "    intervene_batch_entry = intervention_loc_collate_fn(intervene_batch)\n",
    "\n",
    "    batch_entry = {}\n",
    "    id = []\n",
    "    instructions = []\n",
    "    # Collate `instruction` and `id`\n",
    "    for i, entry in enumerate(batch):\n",
    "        if 'instruction' in entry:\n",
    "            instructions.append(entry['instruction'])\n",
    "        if 'id' in entry:\n",
    "            id.append(entry['id'])\n",
    "    import numpy as np\n",
    "    batch_entry['id'] = np.array(id)\n",
    "    batch_entry['instruction'] = instructions\n",
    "    \n",
    "    # Pad `intervention_locations` with other stuff in the batch\n",
    "    if \"intervention_locations\" in batch[0]:\n",
    "        batch_entry[\"intervention_locations\"] = intervene_batch_entry[\"intervention_locations\"]\n",
    "    return batch_entry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f6fdd0-cf51-4f6c-9122-fc115a3e2940",
   "metadata": {},
   "source": [
    "ReFT data adds the `intervention_locations` to the VQA fine-tune dataset. Here we add the ReFT specifics to the original VQA dataset (coming from DoRA/VLAdapter). \n",
    "\n",
    "`VQAFineTuneDataset` integrates with the CLIP features of COCO images. The VQA task contains the mapping from the answers to their labels in the `datasets/vqa/v2_mscoco_train2014_annotations.json` file, and the mapping from question IDs to answers in the `datasets/vqa/karpathy_train.json` file. The VQA images are from the COCO dataset, which should be placed separately in `datasets/COCO/clip_features/data_clip_RN101_att` and `datasets/COCO/clip_features/data_clip_RN101_fc`. You can download all the datasets using the following command:\n",
    "\n",
    "```\n",
    "gdrive download 1O_RU1iFh_sbItZCTkOHUrbVIQQ_89Djj\n",
    "```\n",
    "\n",
    "Here is the [google drive link](https://drive.google.com/file/d/1O_RU1iFh_sbItZCTkOHUrbVIQQ_89Djj/view), we suggest you use [gdrive](https://github.com/prasmussen/gdrive) to download it. Put the `datasets` folder directly under `image_video_text_understanding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32011cda-462e-4c73-9ff6-0569b7a338aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "import vqa_clip_data as vqa_data\n",
    "\n",
    "class ReftVQAFineTuneDataset(vqa_data.VQAFineTuneDataset):\n",
    "    def __init__(self, split='train', raw_dataset=None, rank=-1, topk=-1, verbose=True, args=None, mode='train'):\n",
    "        super().__init__(split, raw_dataset, rank, topk, verbose, args, mode)\n",
    "        self.split = split\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        out_dict = super().__getitem__(idx)\n",
    "\n",
    "        out_dict[\"instruction\"] = self.tokenizer.decode(\n",
    "            out_dict['input_ids'], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        last_position = len(out_dict['input_ids']) - 1\n",
    "        out_dict = reft_post_process(\n",
    "            out_dict,\n",
    "            self.tokenizer,\n",
    "            idx,\n",
    "            last_position,\n",
    "            self.args,\n",
    "            pad_mode=\"first\",\n",
    "            fields_to_pad=[\"input_ids\"],\n",
    "            fields_to_mask=[\"input_ids\"]\n",
    "        )\n",
    "\n",
    "        return out_dict\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        batch_entry = super().collate_fn(batch)\n",
    "        # BEGIN ADD\n",
    "        extra_batch = reft_supplemental_data_collator(batch, self.tokenizer)\n",
    "        for k, v in extra_batch.items():\n",
    "            batch_entry[k] = v\n",
    "        # END ADD\n",
    "        # print(\"LOGITS:\", batch_entry[\"logits\"])\n",
    "        # print(\"LABELS:\", batch_entry[\"labels\"])\n",
    "        if self.split == \"karpathy_val\" or self.split == \"karpathy_test\":\n",
    "            print(\"In dataset:\", batch_entry[\"instruction\"], \" \", batch_entry[\"question_ids\"])\n",
    "\n",
    "        return batch_entry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cb3d460-8857-4368-9932-101919493bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(args, split='karpathy_train', mode='train',\n",
    "               batch_size=32, workers=4, distributed=False, gpu=0, topk=-1):\n",
    "\n",
    "    verbose = (gpu == 0)\n",
    "\n",
    "    _dset = vqa_data.VQADataset(split, verbose)\n",
    "    # print(\"Batch size:\", batch_size, \"Num workers:\", workers, \"Topk:\", topk)\n",
    "\n",
    "    dataset = ReftVQAFineTuneDataset(\n",
    "        split,\n",
    "        raw_dataset=_dset,\n",
    "        rank=gpu,\n",
    "        topk=topk,\n",
    "        verbose=verbose,\n",
    "        args=args,\n",
    "        mode=mode)\n",
    "    sampler = None\n",
    "\n",
    "    if mode == 'train':\n",
    "        loader = DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=(sampler is None),\n",
    "            num_workers=workers, pin_memory=True, sampler=sampler,\n",
    "            collate_fn=dataset.collate_fn)\n",
    "    else:\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=workers, pin_memory=True,\n",
    "            sampler=sampler,\n",
    "            shuffle=None if (sampler is not None) else False,\n",
    "            collate_fn=dataset.collate_fn,\n",
    "            drop_last=False)\n",
    "\n",
    "    if verbose:\n",
    "        loader.evaluator = vqa_data.VQAEvaluator(_dset)\n",
    "\n",
    "    loader.task = 'vqa'\n",
    "\n",
    "    return loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbfcc23-040f-43ba-8cf3-c3cb971cf3f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. Reft Model Replica for VLBart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790d9a60-89b4-4ea2-802f-db9c23e73bac",
   "metadata": {},
   "source": [
    "The forward pass API of ReFT is different from normal - we also need to pass in `intervention_locations`. `VLBartReft` wraps up the `VLBart` model implementation in VLAdapter/DoRA with `intervention_locations` passed in.\n",
    "\n",
    "In addition, we need to integrate ReFT's trainable parameters into VLBart Model's, so that gradient will propagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d574fd3-aa93-4dc9-a7e8-8a4f23e15da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_bart import VLBart\n",
    "class VLBartReft(VLBart):\n",
    "    def __init__(self, config: BartConfig):\n",
    "        super().__init__(config)\n",
    "        from pyreft import get_reft_model\n",
    "        self.intervenable = get_reft_model(self.model, config.reft_config)\n",
    "        # print(\"Reft parameters:\", self.intervenable.interventions)\n",
    "        # self.intervenable.unfreeze_intervention_parameters()\n",
    "        self.intervenable.print_trainable_parameters()\n",
    "        # print(\"INTERVENABLE:\", self.intervenable.model)\n",
    "\n",
    "        # Unfreeze the PyVene intervention parameters\n",
    "        for k, v in self.intervenable.unfreeze_intervention_parameters().items():\n",
    "            n = k.replace(\".\", \"#\")\n",
    "            print(n)\n",
    "            self.register_parameter(n, v)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "\n",
    "        vis_inputs=None,\n",
    "        vis_attention_mask=None,\n",
    "\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        task=None,\n",
    "\n",
    "        reduce_loss=False,\n",
    "        intervention_locations = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if labels is not None:\n",
    "            if decoder_input_ids is None:\n",
    "                decoder_input_ids = shift_tokens_right(\n",
    "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
    "                )\n",
    "        \n",
    "        if intervention_locations is not None:\n",
    "            # print(\"Intervention locs not None\")\n",
    "            # Pyvene forward pass\n",
    "            intervention_locations = intervention_locations.clone().detach().permute(1, 0, 2)\n",
    "            _, outputs = self.intervenable(\n",
    "                {\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"vis_inputs\": vis_inputs,\n",
    "                    \"vis_attention_mask\": vis_attention_mask,\n",
    "                    \"decoder_input_ids\": decoder_input_ids,\n",
    "                    \"decoder_attention_mask\": decoder_attention_mask,\n",
    "                    \"encoder_outputs\": encoder_outputs,\n",
    "                    \"past_key_values\": past_key_values,\n",
    "                    \"inputs_embeds\": inputs_embeds,\n",
    "                    \"decoder_inputs_embeds\": decoder_inputs_embeds,\n",
    "                    \"output_attentions\": output_attentions,\n",
    "                    \"output_hidden_states\": output_hidden_states,\n",
    "                    \"task\": task,\n",
    "                    \"return_dict\": return_dict,\n",
    "                },\n",
    "                unit_locations={\"sources->base\": (\n",
    "                    None,\n",
    "                    intervention_locations\n",
    "                )},\n",
    "                labels=labels,\n",
    "                return_dict=False,\n",
    "                subspaces=None,\n",
    "                use_cache=use_cache,\n",
    "            )\n",
    "        else:\n",
    "            # print(\"Intervention locs None\")\n",
    "            outputs = self.model(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "\n",
    "                vis_inputs=vis_inputs,\n",
    "                vis_attention_mask=vis_attention_mask,\n",
    "\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                decoder_attention_mask=decoder_attention_mask,\n",
    "                past_key_values=past_key_values,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                task=task,\n",
    "            )\n",
    "\n",
    "        # print(\"Outputs:\", outputs)\n",
    "        lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n",
    "        \n",
    "        masked_lm_loss = None\n",
    "        # print(\"LOGITS:\", lm_logits)\n",
    "        # print(\"LABELS\", labels)\n",
    "        if labels is not None:\n",
    "            # loss_fct = CrossEntropyLoss()\n",
    "            if reduce_loss:\n",
    "                loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss(ignore_index=-100, reduction='none')\n",
    "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            \n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + outputs[1:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "        # if masked_lm_loss is not None and len(masked_lm_loss) > 1:\n",
    "        #     masked_lm_loss = masked_lm_loss[0]\n",
    "        # print(\"LOSS 0:\", masked_lm_loss)\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
    "            decoder_attentions=outputs.decoder_attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
    "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
    "            encoder_attentions=outputs.encoder_attentions,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609328e2-5618-4e2e-a48d-c36829291bc5",
   "metadata": {},
   "source": [
    "Here we calculate the VQA training loss (whether the model output matches the label, weighted by the score). We also specify the correct VQA generation parameters here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e885ce0-d164-44b9-883c-3cedbbbd0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLBartVQA(VLBartReft):\n",
    "    def __init__(self, config, num_answers=None, label2ans=None):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.num_answers = num_answers\n",
    "        self.label2ans = label2ans\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def train_step(self, batch):\n",
    "\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        batch = self.vis_forward(batch, device)\n",
    "        task = batch[\"task\"]\n",
    "\n",
    "        vis_feats = batch['vis_feats'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        vis_pos = batch['boxes'].to(device)\n",
    "        intervention_locations = batch['intervention_locations'].to(device)\n",
    "\n",
    "        lm_labels = batch[\"target_ids\"].to(device)\n",
    "\n",
    "        output = self(\n",
    "            input_ids=input_ids,\n",
    "            vis_inputs=(vis_feats, vis_pos),\n",
    "            labels=lm_labels,\n",
    "            return_dict=True,\n",
    "            task=task,\n",
    "            intervention_locations=intervention_locations\n",
    "        )\n",
    "        assert 'loss' in output\n",
    "\n",
    "        lm_mask = (lm_labels != -100).float()\n",
    "        B, L = lm_labels.size()\n",
    "\n",
    "        loss = output['loss']\n",
    "\n",
    "        loss = loss.view(B, L) * lm_mask\n",
    "\n",
    "        loss = loss.sum(dim=1) / lm_mask.sum(dim=1).clamp(min=1)  # B\n",
    "\n",
    "        loss = loss * batch['scores'].to(device=device)\n",
    "\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        # print(\"LOSS 1:\", batch[\"scores\"], loss.item())\n",
    "        result = {\n",
    "            'loss': loss\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_step(self, batch, **kwargs):\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        batch = self.vis_forward(batch, device)\n",
    "\n",
    "        vis_feats = batch['vis_feats'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        vis_pos = batch['boxes'].to(device)\n",
    "        task = batch[\"task\"]\n",
    "        intervention_locations = batch['intervention_locations'].to(device)\n",
    "\n",
    "        result = {}\n",
    "        generation_args = {\n",
    "            \"base\": {\n",
    "                \"input_ids\":input_ids,\n",
    "                \"vis_inputs\":(vis_feats, vis_pos),\n",
    "                \"task\":task,\n",
    "                **kwargs\n",
    "            },\n",
    "            \"unit_locations\": {\"sources->base\": (None, \n",
    "                intervention_locations.permute(1, 0, 2))},\n",
    "            \"intervene_on_prompt\": True,\n",
    "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "            \"early_stopping\": True,\n",
    "            \"model\": self,\n",
    "        }\n",
    "        # print(\"Generating...\", input_ids.shape, intervention_locations)\n",
    "        # TODO: temperature, top_p, top_k\n",
    "        # print(\"GENERATE MODEL:\", self.intervenable.model)\n",
    "        _, output = self.intervenable.generate(**generation_args)\n",
    "        generated_sents = self.tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        result['token_ids'] = output\n",
    "        result['pred_ans'] = generated_sents\n",
    "\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4684f989-0756-47f1-a82b-305d4a3d38c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3. Multitask VLBart Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba702d3-ab30-4e98-acc7-54a2e288414b",
   "metadata": {},
   "source": [
    "#### 1.3.1 ReftConfig\n",
    "\n",
    "ReFT model needs `ReftConfig` to properly initialize. Here we specify the text and image intervention specs. They are all separate.\n",
    "\n",
    "Also, we change the weight decay of ReFT parameters to 0. ReFT does not like wd, but other parameters in the training of VLBart do (visual embeddings, layer norms, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3a9aa16-891b-4b60-a536-c3dae3fbc573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyreft import ReftConfig, LoreftIntervention, TaskType\n",
    "\n",
    "class ReftTrainer(TrainerBase):\n",
    "    def __init__(self, args, train_loader=None, val_loader=None, test_loader=None, train=True):\n",
    "        super().__init__(\n",
    "            args,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "            train=train)\n",
    "\n",
    "    def create_reft_config(self, config):\n",
    "        args = self.args\n",
    "        layers = args.layers\n",
    "        # ReFT layers - right now only \"all\" works properly\n",
    "        # TODO: properly process \"layers\" when it is not \"all\"\n",
    "        if layers != \"all\":\n",
    "            layers = [int(l) for l in layers.split(\";\")]\n",
    "        else:\n",
    "            layers = [l for l in range(config.num_hidden_layers)]\n",
    "        if '+' in self.args.positions and not args.share_weights:\n",
    "            layers += layers\n",
    "        \n",
    "        image_rank = args.reft_image_rank\n",
    "        text_rank = args.reft_rank\n",
    "        embed_dim = args.mid_dim\n",
    "\n",
    "        # print(\"REFT PARAMS:\",embed_dim, rank, args.dropout)\n",
    "        representations = []\n",
    "        # Text interventions\n",
    "        if text_rank != -1:\n",
    "            representations += [{\n",
    "                \"layer\": l, \"component\": \"block_output\",\n",
    "                \"low_rank_dimension\": text_rank,\n",
    "                \"intervention\": LoreftIntervention(\n",
    "                    embed_dim=embed_dim, low_rank_dimension=text_rank,\n",
    "                    dropout=args.reft_dropout, dtype=torch.float32, act_fn=None, device=\"cuda\",\n",
    "                    add_bias=True\n",
    "                )\n",
    "            } for l in layers]\n",
    "        # Image interventions\n",
    "        if image_rank != -1:\n",
    "            representations += [{\n",
    "                \"layer\": l, \"component\": \"block_output\",\n",
    "                \"low_rank_dimension\": image_rank,\n",
    "                \"intervention\": LoreftIntervention(\n",
    "                    embed_dim=embed_dim, low_rank_dimension=image_rank,\n",
    "                    dropout=args.reft_image_dropout, dtype=torch.float32, act_fn=None, device=\"cuda\",\n",
    "                    add_bias=True\n",
    "                )\n",
    "            } for l in layers]\n",
    "        reft_config = ReftConfig(representations=representations)\n",
    "        print(reft_config)\n",
    "        return reft_config\n",
    "\n",
    "    def create_config(self):\n",
    "        config = super().create_config()\n",
    "        setattr(config, \"reft_config\", self.create_reft_config(config))\n",
    "        return config\n",
    "\n",
    "    def create_optimizer_and_scheduler(self):\n",
    "        if self.verbose:\n",
    "            print('Building Optimizer')\n",
    "\n",
    "        lr_scheduler = None\n",
    "\n",
    "        from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "        # Added \"#unit#pos\" to `no_decay` to keep ReFT intervention's weight decay to 0\n",
    "        # Bart's bias and layer norm's weight decay is 0, others are not zero \n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\", \"#unit#pos\"]\n",
    "\n",
    "        if 'adamw' in self.args.optim:\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                    \"weight_decay\": self.args.weight_decay,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "            ]\n",
    "            optim = AdamW(optimizer_grouped_parameters,\n",
    "                        lr=self.args.lr, eps=self.args.adam_eps)\n",
    "\n",
    "        else:\n",
    "            # print(\"Parameters:\", self.model.named_parameters())\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                    \"weight_decay\": self.args.weight_decay,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "            ]\n",
    "            optim = self.args.optimizer(optimizer_grouped_parameters, self.args.lr)\n",
    "\n",
    "        batch_per_epoch = len(self.train_loader)\n",
    "        t_total = batch_per_epoch // self.args.gradient_accumulation_steps * self.args.epochs\n",
    "        warmup_ratio = self.args.warmup_ratio\n",
    "        warmup_iters = int(t_total * warmup_ratio)\n",
    "        if self.verbose:\n",
    "            print(\"Batch per epoch: %d\" % batch_per_epoch)\n",
    "            print(\"Total Iters: %d\" % t_total)\n",
    "            print('Warmup ratio:', warmup_ratio)\n",
    "            print(\"Warm up Iters: %d\" % warmup_iters)\n",
    "\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(optim, warmup_iters, t_total)\n",
    "\n",
    "        return optim, lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5708c2-6eed-4162-b5a3-bab4d1288020",
   "metadata": {},
   "source": [
    "#### 1.3.2 Reft images trainer\n",
    "\n",
    "This class is a complete VLBart trainer, integrated with ReFT. We\n",
    "\n",
    "- Unfreezed ReFT's parameters\n",
    "- Updated the model's config with ReFT\n",
    "- Performed gradient clipping (needed for visual embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "495a45bf-dfdb-4554-ada9-9393ac69406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(ReftTrainer):\n",
    "    def __init__(self, args, train_loader=None, val_loader=None, test_loader=None, train=True):\n",
    "        super().__init__(\n",
    "            args,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "            train=train)\n",
    "\n",
    "        model_kwargs = {}\n",
    "        if 'bart' in args.backbone:\n",
    "            model_class = VLBartVQA\n",
    "\n",
    "        config = self.create_config()\n",
    "        self.tokenizer = self.create_tokenizer()\n",
    "\n",
    "        if 'bart' in self.args.tokenizer:\n",
    "            num_added_toks = 0\n",
    "            if config.use_vis_order_embedding:\n",
    "                additional_special_tokens = [f'<extra_id_{i}>' for i in range(100-1, -1, -1)] + \\\n",
    "                        [f'<vis_extra_id_{i}>' for i in range(100-1, -1, -1)]\n",
    "                special_tokens_dict = {'additional_special_tokens': additional_special_tokens}\n",
    "                num_added_toks = self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "                config.default_obj_order_ids = self.tokenizer.convert_tokens_to_ids([f'<vis_extra_id_{i}>' for i in range(100)])\n",
    "\n",
    "        self.model = self.create_model(model_class, config, **model_kwargs)\n",
    "\n",
    "        if 't5' in self.args.tokenizer:\n",
    "            self.model.resize_token_embeddings(self.tokenizer.vocab_size)\n",
    "        elif 'bart' in self.args.tokenizer:\n",
    "            self.model.resize_token_embeddings(self.model.model.shared.num_embeddings + num_added_toks)\n",
    "\n",
    "        self.model.tokenizer = self.tokenizer\n",
    "        if 't5' in self.args.tokenizer or 'bart' in self.args.tokenizer:\n",
    "            self.model.true_id = self.tokenizer('true', add_special_tokens=False).input_ids[0]\n",
    "            self.model.false_id = self.tokenizer('false', add_special_tokens=False).input_ids[0]\n",
    "\n",
    "        # Load Checkpoint\n",
    "        self.start_epoch = None\n",
    "        if args.load is not None:\n",
    "            ckpt_path = args.load\n",
    "            self.load_checkpoint(ckpt_path)\n",
    "        if self.args.from_scratch:\n",
    "            self.init_weights()\n",
    "\n",
    "        # GPU Options\n",
    "        print(f'Model Launching at GPU {self.args.gpu}')\n",
    "        if self.verbose:\n",
    "            from time import time\n",
    "            start = time()\n",
    "        self.model = self.model.to(args.gpu)\n",
    "        \n",
    "        # Only thing changed: set device to cuda, and unfreeze ReFT params\n",
    "\n",
    "        self.model.intervenable.set_device(self.model.model.device)\n",
    "\n",
    "        self.freeze_whole_model() # freeze whole parameters first\n",
    "        self.unfreeze_parameters() # unfreeze selected parameters\n",
    "        self.model.intervenable.unfreeze_intervention_parameters()\n",
    "        # print(self.model)\n",
    "        self.percent_updated_parameters = self.print_trainable_params_percentage(self.model)\n",
    "\n",
    "        # Optimizer\n",
    "        if train:\n",
    "            self.optim, self.lr_scheduler = self.create_optimizer_and_scheduler()\n",
    "\n",
    "            if self.args.fp16 and _use_native_amp:\n",
    "                self.scaler = torch.cuda.amp.GradScaler()\n",
    "            elif _use_apex:\n",
    "                self.model, self.optim = amp.initialize(\n",
    "                    self.model, self.optim, opt_level='O1', verbosity=self.verbose)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'It took {time() - start:.1f}s')\n",
    "\n",
    "    def train(self):\n",
    "        if self.verbose:\n",
    "            vqa_loss_meter = LossMeter()\n",
    "            # best_eval_loss = 9595.\n",
    "            quesid2ans = {}\n",
    "            best_vqa_valid = 0.\n",
    "            best_vqa_epoch = 0\n",
    "\n",
    "            wandb.init(project=self.args.project_name)\n",
    "            wandb.run.name = self.args.run_name\n",
    "            wandb.config.update(self.args)\n",
    "            wandb.watch(self.model)\n",
    "            wandb.log(\n",
    "                {\"percent of updated parameters (%)\": self.percent_updated_parameters}\n",
    "            )\n",
    "\n",
    "            src_dir = os.path.dirname(os.getcwd())\n",
    "            base_path = os.path.dirname(src_dir)\n",
    "            src_dir = str(src_dir)\n",
    "            wandb.save(os.path.join(src_dir + \"/*.py\"), base_path=base_path)\n",
    "\n",
    "        global_step = 0\n",
    "        for epoch in range(self.args.epochs):\n",
    "            if self.start_epoch is not None:\n",
    "                epoch += self.start_epoch\n",
    "            self.model.train()\n",
    "            self.partial_eval()\n",
    "\n",
    "            if self.verbose:\n",
    "                pbar = tqdm(total=len(self.train_loader), ncols=250)\n",
    "\n",
    "            epoch_results = {\n",
    "                'loss': 0.,\n",
    "            }\n",
    "\n",
    "            task_counter = {\n",
    "                'vqa': 0,\n",
    "            }\n",
    "\n",
    "            # vqa\n",
    "            quesid2ans = {}\n",
    "            train_acc = 0.\n",
    "            # train_acc_steps = int(len(self.train_loader) * 0.05)\n",
    "            # last_acc_step = 0\n",
    "\n",
    "            for step_i, batch in enumerate(self.train_loader):\n",
    "\n",
    "                # print(f'GPU{self.args.gpu} inside training loop')\n",
    "                # print(batch)\n",
    "                task = batch['task']\n",
    "                # if self.verbose:\n",
    "                #     print('task', task)\n",
    "                task_counter[task] += 1\n",
    "\n",
    "                batch['log_train_accuracy'] = self.args.log_train_accuracy\n",
    "\n",
    "                # self.optim.zero_grad()\n",
    "                if self.args.fp16 and _use_native_amp:\n",
    "                    with autocast():\n",
    "                        results = self.model.train_step(batch)\n",
    "                else:\n",
    "                    results = self.model.train_step(batch)\n",
    "\n",
    "                loss = results['loss']\n",
    "\n",
    "                if self.args.fp16 and _use_native_amp:\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                elif self.args.fp16 and _use_apex:\n",
    "                    with amp.scale_loss(loss, self.optim) as scaled_loss:\n",
    "                        scaled_loss.backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "\n",
    "                # print(f'GPU{self.args.gpu} after backward')\n",
    "\n",
    "                loss = loss.detach()\n",
    "\n",
    "                # Update Parameters\n",
    "                if self.args.clip_grad_norm > 0:\n",
    "                    if self.args.fp16 and _use_native_amp:\n",
    "                        self.scaler.unscale_(self.optim)\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            self.model.parameters(), self.args.clip_grad_norm)\n",
    "                    elif self.args.fp16 and _use_apex:\n",
    "                        torch.nn.utils.clip_grad_norm_(amp.master_params(\n",
    "                            self.optim), self.args.clip_grad_norm)\n",
    "                    else:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            self.model.parameters(), self.args.clip_grad_norm)\n",
    "\n",
    "                if self.args.fp16 and _use_native_amp:\n",
    "                    self.scaler.step(self.optim)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    self.optim.step()\n",
    "\n",
    "                if self.lr_scheduler:\n",
    "                    self.lr_scheduler.step()\n",
    "                for param in self.model.parameters():\n",
    "                    param.grad = None\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "                for k, v in results.items():\n",
    "                    if k in epoch_results:\n",
    "                        epoch_results[k] += v.item()\n",
    "\n",
    "                if self.lr_scheduler:\n",
    "                    if version.parse(torch.__version__) >= version.parse(\"1.4\"):\n",
    "                        lr = self.lr_scheduler.get_last_lr()[0]\n",
    "                    else:\n",
    "                        lr = self.lr_scheduler.get_lr()[0]\n",
    "                else:\n",
    "                    try:\n",
    "                        lr = self.optim.get_lr()[0]\n",
    "                    except AttributeError:\n",
    "                        lr = self.args.lr\n",
    "\n",
    "                if self.verbose:\n",
    "                    if task == 'vqa':\n",
    "                        vqa_loss_meter.update(loss.item())\n",
    "\n",
    "                    desc_str = f'Epoch {epoch} | LR {lr:.6f}'\n",
    "\n",
    "                    desc_str += f\" |\"\n",
    "                    if 'vqa' in self.args.tasks:\n",
    "                        desc_str += f\" VQA {task_counter['vqa']}\"\n",
    "                    if len(vqa_loss_meter) > 0:\n",
    "                        desc_str += f' | VQA Loss {vqa_loss_meter.val:4f}'\n",
    "\n",
    "                    pbar.set_description(desc_str)\n",
    "                    pbar.update(1)\n",
    "\n",
    "            if self.verbose:\n",
    "                pbar.close()\n",
    "\n",
    "            if self.args.log_train_accuracy:\n",
    "                train_score_dict = {\n",
    "                    'n_correct': n_correct,\n",
    "                    'n_total': n_total\n",
    "                }\n",
    "                train_score_dict = reduce_dict(train_score_dict, self.args.gpu)\n",
    "\n",
    "            if self.verbose:\n",
    "                # Validation\n",
    "                log_str = ''\n",
    "                wandb_log_dict = {}\n",
    "\n",
    "                if 'vqa' in self.args.tasks:\n",
    "                    # VQA\n",
    "                    vqa_val_loader = self.val_loader['vqa']\n",
    "                    score_dict = self.vqa_evaluate(vqa_val_loader)\n",
    "                    valid_score = score_dict['topk_score'] * 100.\n",
    "                    valid_score_raw = score_dict['overall']\n",
    "                    if valid_score_raw > best_vqa_valid or epoch == 0:\n",
    "                        best_vqa_valid = valid_score_raw\n",
    "                        best_vqa_epoch = epoch\n",
    "                        # self.save(\"VQA_BEST\")\n",
    "                    log_str += f\"VQA\"\n",
    "                    log_str += \"\\nEpoch %d: Valid Raw %0.2f Topk %0.2f\" % (epoch, valid_score_raw, valid_score)\n",
    "                    log_str += \"\\nEpoch %d: Best Raw %0.2f\\n\" % (best_vqa_epoch, best_vqa_valid)\n",
    "                    wandb_log_dict['VQA/Valid/score'] = valid_score\n",
    "                    wandb_log_dict['VQA/Valid/raw_score'] = score_dict['overall']\n",
    "                \n",
    "                wandb.log(wandb_log_dict, step=epoch)\n",
    "\n",
    "                print(log_str)\n",
    "\n",
    "        # Test Set\n",
    "        if self.verbose:\n",
    "            self.save(\"LAST\")\n",
    "\n",
    "            log_str = ''\n",
    "            wandb_log_dict = {}\n",
    "\n",
    "            if 'vqa' in self.args.tasks:\n",
    "                # VQA\n",
    "                vqa_test_loader = self.test_loader['vqa']\n",
    "                evaluator = vqa_test_loader.evaluator\n",
    "                dump_path = os.path.join(self.args.output, 'karpathy_test_predict.json')\n",
    "                quesid2ans = self.vqa_predict(vqa_test_loader, dump_path)\n",
    "                wandb.save(dump_path, base_path=self.args.output)\n",
    "\n",
    "                acc_dict_all = evaluator.evaluate_raw(quesid2ans)\n",
    "                acc_dict_answerable = evaluator.evaluate_raw(quesid2ans, is_topk_optimal=True)\n",
    "                acc_dict_unanswerable = evaluator.evaluate_raw(quesid2ans, is_topk_optimal=False)\n",
    "\n",
    "                wandb_log_dict['VQA/Test/overall'] = acc_dict_all['overall']\n",
    "                wandb_log_dict['VQA/Test/topk_optimal'] = acc_dict_answerable['overall']\n",
    "                wandb_log_dict['VQA/Test/topk_not_optimal'] = acc_dict_unanswerable['overall']\n",
    "\n",
    "                if self.test_loader.get(\"vqa_submit\", None):\n",
    "                    vqa_submit_test_loader = self.test_loader['vqa_submit']\n",
    "                    dump_path = os.path.join(self.args.output, 'vqa_submit.json')\n",
    "                    self.vqa_predict(vqa_submit_test_loader, dump_path=dump_path)\n",
    "                    wandb.save(dump_path, base_path=self.args.output)\n",
    "\n",
    "            print(log_str)\n",
    "            wandb.log(wandb_log_dict, step=self.args.epochs)\n",
    "\n",
    "            wandb.log({'finished': True})\n",
    "\n",
    "    def vqa_predict(self, loader, dump_path=None):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            quesid2ans = {}\n",
    "\n",
    "            gen_kwargs = {}\n",
    "            gen_kwargs['num_beams'] = 1\n",
    "\n",
    "            for i, batch in enumerate(tqdm(loader, ncols=150, desc=\"VQA Validation\")):\n",
    "\n",
    "                if self.args.distributed:\n",
    "                    results = self.model.module.test_step(batch, **gen_kwargs)\n",
    "                else:\n",
    "                    results = self.model.test_step(batch, **gen_kwargs)\n",
    "\n",
    "                pred_ans = results['pred_ans']\n",
    "                ques_ids = batch['question_ids']\n",
    "\n",
    "                for qid, ans in zip(ques_ids, pred_ans):\n",
    "                    quesid2ans[qid] = ans\n",
    "\n",
    "            if dump_path is not None:\n",
    "                loader.evaluator.dump_result(quesid2ans, dump_path)\n",
    "            return quesid2ans\n",
    "\n",
    "    def vqa_evaluate(self, loader, dump_path=None):\n",
    "        evaluator = loader.evaluator\n",
    "        quesid2ans = self.vqa_predict(loader, dump_path)\n",
    "\n",
    "        acc_dict = evaluator.evaluate_raw(quesid2ans)\n",
    "\n",
    "        topk_score = evaluator.evaluate(quesid2ans)\n",
    "        acc_dict['topk_score'] = topk_score\n",
    "\n",
    "        return acc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b4ae2-8de0-4362-a759-bfb047854f03",
   "metadata": {},
   "source": [
    "### 4. Main Worker, Params, and Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e82e41-d794-4f8d-b2a0-30cffc1dc694",
   "metadata": {},
   "source": [
    "Here we specify all the parameters for VLBart and ReFT. Note that we unfroze the bias and the layer norms, as well as the visual embeddings. For the detailed modules that we tune, see `trainer_base.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aef8517f-aa1f-41d3-a01d-0f5c8edb7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "args = parse_args(False)\n",
    "ngpus_per_node = torch.cuda.device_count()\n",
    "args.world_size = ngpus_per_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1c6a607-bc07-4236-be0a-ed6af23ac2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.distributed = False\n",
    "args.nproc_per_node = 1\n",
    "args.master_port = 26464\n",
    "args.multiGPU = True\n",
    "args.optim = \"adamw\"\n",
    "args.warmup_ratio = 0.1\n",
    "args.clip_grad_norm = 5\n",
    "args.weight_decay = 0.01\n",
    "args.lr = 1e-3\n",
    "args.epochs = 3\n",
    "args.num_workers = 4\n",
    "args.backbone = \"facebook/bart-base\"\n",
    "args.output = \"snap/VLBart_dora_reft/test/\"\n",
    "args.num_beams = 5\n",
    "args.use_tasks_prompts = True\n",
    "args.train_topk = 10000\n",
    "args.valid_topk = 4\n",
    "args.batch_size = 100\n",
    "args.valid_batch_size = 1\n",
    "# args.use_dora = True\n",
    "args.unfreeze_bias = True\n",
    "args.unfreeze_layer_norms = True\n",
    "# args.lora_settings = True\n",
    "# args.lora_dim = 128\n",
    "args.tasks = \"vqa\"\n",
    "args.dropout = 0.00\n",
    "args.reft_dropout = 0.00\n",
    "args.reft_image_dropout = 0.00\n",
    "args.reft_rank = 4\n",
    "args.reft_image_rank = 64\n",
    "args.positions = \"f3+l3\"\n",
    "args.image_positions = \"f3+l3\"\n",
    "\n",
    "args.feature = \"RN101\"\n",
    "args.n_boxes = 36\n",
    "args.downsample = True\n",
    "args.image_size = \"(224,224)\"\n",
    "args.project_name = \"Test\"\n",
    "args.run_name = \"tune+lr1e-3\"\n",
    "args.local_rank = 0\n",
    "args.feature_type = \"RN101\"\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '26464'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0accb8d-5a6f-4766-933b-50141249e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations\n",
      "{'adam_beta1': 0.9,\n",
      " 'adam_beta2': 0.999,\n",
      " 'adam_eps': 1e-06,\n",
      " 'add_adapter_cross_attn': True,\n",
      " 'add_layer_norm_after_adapter': False,\n",
      " 'add_layer_norm_before_adapter': False,\n",
      " 'additional_visual_embedding_layers': 0,\n",
      " 'answer_normalize': False,\n",
      " 'backbone': 'facebook/bart-base',\n",
      " 'batch_size': 100,\n",
      " 'caption_cocoonly': True,\n",
      " 'caption_only': False,\n",
      " 'classifier': False,\n",
      " 'clip_grad_norm': 5,\n",
      " 'cls_task': 'tinyimagenet',\n",
      " 'coco_only': False,\n",
      " 'comment': '',\n",
      " 'decoder_prompt_len': 0,\n",
      " 'deepspeed': None,\n",
      " 'distributed': False,\n",
      " 'do_lower_case': False,\n",
      " 'dora_simple': False,\n",
      " 'downsample': True,\n",
      " 'dropout': 0.0,\n",
      " 'dry': False,\n",
      " 'efficient_unique_hyper_net': False,\n",
      " 'encoder_prompt_len': 0,\n",
      " 'epochs': 3,\n",
      " 'expand_vis_embedding': False,\n",
      " 'factorized_phm': True,\n",
      " 'feat_dim': 2048,\n",
      " 'feature': 'RN101',\n",
      " 'feature_type': 'RN101',\n",
      " 'fp16': False,\n",
      " 'freeze_bn_statistics': False,\n",
      " 'freeze_ln_statistics': False,\n",
      " 'from_scratch': False,\n",
      " 'gen_max_length': 20,\n",
      " 'gradient_accumulation_steps': 1,\n",
      " 'hypercomplex_division': 4,\n",
      " 'image_positions': 'f3+l3',\n",
      " 'image_size': '(224,224)',\n",
      " 'individual_vis_layer_norm': True,\n",
      " 'lambda_z': 0.001,\n",
      " 'layers': '0;1;2;3;4;5',\n",
      " 'load': None,\n",
      " 'load_lxmert_qa': None,\n",
      " 'local_rank': 0,\n",
      " 'log_train_accuracy': False,\n",
      " 'lora_alpha': 32,\n",
      " 'lora_dim': 4,\n",
      " 'lora_settings': False,\n",
      " 'losses': 'lm,obj,attr,feat',\n",
      " 'low_rank_rank': 1,\n",
      " 'lr': 0.001,\n",
      " 'master_port': 26464,\n",
      " 'max_n_boxes': 36,\n",
      " 'max_text_length': 20,\n",
      " 'mid_dim': 768,\n",
      " 'multiGPU': True,\n",
      " 'multitask_sampling': 'roundrobin',\n",
      " 'n_boxes': 36,\n",
      " 'n_ground': 1,\n",
      " 'n_image_tokens': 4,\n",
      " 'nproc_per_node': 1,\n",
      " 'num_beams': 5,\n",
      " 'num_workers': 4,\n",
      " 'obj_mask_rate': 0.15,\n",
      " 'oneddownsample': False,\n",
      " 'optim': 'adamw',\n",
      " 'optimizer': 'adamw',\n",
      " 'oscar_tags': False,\n",
      " 'output': 'snap/VLBart_dora_reft/test/',\n",
      " 'phm_init_range': 0.01,\n",
      " 'phm_rank': 1,\n",
      " 'pos_dim': 4,\n",
      " 'positions': 'f3+l3',\n",
      " 'post_prompt': '',\n",
      " 'prefix': None,\n",
      " 'project_name': 'Test',\n",
      " 'projected_task_embedding_dim': -1,\n",
      " 'prompt': 'vqa: ',\n",
      " 'raw_label': False,\n",
      " 'reduction_factor': 16,\n",
      " 'reft_dropout': 0.0,\n",
      " 'reft_image_dropout': 0.0,\n",
      " 'reft_image_rank': 64,\n",
      " 'reft_rank': 4,\n",
      " 'remove_bn_vis_adapter': False,\n",
      " 'run_name': 'tune+lr1e-3',\n",
      " 'seed': 9595,\n",
      " 'share_down_sampler': False,\n",
      " 'share_up_sampler': False,\n",
      " 'share_vis_lang_layer_norm': False,\n",
      " 'share_weights': False,\n",
      " 'shared_phm_rule': True,\n",
      " 'shared_phm_rule_over_tasks': False,\n",
      " 'sparse_sample': False,\n",
      " 'submit': False,\n",
      " 'tasks': 'vqa',\n",
      " 'test': None,\n",
      " 'test_answerable': False,\n",
      " 'test_only': False,\n",
      " 'testing': False,\n",
      " 'tokenizer': None,\n",
      " 'track_z': False,\n",
      " 'train': 'train',\n",
      " 'train_topk': 10000,\n",
      " 'unfreeze_batch_norms': False,\n",
      " 'unfreeze_bias': True,\n",
      " 'unfreeze_decoder_layer_norms': False,\n",
      " 'unfreeze_encoder_layer_norms': False,\n",
      " 'unfreeze_language_model': False,\n",
      " 'unfreeze_layer_norms': True,\n",
      " 'unfreeze_lm_head': False,\n",
      " 'unfreeze_vis_encoder': False,\n",
      " 'unfreeze_vis_last_layer': False,\n",
      " 'unique_hyper_net': False,\n",
      " 'use_adam_for_visual': False,\n",
      " 'use_adapter': False,\n",
      " 'use_attn_prefix': False,\n",
      " 'use_compacter': False,\n",
      " 'use_data_augmentation': False,\n",
      " 'use_dora': False,\n",
      " 'use_hyperformer': False,\n",
      " 'use_lm_head_adapter': False,\n",
      " 'use_lora': False,\n",
      " 'use_lradapter': False,\n",
      " 'use_separate_optimizer_for_visual': False,\n",
      " 'use_single_adapter': False,\n",
      " 'use_single_lora': False,\n",
      " 'use_single_prompt': False,\n",
      " 'use_tasks_prompts': True,\n",
      " 'use_vis_adapter': False,\n",
      " 'use_vis_layer_norm': True,\n",
      " 'use_vis_order_embedding': True,\n",
      " 'use_vision': True,\n",
      " 'valid': 'valid',\n",
      " 'valid_batch_size': 1,\n",
      " 'valid_topk': 4,\n",
      " 'vis_adapter_type': 'middle-bottleneck',\n",
      " 'vis_lr': 0.0001,\n",
      " 'vis_pooling_output': False,\n",
      " 'vis_reduction_factor': 2,\n",
      " 'vis_weight_decay': 0.01,\n",
      " 'warmup_ratio': 0.1,\n",
      " 'weight_decay': 0.01,\n",
      " 'word_mask_rate': 0.15,\n",
      " 'world_size': 1}\n"
     ]
    }
   ],
   "source": [
    "# cudnn.benchmark = True\n",
    "# args = parse_args(False)\n",
    "ngpus_per_node = torch.cuda.device_count()\n",
    "args.world_size = ngpus_per_node\n",
    "if args.local_rank in [0, -1]:\n",
    "    print(args)\n",
    "\n",
    "    comments = []\n",
    "    if args.load is not None:\n",
    "        ckpt_str = \"_\".join(args.load.split('/')[-3:])\n",
    "        comments.append(ckpt_str)\n",
    "    if args.comment != '':\n",
    "        comments.append(args.comment)\n",
    "    comment = '_'.join(comments)\n",
    "\n",
    "    from datetime import datetime\n",
    "    current_time = datetime.now().strftime('%b%d_%H-%M')\n",
    "    run_name = f'{current_time}_GPU{args.world_size}'\n",
    "    if len(comments) > 0:\n",
    "        run_name += f'_{comment}'\n",
    "\n",
    "    if args.run_name == \"\":\n",
    "        args.run_name = run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71a508de-c65b-45ce-964e-ae7f138ca8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching at GPU 0\n",
      "args.feature_type RN101\n"
     ]
    }
   ],
   "source": [
    "# GPU is assigned\n",
    "gpu = args.local_rank\n",
    "args.gpu = gpu\n",
    "args.rank = gpu\n",
    "print(f'Launching at GPU {gpu}')\n",
    "\n",
    "print(f\"args.feature_type {args.feature_type}\")\n",
    "feat_dim_dict = {\n",
    "    \"RN50\": 2048,\n",
    "    \"RN101\": 2048,\n",
    "    \"RN50x4\": 2560,\n",
    "    \"ViT\": 768\n",
    "}\n",
    "args.feat_dim = feat_dim_dict[args.feature_type]\n",
    "\n",
    "vqa_args = deepcopy(args)\n",
    "vqa_args.max_text_length = 20\n",
    "\n",
    "\n",
    "if args.use_tasks_prompts:\n",
    "    vqa_args.prompt = \"vqa: \"\n",
    "else:\n",
    "    vqa_args.prompt = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff4312-8146-45d5-a00e-f668d4df84f2",
   "metadata": {},
   "source": [
    "Now we create the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85c17a83-f8ff-4568-99d2-2701ee41387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 605102 data from split(s) karpathy_train.\n",
      "# Answers: 3129\n",
      "Data sources:  ['karpathy_train']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/peterwz/miniconda3/envs/peterwz-dora/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 605102 data from karpathy_train\n",
      "Use only 10000 data\n",
      "# all sentences: 10000\n",
      "Building VQA val loader at GPU 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/peterwz/miniconda3/envs/peterwz-dora/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 26729 data from split(s) karpathy_val.\n",
      "# Answers: 3129\n",
      "Data sources:  ['karpathy_val']\n",
      "Loaded 26729 data from karpathy_val\n",
      "Use only 4 data\n",
      "# all sentences: 4\n",
      "Building VQA test loader at GPU 0\n",
      "Load 26280 data from split(s) karpathy_test.\n",
      "# Answers: 3129\n",
      "Data sources:  ['karpathy_test']\n",
      "Loaded 26280 data from karpathy_test\n",
      "Use only 4 data\n",
      "# all sentences: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loaders = []\n",
    "\n",
    "train_loader = get_loader(\n",
    "    vqa_args,\n",
    "    split='karpathy_train', mode='train', batch_size=vqa_args.batch_size,\n",
    "    distributed=False, gpu=args.gpu,\n",
    "    workers=args.num_workers,\n",
    "    topk=args.train_topk,\n",
    ")\n",
    "\n",
    "val_num_workers = 4\n",
    "# Validation set\n",
    "if gpu == 0:\n",
    "    val_loader = {}\n",
    "    if args.epochs > 0:\n",
    "        if 'vqa' in args.tasks:\n",
    "            print(f'Building VQA val loader at GPU {gpu}')\n",
    "            vqa_val_loader = get_loader(\n",
    "                vqa_args,\n",
    "                split='karpathy_val', mode='val', batch_size=vqa_args.valid_batch_size,\n",
    "                distributed=False, gpu=args.gpu,\n",
    "                workers=val_num_workers,\n",
    "                topk=args.valid_topk,\n",
    "            )\n",
    "            val_loader['vqa'] = vqa_val_loader\n",
    "\n",
    "    # Test set\n",
    "    test_loader = {}\n",
    "    if 'vqa' in args.tasks:\n",
    "        print(f'Building VQA test loader at GPU {gpu}')\n",
    "        vqa_test_loader = get_loader(\n",
    "            vqa_args,\n",
    "            split='karpathy_test', mode='val', batch_size=vqa_args.valid_batch_size,\n",
    "            distributed=False, gpu=args.gpu,\n",
    "            workers=val_num_workers,\n",
    "            topk=args.valid_topk,\n",
    "        )\n",
    "        test_loader['vqa'] = vqa_test_loader\n",
    "\n",
    "        if args.testing:\n",
    "            vqa_submit_test_loader = get_loader(\n",
    "                vqa_args,\n",
    "                split='test_4', mode='val', batch_size=vqa_args.valid_batch_size,\n",
    "                distributed=False, gpu=args.gpu,\n",
    "                workers=val_num_workers,\n",
    "                topk=args.valid_topk,\n",
    "            )\n",
    "            test_loader['vqa_submit'] = vqa_submit_test_loader\n",
    "else:\n",
    "    val_loader = None\n",
    "    test_loader = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce4484-c07b-44e9-afe4-2b32065e05e1",
   "metadata": {},
   "source": [
    "Now we start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb661683-666b-432d-b77e-9849fc726a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntervenableConfig\n",
      "{\n",
      "    \"model_type\": \"None\",\n",
      "    \"representations\": [\n",
      "        {\n",
      "            \"layer\": 0,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 1,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 2,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 3,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 4,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 5,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 0,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 1,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 2,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 3,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 4,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 5,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 0,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 1,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 2,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 3,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 4,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 5,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 0,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 1,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 2,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 3,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 4,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 5,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        }\n",
      "    ],\n",
      "    \"intervention_types\": \"[<class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>]\",\n",
      "    \"mode\": \"parallel\",\n",
      "    \"sorted_keys\": \"None\",\n",
      "    \"intervention_dimensions\": \"None\"\n",
      "}\n",
      "Building Model at GPU 0\n",
      "trainable intervention params: 1,254,192 || trainable model params: 0\n",
      "model params: 140,995,584 || trainable%: 0.8895257315292938\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#0#learned_source#weight\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#0#learned_source#bias\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#0#learned_source#weight\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#0#learned_source#bias\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#0#learned_source#weight\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#0#learned_source#bias\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#0#learned_source#weight\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#0#learned_source#bias\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#0#learned_source#weight\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#0#learned_source#bias\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#0#learned_source#weight\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#0#learned_source#bias\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#1#learned_source#weight\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#1#learned_source#bias\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#1#learned_source#weight\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#1#learned_source#bias\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#1#learned_source#weight\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#1#learned_source#bias\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#1#learned_source#weight\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#1#learned_source#bias\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#1#learned_source#weight\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#1#learned_source#bias\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#1#learned_source#weight\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#1#learned_source#bias\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#2#learned_source#weight\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#2#learned_source#bias\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#2#learned_source#weight\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#2#learned_source#bias\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#2#learned_source#weight\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#2#learned_source#bias\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#2#learned_source#weight\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#2#learned_source#bias\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#2#learned_source#weight\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#2#learned_source#bias\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#2#learned_source#weight\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#2#learned_source#bias\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#3#learned_source#weight\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#3#learned_source#bias\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#3#learned_source#weight\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#3#learned_source#bias\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#3#learned_source#weight\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#3#learned_source#bias\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#3#learned_source#weight\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#3#learned_source#bias\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#3#learned_source#weight\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#3#learned_source#bias\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#3#learned_source#weight\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#3#learned_source#bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VLBartVQA were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['layer#5#comp#block_output#unit#pos#nunit#1#3#learned_source#weight', 'layer#3#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original', 'layer#1#comp#block_output#unit#pos#nunit#1#3#learned_source#bias', 'layer#3#comp#block_output#unit#pos#nunit#1#3#learned_source#weight', 'encoder.visual_embedding.feat_embedding.1.weight', 'layer#1#comp#block_output#unit#pos#nunit#1#2#learned_source#weight', 'layer#2#comp#block_output#unit#pos#nunit#1#1#learned_source#bias', 'layer#5#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original', 'layer#4#comp#block_output#unit#pos#nunit#1#3#learned_source#bias', 'layer#3#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original', 'layer#0#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original', 'layer#5#comp#block_output#unit#pos#nunit#1#1#learned_source#weight', 'layer#4#comp#block_output#unit#pos#nunit#1#2#learned_source#weight', 'layer#1#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original', 'layer#3#comp#block_output#unit#pos#nunit#1#1#learned_source#bias', 'layer#0#comp#block_output#unit#pos#nunit#1#1#learned_source#weight', 'layer#0#comp#block_output#unit#pos#nunit#1#2#learned_source#bias', 'layer#2#comp#block_output#unit#pos#nunit#1#0#learned_source#bias', 'layer#2#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original', 'layer#5#comp#block_output#unit#pos#nunit#1#0#learned_source#weight', 'layer#5#comp#block_output#unit#pos#nunit#1#1#learned_source#bias', 'layer#1#comp#block_output#unit#pos#nunit#1#1#learned_source#bias', 'layer#5#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original', 'layer#5#comp#block_output#unit#pos#nunit#1#0#learned_source#bias', 'layer#0#comp#block_output#unit#pos#nunit#1#3#learned_source#weight', 'layer#3#comp#block_output#unit#pos#nunit#1#1#learned_source#weight', 'layer#1#comp#block_output#unit#pos#nunit#1#0#learned_source#weight', 'layer#4#comp#block_output#unit#pos#nunit#1#0#learned_source#bias', 'layer#2#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original', 'layer#5#comp#block_output#unit#pos#nunit#1#2#learned_source#weight', 'layer#0#comp#block_output#unit#pos#nunit#1#0#learned_source#weight', 'layer#1#comp#block_output#unit#pos#nunit#1#2#learned_source#bias', 'layer#0#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original', 'layer#3#comp#block_output#unit#pos#nunit#1#0#learned_source#weight', 'layer#0#comp#block_output#unit#pos#nunit#1#0#learned_source#bias', 'layer#2#comp#block_output#unit#pos#nunit#1#1#learned_source#weight', 'intervenable.model.encoder.visual_embedding.feat_embedding.1.bias', 'layer#3#comp#block_output#unit#pos#nunit#1#2#learned_source#weight', 'layer#3#comp#block_output#unit#pos#nunit#1#2#learned_source#bias', 'intervenable.model.encoder.visual_embedding.feat_embedding.0.bias', 'layer#4#comp#block_output#unit#pos#nunit#1#3#learned_source#weight', 'layer#2#comp#block_output#unit#pos#nunit#1#2#learned_source#bias', 'layer#1#comp#block_output#unit#pos#nunit#1#1#learned_source#weight', 'layer#1#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original', 'layer#5#comp#block_output#unit#pos#nunit#1#3#learned_source#bias', 'layer#3#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original', 'layer#3#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original', 'layer#2#comp#block_output#unit#pos#nunit#1#0#learned_source#weight', 'intervenable.model.encoder.visual_embedding.feat_embedding.0.weight', 'layer#1#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original', 'encoder.visual_embedding.feat_embedding.0.bias', 'layer#4#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original', 'layer#3#comp#block_output#unit#pos#nunit#1#0#learned_source#bias', 'layer#4#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original', 'intervenable.model.encoder.visual_embedding.feat_embedding.1.weight', 'layer#2#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original', 'layer#2#comp#block_output#unit#pos#nunit#1#2#learned_source#weight', 'layer#3#comp#block_output#unit#pos#nunit#1#3#learned_source#bias', 'layer#4#comp#block_output#unit#pos#nunit#1#2#learned_source#bias', 'layer#2#comp#block_output#unit#pos#nunit#1#3#learned_source#weight', 'layer#4#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original', 'layer#4#comp#block_output#unit#pos#nunit#1#1#learned_source#bias', 'layer#1#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original', 'layer#2#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original', 'layer#4#comp#block_output#unit#pos#nunit#1#1#learned_source#weight', 'layer#0#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original', 'layer#5#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original', 'layer#4#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original', 'layer#0#comp#block_output#unit#pos#nunit#1#1#learned_source#bias', 'layer#4#comp#block_output#unit#pos#nunit#1#0#learned_source#weight', 'layer#1#comp#block_output#unit#pos#nunit#1#3#learned_source#weight', 'layer#0#comp#block_output#unit#pos#nunit#1#2#learned_source#weight', 'encoder.visual_embedding.feat_embedding.1.bias', 'layer#5#comp#block_output#unit#pos#nunit#1#2#learned_source#bias', 'layer#2#comp#block_output#unit#pos#nunit#1#3#learned_source#bias', 'layer#5#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original', 'layer#0#comp#block_output#unit#pos#nunit#1#3#learned_source#bias', 'encoder.visual_embedding.feat_embedding.0.weight', 'layer#0#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original', 'layer#1#comp#block_output#unit#pos#nunit#1#0#learned_source#bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50465. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Launching at GPU 0\n",
      "model.encoder.visual_embedding.feat_embedding.0.weight is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.0.bias is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.1.weight is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.1.bias is trainable...\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#0#learned_source#bias is trainable...(4)\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#0#learned_source#bias is trainable...(4)\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#0#learned_source#bias is trainable...(4)\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#0#learned_source#bias is trainable...(4)\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#0#learned_source#bias is trainable...(4)\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#0#learned_source#bias is trainable...(4)\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#1#learned_source#bias is trainable...(4)\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#1#learned_source#bias is trainable...(4)\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#1#learned_source#bias is trainable...(4)\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#1#learned_source#bias is trainable...(4)\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#1#learned_source#bias is trainable...(4)\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#1#learned_source#bias is trainable...(4)\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#2#learned_source#bias is trainable...(64)\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#2#learned_source#bias is trainable...(64)\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#2#learned_source#bias is trainable...(64)\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#2#learned_source#bias is trainable...(64)\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#2#learned_source#bias is trainable...(64)\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#2#learned_source#bias is trainable...(64)\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#3#learned_source#bias is trainable...(64)\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#3#learned_source#bias is trainable...(64)\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#3#learned_source#bias is trainable...(64)\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#3#learned_source#bias is trainable...(64)\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#3#learned_source#bias is trainable...(64)\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#3#learned_source#bias is trainable...(64)\n",
      "model.encoder.layers.0.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.0.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.0.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.0.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.0.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.0.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.0.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.0.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.1.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.1.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.1.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.1.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.1.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.1.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.1.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.1.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.2.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.2.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.2.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.2.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.2.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.2.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.2.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.2.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.3.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.3.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.3.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.3.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.3.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.3.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.3.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.3.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.4.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.4.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.4.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.4.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.4.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.4.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.4.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.4.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.5.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.5.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.5.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.5.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.5.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.5.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.5.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.5.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layernorm_embedding.bias is trainable...(768)\n",
      "model.encoder.visual_embedding.feat_embedding.0.bias is trainable...(768)\n",
      "model.encoder.visual_embedding.feat_embedding.1.bias is trainable...(768)\n",
      "model.decoder.layers.0.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.0.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.0.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.0.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.0.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.1.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.1.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.1.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.1.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.1.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.2.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.2.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.2.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.2.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.2.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.3.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.3.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.3.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.3.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.3.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.4.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.4.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.4.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.4.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.4.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.5.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.5.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.5.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.5.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.5.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layernorm_embedding.bias is trainable...(768)\n",
      "model.encoder.layers.0.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.0.final_layer_norm is trainable...\n",
      "model.encoder.layers.1.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.1.final_layer_norm is trainable...\n",
      "model.encoder.layers.2.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.2.final_layer_norm is trainable...\n",
      "model.encoder.layers.3.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.3.final_layer_norm is trainable...\n",
      "model.encoder.layers.4.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.4.final_layer_norm is trainable...\n",
      "model.encoder.layers.5.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.5.final_layer_norm is trainable...\n",
      "model.encoder.layernorm_embedding is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.1 is trainable...\n",
      "model.decoder.layers.0.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.0.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.0.final_layer_norm is trainable...\n",
      "model.decoder.layers.1.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.1.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.1.final_layer_norm is trainable...\n",
      "model.decoder.layers.2.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.2.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.2.final_layer_norm is trainable...\n",
      "model.decoder.layers.3.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.3.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.3.final_layer_norm is trainable...\n",
      "model.decoder.layers.4.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.4.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.4.final_layer_norm is trainable...\n",
      "model.decoder.layers.5.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.5.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.5.final_layer_norm is trainable...\n",
      "model.decoder.layernorm_embedding is trainable...\n",
      "VLBartVQA(\n",
      "  (model): VLBartModel(\n",
      "    (shared): Embedding(50465, 768)\n",
      "    (encoder): JointEncoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (visual_embedding): VisualEmbedding(\n",
      "        (feat_embedding): Sequential(\n",
      "          (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (downsample): Downsample(\n",
      "        (pool): AdaptiveMaxPool2d(output_size=(6, 6))\n",
      "      )\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50465, bias=False)\n",
      "  (intervenable): ReftModel(\n",
      "    (model): VLBartModel(\n",
      "      (shared): Embedding(50465, 768)\n",
      "      (encoder): JointEncoder(\n",
      "        (embed_tokens): Embedding(50465, 768)\n",
      "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "        (layers): ModuleList(\n",
      "          (0-5): 6 x BartEncoderLayer(\n",
      "            (self_attn): BartAttention(\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (activation_fn): GELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (visual_embedding): VisualEmbedding(\n",
      "          (feat_embedding): Sequential(\n",
      "            (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "            (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (downsample): Downsample(\n",
      "          (pool): AdaptiveMaxPool2d(output_size=(6, 6))\n",
      "        )\n",
      "      )\n",
      "      (decoder): BartDecoder(\n",
      "        (embed_tokens): Embedding(50465, 768)\n",
      "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "        (layers): ModuleList(\n",
      "          (0-5): 6 x BartDecoderLayer(\n",
      "            (self_attn): BartAttention(\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (activation_fn): GELUActivation()\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (encoder_attn): BartAttention(\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (bce_loss): BCEWithLogitsLoss()\n",
      ")\n",
      "Trainable param percentage: 2.09% (2979888/142403376)\n",
      "Building Optimizer\n",
      "Batch per epoch: 100\n",
      "Total Iters: 300\n",
      "Warmup ratio: 0.1\n",
      "Warm up Iters: 30\n",
      "It took 0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/peterwz/miniconda3/envs/peterwz-dora/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeterzw494\u001b[0m (\u001b[33mpeterwz\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sailhome/peterwz/workspace/pyreft/examples/vlbart/ReftDora/image_video_text_understanding/VL-T5/src/wandb/run-20240708_171816-ld0th829</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peterwz/Test/runs/ld0th829' target=\"_blank\">misunderstood-planet-109</a></strong> to <a href='https://wandb.ai/peterwz/Test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peterwz/Test' target=\"_blank\">https://wandb.ai/peterwz/Test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peterwz/Test/runs/ld0th829' target=\"_blank\">https://wandb.ai/peterwz/Test/runs/ld0th829</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 0 file into the W&B run directory, call wandb.save again to sync new files.\n",
      "  0%|                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]/nlp/scr/peterwz/miniconda3/envs/peterwz-dora/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/nlp/scr/peterwz/miniconda3/envs/peterwz-dora/lib/python3.8/site-packages/pyvene/models/modeling_utils.py:288: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  unit_locations = torch.tensor(\n",
      "/nlp/scr/peterwz/miniconda3/envs/peterwz-dora/lib/python3.8/site-packages/pyvene/models/modeling_utils.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  unit_locations = torch.tensor(\n",
      "Epoch 0 | LR 0.000741 | VQA 100 | VQA Loss 1.298110: 100%|| 100/100 [01:37<00:00,  1.02it/s]\n",
      "VQA Validation:   0%|                                                                                                           | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In dataset:In dataset:In dataset:In dataset:   ['vqa: Will this kid leave the powdered sugar on his face?'] ['vqa: Is this person wearing a tie?']['vqa: What color is the kids hair?']['vqa: What is the child eating?']        [393243003] [393243000] [393243001]\n",
      "\n",
      " \n",
      " [393243002]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/peterwz/miniconda3/envs/peterwz-dora/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "/nlp/scr/peterwz/miniconda3/envs/peterwz-dora/lib/python3.8/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "VQA Validation: 100%|| 4/4 [00:02<00:00,  1.43it/s]\n",
      "100%|| 4/4 [00:00<00:00, 5751.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 0: Valid Raw 47.50 Topk 47.50\n",
      "Epoch 0: Best Raw 47.50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 0 is less than current step: 1. Dropping entry: {'VQA/Valid/score': 47.5, 'VQA/Valid/raw_score': 47.5, '_timestamp': 1720484405.6579585}).\n",
      "Epoch 1 | LR 0.000370 | VQA 100 | VQA Loss 0.760457: 100%|| 100/100 [01:23<00:00,  1.20it/s]\n",
      "VQA Validation:   0%|                                                                                                           | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In dataset:In dataset:In dataset: ['vqa: Will this kid leave the powdered sugar on his face?']  In dataset:['vqa: Is this person wearing a tie?'] ['vqa: What is the child eating?']     [393243000]   \n",
      " ['vqa: What color is the kids hair?'][393243001] [393243002]\n",
      " \n",
      " [393243003]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VQA Validation: 100%|| 4/4 [00:02<00:00,  1.61it/s]\n",
      "100%|| 4/4 [00:00<00:00, 3307.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 1: Valid Raw 25.00 Topk 25.00\n",
      "Epoch 0: Best Raw 47.50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 | LR 0.000000 | VQA 100 | VQA Loss 0.685865: 100%|| 100/100 [01:22<00:00,  1.21it/s]\n",
      "VQA Validation:   0%|                                                                                                           | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In dataset:In dataset:In dataset:In dataset:   ['vqa: Will this kid leave the powdered sugar on his face?'] ['vqa: Is this person wearing a tie?']['vqa: What is the child eating?']    ['vqa: What color is the kids hair?']    [393243002]  \n",
      "[393243000][393243001] \n",
      "\n",
      " [393243003]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VQA Validation: 100%|| 4/4 [00:02<00:00,  1.57it/s]\n",
      "100%|| 4/4 [00:00<00:00, 3821.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 2: Valid Raw 22.50 Topk 22.50\n",
      "Epoch 0: Best Raw 47.50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer(args, train_loader, val_loader, test_loader, train=True)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a2bfd3-2e12-4141-95bc-b15e77317f6f",
   "metadata": {},
   "source": [
    "### 5. Demo\n",
    "Now let's see what are the test results of our trained ReFT model!\n",
    "Below is one image that we used in our testing set. Download the image annotations at [this link](http://images.cocodataset.org/annotations/annotations_trainval2014.zip). If your random seed is different, the image IDs in your test set is different. In this case, you can change the Image ID below. The above cell has printed out the question IDs corresponding to the questions. The COCO image ID of the question is the first 6 digits of the question ID. For example, the Image ID of question \"262148000\" is \"262148\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa78825-8cce-4d88-b34d-d90d15df6d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = \"../../datasets/COCO/images/annotations/captions_val2014.json\"\n",
    "\n",
    "# Initialize COCO API for instance annotations\n",
    "coco = COCO(path)\n",
    "\n",
    "# Specify the image ID\n",
    "image_id = 262148  # Replace with your COCO image ID\n",
    "\n",
    "# Get image info\n",
    "img_info = coco.loadImgs(image_id)[0]\n",
    "\n",
    "# Get the image URL\n",
    "img_url = img_info['coco_url']\n",
    "\n",
    "# Download the image\n",
    "response = requests.get(img_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b6513a-4625-4c82-adf3-91656b480c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the image ID\n",
    "image_id = 131089  # Replace with your COCO image ID\n",
    "\n",
    "# Get image info\n",
    "img_info = coco.loadImgs(image_id)[0]\n",
    "\n",
    "# Get the image URL\n",
    "img_url = img_info['coco_url']\n",
    "\n",
    "# Download the image\n",
    "response = requests.get(img_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb7810f-7b11-4fb7-952e-96e6dea0e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_test_loader = test_loader['vqa']\n",
    "evaluator = vqa_test_loader.evaluator\n",
    "dump_path = os.path.join(args.output, 'karpathy_test_predict.json')\n",
    "quesid2ans = trainer.vqa_predict(vqa_test_loader, dump_path)\n",
    "\n",
    "acc_dict_all = evaluator.evaluate_raw(quesid2ans)\n",
    "acc_dict_answerable = evaluator.evaluate_raw(quesid2ans, is_topk_optimal=True)\n",
    "acc_dict_unanswerable = evaluator.evaluate_raw(quesid2ans, is_topk_optimal=False)\n",
    "print(acc_dict_all, acc_dict_answerable, acc_dict_unanswerable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1af82a-030a-4a89-bef2-f33c89dfcaef",
   "metadata": {},
   "source": [
    "Below are the ReFT model's answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51435aa3-29c2-4f0d-a617-f6c312c4975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quesid2ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b12ce28-d689-4a80-888d-c66b342c221b",
   "metadata": {},
   "source": [
    "You can see that 3 epochs of training already gave ReFT lots of understanding about the image features. For example, for the skateboarding picture, ReFT correctly answers that the person is looking \"on the ground\", and the person is on top of a \"skateboard\" (although also on top of the dining table). However, ReFT model fails to answer \"What are the people in the background doing\" - ReFT considers them as skateboarding too. ReFT also misidentifies the color of the grass as \"brown\". \n",
    "\n",
    "These failure cases potentially show that simply rotating the language model representations are insufficient at identifying the niche relationships between image elements, which potentially are out of the distribution of the training model. Also, a lot of these capabilities might have been picked up while tuning the visual embedding. Potentially tuning more of the visual embedding in compensation to the capability loss of ReFT on images may be a good way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e7c4c5-d333-43a7-96f6-07faa277f266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
