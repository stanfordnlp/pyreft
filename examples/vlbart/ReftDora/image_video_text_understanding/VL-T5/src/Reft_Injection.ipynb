{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd53f5e-8059-4338-bbb4-88abc6202208",
   "metadata": {},
   "source": [
    "## Reft + VLBart experiments\n",
    "\n",
    "Does ReFT works with Vision-language? Let's find out with the VQA Task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0156e327-4da1-41e3-87c8-8bf5fec3d1c0",
   "metadata": {},
   "source": [
    "I replicated the following code mostly from src/multitask.py."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbfcc23-040f-43ba-8cf3-c3cb971cf3f4",
   "metadata": {},
   "source": [
    "### 1.1 Reft Model Replica for VLBart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb906f7-b8ba-42ee-a114-bc1dee0378ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer_base import TrainerBase\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import os\n",
    "import collections\n",
    "from pathlib import Path\n",
    "from packaging import version\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import shutil\n",
    "from pprint import pprint, pformat\n",
    "from copy import deepcopy\n",
    "\n",
    "from param import parse_args\n",
    "\n",
    "\n",
    "import vqa\n",
    "import gqa\n",
    "import nlvr\n",
    "import vcr\n",
    "import caption\n",
    "import mmt\n",
    "import refcoco\n",
    "\n",
    "import multitask_data\n",
    "\n",
    "from utils import LossMeter, set_global_logging_level\n",
    "from dist_utils import reduce_dict\n",
    "import wandb\n",
    "\n",
    "from vis_encoder import get_vis_encoder\n",
    "from transformers.models.t5.modeling_t5 import T5LayerNorm\n",
    "import modeling_t5\n",
    "import modeling_bart\n",
    "from clip.model import VisualAdapter\n",
    "from ddp_fix import ddp_forward\n",
    "\n",
    "from adapters import AdapterController, MetaLayersAdapterController\n",
    "\n",
    "proj_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "\n",
    "\n",
    "_use_native_amp = False\n",
    "_use_apex = False\n",
    "\n",
    "# Check if Pytorch version >= 1.6 to switch between Native AMP and Apex\n",
    "if version.parse(torch.__version__) < version.parse(\"1.6\"):\n",
    "    from transormers.file_utils import is_apex_available\n",
    "    if is_apex_available():\n",
    "        from apex import amp\n",
    "    _use_apex = True\n",
    "else:\n",
    "    _use_native_amp = True\n",
    "    from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27b0012c-e0e5-47b1-8f0f-1d0c7debf32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers.models.bart.modeling_bart import (\n",
    "    BartLearnedPositionalEmbedding,\n",
    "    BartEncoderLayer,\n",
    "    BartPretrainedModel,\n",
    "    BartConfig,\n",
    "    ACT2FN,\n",
    "    shift_tokens_right, _make_causal_mask, _expand_mask\n",
    ")\n",
    "\n",
    "from my_transformers.modeling_bart import BartModel, BartForConditionalGeneration, BartDecoder, BartEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple\n",
    "import copy\n",
    "\n",
    "from transformers.modeling_outputs import ModelOutput, BaseModelOutput, BaseModelOutputWithPast, BaseModelOutputWithPastAndCrossAttentions, Seq2SeqLMOutput, Seq2SeqModelOutput\n",
    "from transformers.modeling_utils import PreTrainedModel, find_pruneable_heads_and_indices, prune_linear_layer\n",
    "from transformers.utils import logging\n",
    "from transformers import BeamScorer, BeamSearchScorer\n",
    "\n",
    "from adapters import (\n",
    "    AdapterLayer, \n",
    "    AdapterController,\n",
    "    OutputParallelAdapterLayer, \n",
    "    TaskEmbeddingController,\n",
    "    AdapterLayersHyperNetController,\n",
    "    AdapterLayersOneHyperNetController,\n",
    "    MetaLayersAdapterController\n",
    ")\n",
    "\n",
    "from adapters.hypercomplex.layers import PHMLinear\n",
    "\n",
    "from prompt import (\n",
    "    PromptController,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "from modeling_bart import VLBart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790d9a60-89b4-4ea2-802f-db9c23e73bac",
   "metadata": {},
   "source": [
    "Here we replace the `VLBart` class in the vladapter/DoRA codebase with VLBartReft. Which wraps the models into Pyvene intervenables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d574fd3-aa93-4dc9-a7e8-8a4f23e15da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLBartReft(VLBart):\n",
    "    def __init__(self, config: BartConfig):\n",
    "        super().__init__(config)\n",
    "        from pyreft import get_reft_model\n",
    "        self.intervenable = get_reft_model(self.model, config.reft_config)\n",
    "        # print(\"Reft parameters:\", self.intervenable.interventions)\n",
    "        # self.intervenable.unfreeze_intervention_parameters()\n",
    "        self.intervenable.print_trainable_parameters()\n",
    "        # print(\"INTERVENABLE:\", self.intervenable.model)\n",
    "\n",
    "        # Unfreeze the PyVene intervention parameters\n",
    "        for k, v in self.intervenable.unfreeze_intervention_parameters().items():\n",
    "            n = k.replace(\".\", \"#\")\n",
    "            print(n)\n",
    "            self.register_parameter(n, v)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "\n",
    "        vis_inputs=None,\n",
    "        vis_attention_mask=None,\n",
    "\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        task=None,\n",
    "\n",
    "        reduce_loss=False,\n",
    "        intervention_locations = None,\n",
    "\n",
    "\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if labels is not None:\n",
    "            if decoder_input_ids is None:\n",
    "                decoder_input_ids = shift_tokens_right(\n",
    "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
    "                )\n",
    "        \n",
    "        if intervention_locations is not None:\n",
    "            # print(\"Intervention locs not None\")\n",
    "            # Pyvene forward pass\n",
    "            intervention_locations = intervention_locations.clone().detach().permute(1, 0, 2)\n",
    "            _, outputs = self.intervenable(\n",
    "                {\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"vis_inputs\": vis_inputs,\n",
    "                    \"vis_attention_mask\": vis_attention_mask,\n",
    "                    \"decoder_input_ids\": decoder_input_ids,\n",
    "                    \"decoder_attention_mask\": decoder_attention_mask,\n",
    "                    \"encoder_outputs\": encoder_outputs,\n",
    "                    \"past_key_values\": past_key_values,\n",
    "                    \"inputs_embeds\": inputs_embeds,\n",
    "                    \"decoder_inputs_embeds\": decoder_inputs_embeds,\n",
    "                    \"output_attentions\": output_attentions,\n",
    "                    \"output_hidden_states\": output_hidden_states,\n",
    "                    \"task\": task,\n",
    "                    \"return_dict\": return_dict,\n",
    "                },\n",
    "                unit_locations={\"sources->base\": (\n",
    "                    None,\n",
    "                    intervention_locations\n",
    "                )},\n",
    "                labels=labels,\n",
    "                return_dict=False,\n",
    "                subspaces=None,\n",
    "                use_cache=use_cache,\n",
    "            )\n",
    "        else:\n",
    "            # print(\"Intervention locs None\")\n",
    "            outputs = self.model(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "\n",
    "                vis_inputs=vis_inputs,\n",
    "                vis_attention_mask=vis_attention_mask,\n",
    "\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                decoder_attention_mask=decoder_attention_mask,\n",
    "                past_key_values=past_key_values,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                task=task,\n",
    "            )\n",
    "\n",
    "        # print(\"Outputs:\", outputs)\n",
    "        lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n",
    "\n",
    "        if self.output_adapter is not None:\n",
    "            lm_logits = lm_logits + self.output_adapter(outputs[0])\n",
    "        \n",
    "        masked_lm_loss = None\n",
    "        # print(\"LOGITS:\", lm_logits)\n",
    "        # print(\"LABELS\", labels)\n",
    "        if labels is not None:\n",
    "            # loss_fct = CrossEntropyLoss()\n",
    "            if reduce_loss:\n",
    "                loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss(ignore_index=-100, reduction='none')\n",
    "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            \n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + outputs[1:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "        # if masked_lm_loss is not None and len(masked_lm_loss) > 1:\n",
    "        #     masked_lm_loss = masked_lm_loss[0]\n",
    "        # print(\"LOSS 0:\", masked_lm_loss)\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
    "            decoder_attentions=outputs.decoder_attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
    "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
    "            encoder_attentions=outputs.encoder_attentions,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609328e2-5618-4e2e-a48d-c36829291bc5",
   "metadata": {},
   "source": [
    "This class is the same as `VLBartVQA` in vqa_model.py, except that we inherit from `VLBartReft`, and we adapt to Pyvene generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e885ce0-d164-44b9-883c-3cedbbbd0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLBartVQA(VLBartReft):\n",
    "    def __init__(self, config, num_answers=None, label2ans=None):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if config.classifier:\n",
    "            self.answer_head = nn.Sequential(\n",
    "                nn.Linear(config.d_model, config.d_model * 2),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(config.d_model * 2),\n",
    "                nn.Linear(config.d_model * 2, num_answers)\n",
    "            )\n",
    "\n",
    "        self.num_answers = num_answers\n",
    "        self.label2ans = label2ans\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def train_step(self, batch):\n",
    "\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        batch = self.vis_forward(batch, device)\n",
    "        task = batch[\"task\"]\n",
    "\n",
    "        vis_feats = batch['vis_feats'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        vis_pos = batch['boxes'].to(device)\n",
    "        intervention_locations = batch['intervention_locations'].to(device)\n",
    "\n",
    "        if self.config.classifier:\n",
    "            B = len(input_ids)\n",
    "\n",
    "            decoder_input_ids = torch.tensor(\n",
    "                [self.config.decoder_start_token_id, self.config.bos_token_id],\n",
    "                dtype=torch.long, device=device).unsqueeze(0).expand(B, 2)\n",
    "\n",
    "            output = self(\n",
    "                input_ids=input_ids,\n",
    "                vis_inputs=(vis_feats, vis_pos),\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                output_hidden_states=True,\n",
    "                return_dict=True,\n",
    "                task=task,\n",
    "                intervention_locations=intervention_locations\n",
    "            )\n",
    "\n",
    "            target = batch['targets'].to(device)\n",
    "\n",
    "            last_layer_hidden_state = output.decoder_hidden_states[-1]\n",
    "            last_hidden_state = last_layer_hidden_state.view(B, -1, self.config.d_model)[:, -1]\n",
    "\n",
    "            # [B, num_answers]\n",
    "            logit = self.answer_head(last_hidden_state)\n",
    "\n",
    "            loss = self.bce_loss(logit, target)\n",
    "\n",
    "        else:\n",
    "            lm_labels = batch[\"target_ids\"].to(device)\n",
    "\n",
    "            output = self(\n",
    "                input_ids=input_ids,\n",
    "                vis_inputs=(vis_feats, vis_pos),\n",
    "                labels=lm_labels,\n",
    "                return_dict=True,\n",
    "                task=task,\n",
    "                intervention_locations=intervention_locations\n",
    "            )\n",
    "            assert 'loss' in output\n",
    "\n",
    "            lm_mask = (lm_labels != -100).float()\n",
    "            B, L = lm_labels.size()\n",
    "\n",
    "            loss = output['loss']\n",
    "\n",
    "            loss = loss.view(B, L) * lm_mask\n",
    "\n",
    "            loss = loss.sum(dim=1) / lm_mask.sum(dim=1).clamp(min=1)  # B\n",
    "\n",
    "            loss = loss * batch['scores'].to(device=device)\n",
    "\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        # print(\"LOSS 1:\", batch[\"scores\"], loss.item())\n",
    "        result = {\n",
    "            'loss': loss\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_step(self, batch, **kwargs):\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        batch = self.vis_forward(batch, device)\n",
    "\n",
    "        vis_feats = batch['vis_feats'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        vis_pos = batch['boxes'].to(device)\n",
    "        task = batch[\"task\"]\n",
    "        intervention_locations = batch['intervention_locations'].to(device)\n",
    "\n",
    "        result = {}\n",
    "        if self.config.classifier:\n",
    "            B = len(input_ids)\n",
    "\n",
    "            decoder_input_ids = torch.tensor(\n",
    "                [self.config.decoder_start_token_id, self.config.bos_token_id],\n",
    "                dtype=torch.long, device=device).unsqueeze(0).expand(B, 2)\n",
    "\n",
    "            output = self(\n",
    "                input_ids=input_ids,\n",
    "                vis_inputs=(vis_feats, vis_pos),\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                output_hidden_states=True,\n",
    "                return_dict=True,\n",
    "                task=task,\n",
    "                intervention_locations=intervention_locations\n",
    "            )\n",
    "\n",
    "            last_layer_hidden_state = output.decoder_hidden_states[-1]\n",
    "            last_hidden_state = last_layer_hidden_state.view(B, -1, self.config.d_model)[:, -1]\n",
    "\n",
    "            # [B, num_answers]\n",
    "            logit = self.answer_head(last_hidden_state)\n",
    "\n",
    "            score, pred_ans_id = logit.max(1)\n",
    "            pred_ans_id = pred_ans_id.cpu().numpy()\n",
    "            pred_ans = [self.label2ans[ans_id] for ans_id in pred_ans_id]\n",
    "\n",
    "            result['pred_ans'] = pred_ans\n",
    "\n",
    "        else:\n",
    "            generation_args = {\n",
    "                \"base\": {\n",
    "                    \"input_ids\":input_ids,\n",
    "                    \"vis_inputs\":(vis_feats, vis_pos),\n",
    "                    \"task\":task,\n",
    "                    **kwargs\n",
    "                },\n",
    "                \"unit_locations\": {\"sources->base\": (None, \n",
    "                    intervention_locations.permute(1, 0, 2))},\n",
    "                \"intervene_on_prompt\": True,\n",
    "                \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "                \"early_stopping\": True,\n",
    "                \"model\": self,\n",
    "            }\n",
    "            # print(\"Generating...\", input_ids.shape, intervention_locations)\n",
    "            # TODO: temperature, top_p, top_k\n",
    "            # print(\"GENERATE MODEL:\", self.intervenable.model)\n",
    "            _, output = self.intervenable.generate(**generation_args)\n",
    "            generated_sents = self.tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "            result['token_ids'] = output\n",
    "            result['pred_ans'] = generated_sents\n",
    "\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a217649-3971-4fc4-8a9b-cc1c06b4bc5e",
   "metadata": {},
   "source": [
    "### 1.2 Multitask VLBart Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10cc634e-30bc-4c1b-be4b-7850b988284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLBartMultiTask(VLBartReft):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def train_step(self, batch, **kwargs):\n",
    "        task = batch['task']\n",
    "        if task == 'vqa':\n",
    "            return VLBartVQA.train_step(self, batch, **kwargs)\n",
    "\n",
    "    def valid_step(self, batch, **kwargs):\n",
    "        task = batch['task']\n",
    "        if task == 'vqa':\n",
    "            return VLBartVQA.valid_step(self, batch, **kwargs)\n",
    "\n",
    "    def test_step(self, batch, **kwargs):\n",
    "        task = batch['task']\n",
    "        if task == 'vqa':\n",
    "            return VLBartVQA.test_step(self, batch, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4684f989-0756-47f1-a82b-305d4a3d38c4",
   "metadata": {},
   "source": [
    "### 1.3 Multitask VLBart Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba702d3-ab30-4e98-acc7-54a2e288414b",
   "metadata": {},
   "source": [
    "### 1.3.1 Reft Specific Trainer\n",
    "\n",
    "Here we create `ReftConfig` for ReFT to create its appropriate interventions. Also, we change the weight decay of ReFT parameters to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3a9aa16-891b-4b60-a536-c3dae3fbc573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyreft import ReftConfig, LoreftIntervention, TaskType\n",
    "\n",
    "class ReftTrainer(TrainerBase):\n",
    "    def __init__(self, args, train_loader=None, val_loader=None, test_loader=None, train=True):\n",
    "        super().__init__(\n",
    "            args,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "            train=train)\n",
    "\n",
    "    def create_reft_config(self, config):\n",
    "        args = self.args\n",
    "        layers = args.layers\n",
    "        # ReFT layers - right now only \"all\" works properly\n",
    "        # TODO: properly process \"layers\" when it is not \"all\"\n",
    "        if layers != \"all\":\n",
    "            layers = [int(l) for l in layers.split(\";\")]\n",
    "        else:\n",
    "            # TODO: verify config's hidden layers field\n",
    "            layers = [l for l in range(config.num_hidden_layers)]\n",
    "        if '+' in self.args.positions and not args.share_weights:\n",
    "            layers += layers\n",
    "        \n",
    "        image_rank = args.reft_image_rank\n",
    "        text_rank = args.reft_rank\n",
    "        embed_dim = args.mid_dim\n",
    "\n",
    "        # print(\"REFT PARAMS:\",embed_dim, rank, args.dropout)\n",
    "        representations = []\n",
    "        # Text interventions\n",
    "        if text_rank != -1:\n",
    "            representations += [{\n",
    "                \"layer\": l, \"component\": \"block_output\",\n",
    "                \"low_rank_dimension\": text_rank,\n",
    "                \"intervention\": LoreftIntervention(\n",
    "                    embed_dim=embed_dim, low_rank_dimension=text_rank,\n",
    "                    dropout=args.reft_dropout, dtype=torch.float32, act_fn=None, device=\"cuda\",\n",
    "                    add_bias=True\n",
    "                )\n",
    "            } for l in layers]\n",
    "        # Image interventions\n",
    "        if image_rank != -1:\n",
    "            representations += [{\n",
    "                \"layer\": l, \"component\": \"block_output\",\n",
    "                \"low_rank_dimension\": image_rank,\n",
    "                \"intervention\": LoreftIntervention(\n",
    "                    embed_dim=embed_dim, low_rank_dimension=image_rank,\n",
    "                    dropout=args.reft_image_dropout, dtype=torch.float32, act_fn=None, device=\"cuda\",\n",
    "                    add_bias=True\n",
    "                )\n",
    "            } for l in layers]\n",
    "        reft_config = ReftConfig(representations=representations)\n",
    "        print(reft_config)\n",
    "        return reft_config\n",
    "\n",
    "    def create_config(self):\n",
    "        config = super().create_config()\n",
    "        setattr(config, \"reft_config\", self.create_reft_config(config))\n",
    "        return config\n",
    "\n",
    "    def create_optimizer_and_scheduler(self):\n",
    "        if self.verbose:\n",
    "            print('Building Optimizer')\n",
    "\n",
    "        lr_scheduler = None\n",
    "\n",
    "        from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "        # Added \"#unit#pos\" to `no_decay` to keep ReFT intervention's weight decay to 0\n",
    "        # Bart's bias and layer norm's weight decay is 0, others are not zero \n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\", \"#unit#pos\"]\n",
    "\n",
    "        if 'adamw' in self.args.optim:\n",
    "\n",
    "            if self.args.use_separate_optimizer_for_visual:\n",
    "                \n",
    "                # transformer's parameters\n",
    "                optimizer_grouped_parameters = [\n",
    "                    {\n",
    "                        \"params\": [p for n, p in self.model.named_parameters() if ( (not any(nd in n for nd in no_decay)) and (\"vis_encoder\" not in n) ) ],\n",
    "                        \"weight_decay\": self.args.weight_decay,\n",
    "                        \"lr\": self.args.lr,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": [p for n, p in self.model.named_parameters() if ( (any(nd in n for nd in no_decay)) and (\"vis_encoder\" not in n ))],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                        \"lr\": self.args.lr,\n",
    "                    },\n",
    "                ]\n",
    "                \n",
    "                visn_model = self.model.vis_encoder\n",
    "                if self.args.use_adam_for_visual:\n",
    "\n",
    "                    vis_optimizer_grouped_parameters = [\n",
    "                        {\n",
    "                            \"params\": [p for n, p in visn_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                            \"weight_decay\": self.args.vis_weight_decay,\n",
    "                            \"lr\": self.args.vis_lr,\n",
    "                        },\n",
    "                        {\n",
    "                            \"params\": [p for n, p in visn_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                            \"weight_decay\": 0.0,\n",
    "                            \"lr\": self.args.vis_lr,\n",
    "                        },\n",
    "                    ]\n",
    "                    optim = AdamW(\n",
    "                        optimizer_grouped_parameters + vis_optimizer_grouped_parameters,\n",
    "                        lr=self.args.lr,\n",
    "                        # betas=(0.9, 0.98),\n",
    "                        eps=self.args.adam_eps\n",
    "                    )\n",
    "                else:\n",
    "                    optim = AdamW(\n",
    "                        optimizer_grouped_parameters, lr=self.args.lr, eps=self.args.adam_eps\n",
    "                    )\n",
    "                    vis_optim = torch.optim.SGD(\n",
    "                        visn_model.parameters(), \n",
    "                        self.args.vis_lr,\n",
    "                        momentum=0,\n",
    "                        weight_decay=self.args.vis_weight_decay\n",
    "                    )\n",
    "\n",
    "                    optim = FusedOptimizer([optim, vis_optim])\n",
    "\n",
    "            else:\n",
    "                # for n, _ in self.model.named_parameters():\n",
    "                #     print(\"Parameter: \", n)\n",
    "                # print(\"=======\")\n",
    "                # for n, _ in self.model.intervenable.unfreeze_intervention_parameters().items():\n",
    "                #     print(\"Parameter: \", n)\n",
    "                optimizer_grouped_parameters = [\n",
    "                    {\n",
    "                        \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                        \"weight_decay\": self.args.weight_decay,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                    },\n",
    "                    # {\n",
    "                    #     \"params\": [p for n, p in self.model.intervenable.unfreeze_intervention_parameters().items()],\n",
    "                    #     \"weight_decay\": self.args.weight_decay,\n",
    "                    # }\n",
    "                ]\n",
    "                optim = AdamW(optimizer_grouped_parameters,\n",
    "                            lr=self.args.lr, eps=self.args.adam_eps)\n",
    "\n",
    "        else:\n",
    "            # print(\"Parameters:\", self.model.named_parameters())\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                    \"weight_decay\": self.args.weight_decay,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "                # {\n",
    "                #     \"params\": self.model.intervenable.unfreeze_intervention_parameters(),\n",
    "                #     \"weight_decay\": self.args.weight_decay,\n",
    "                # }\n",
    "            ]\n",
    "\n",
    "            # if self.include_vis_encoder:\n",
    "            #     trainable_parameters = trainable_parameters + list(self.vis_encoder.parameters())\n",
    "\n",
    "            optim = self.args.optimizer(optimizer_grouped_parameters, self.args.lr)\n",
    "\n",
    "        batch_per_epoch = len(self.train_loader)\n",
    "        t_total = batch_per_epoch // self.args.gradient_accumulation_steps * self.args.epochs\n",
    "        warmup_ratio = self.args.warmup_ratio\n",
    "        warmup_iters = int(t_total * warmup_ratio)\n",
    "        if self.verbose:\n",
    "            print(\"Batch per epoch: %d\" % batch_per_epoch)\n",
    "            print(\"Total Iters: %d\" % t_total)\n",
    "            print('Warmup ratio:', warmup_ratio)\n",
    "            print(\"Warm up Iters: %d\" % warmup_iters)\n",
    "\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(optim, warmup_iters, t_total)\n",
    "\n",
    "        return optim, lr_scheduler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5708c2-6eed-4162-b5a3-bab4d1288020",
   "metadata": {},
   "source": [
    "### 1.3.2 Reft images trainer\n",
    "\n",
    "Only thing we changed in this class compare to `trainer_base.py` is changing the intervenable model to Cuda, and unfreeze Reft parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "495a45bf-dfdb-4554-ada9-9393ac69406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(ReftTrainer):\n",
    "    def __init__(self, args, train_loader=None, val_loader=None, test_loader=None, train=True):\n",
    "        super().__init__(\n",
    "            args,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "            train=train)\n",
    "\n",
    "        if not self.verbose:\n",
    "            set_global_logging_level(logging.ERROR, [\"transformers\"])\n",
    "\n",
    "        model_kwargs = {}\n",
    "        if 'bart' in args.backbone:\n",
    "            model_class = VLBartMultiTask\n",
    "\n",
    "        config = self.create_config()\n",
    "        self.tokenizer = self.create_tokenizer()\n",
    "\n",
    "        if 'bart' in self.args.tokenizer:\n",
    "            num_added_toks = 0\n",
    "            if config.use_vis_order_embedding:\n",
    "                additional_special_tokens = [f'<extra_id_{i}>' for i in range(100-1, -1, -1)] + \\\n",
    "                        [f'<vis_extra_id_{i}>' for i in range(100-1, -1, -1)]\n",
    "                special_tokens_dict = {'additional_special_tokens': additional_special_tokens}\n",
    "                num_added_toks = self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "                config.default_obj_order_ids = self.tokenizer.convert_tokens_to_ids([f'<vis_extra_id_{i}>' for i in range(100)])\n",
    "\n",
    "        self.model = self.create_model(model_class, config, **model_kwargs)\n",
    "\n",
    "        if 't5' in self.args.tokenizer:\n",
    "            self.model.resize_token_embeddings(self.tokenizer.vocab_size)\n",
    "        elif 'bart' in self.args.tokenizer:\n",
    "            self.model.resize_token_embeddings(self.model.model.shared.num_embeddings + num_added_toks)\n",
    "\n",
    "        self.model.tokenizer = self.tokenizer\n",
    "        if 't5' in self.args.tokenizer or 'bart' in self.args.tokenizer:\n",
    "            self.model.true_id = self.tokenizer('true', add_special_tokens=False).input_ids[0]\n",
    "            self.model.false_id = self.tokenizer('false', add_special_tokens=False).input_ids[0]\n",
    "\n",
    "        if self.include_vis_encoder:\n",
    "            # train vision encoder end-to-end\n",
    "            vis_encoder_type = self.args.feature_type.split(\"_\")[-1]\n",
    "\n",
    "            if self.args.use_vis_adapter:\n",
    "                self.vis_encoder = get_vis_encoder(\n",
    "                    backbone=vis_encoder_type, \n",
    "                    image_size=eval(self.args.image_size)[0],\n",
    "                    adapter_type=self.args.vis_adapter_type,\n",
    "                    reduction_factor=self.args.vis_reduction_factor,\n",
    "                    use_bn=not self.args.remove_bn_vis_adapter,\n",
    "                )\n",
    "            else:\n",
    "                self.vis_encoder = get_vis_encoder(\n",
    "                    backbone=vis_encoder_type, \n",
    "                    image_size=eval(self.args.image_size)[0],\n",
    "                    adapter_type=None,\n",
    "                )\n",
    "\n",
    "            print(\"include vision encoder\")\n",
    "            self.model.vis_encoder = self.vis_encoder\n",
    "            print(self.model)\n",
    "        # Load Checkpoint\n",
    "        self.start_epoch = None\n",
    "        if args.load is not None:\n",
    "            ckpt_path = args.load\n",
    "            self.load_checkpoint(ckpt_path)\n",
    "        if self.args.from_scratch:\n",
    "            self.init_weights()\n",
    "\n",
    "        # GPU Options\n",
    "        print(f'Model Launching at GPU {self.args.gpu}')\n",
    "        if self.verbose:\n",
    "            from time import time\n",
    "            start = time()\n",
    "        self.model = self.model.to(args.gpu)\n",
    "        \n",
    "        # Only thing changed: set device to cuda, and unfreeze ReFT params\n",
    "\n",
    "        self.model.intervenable.set_device(self.model.model.device)\n",
    "\n",
    "        self.freeze_whole_model() # freeze whole parameters first\n",
    "        self.unfreeze_parameters() # unfreeze selected parameters\n",
    "        self.model.intervenable.unfreeze_intervention_parameters()\n",
    "        # print(self.model)\n",
    "        self.percent_updated_parameters = self.print_trainable_params_percentage(self.model)\n",
    "\n",
    "        # Optimizer\n",
    "        if train:\n",
    "            self.optim, self.lr_scheduler = self.create_optimizer_and_scheduler()\n",
    "\n",
    "            if self.args.fp16 and _use_native_amp:\n",
    "                self.scaler = torch.cuda.amp.GradScaler()\n",
    "            elif _use_apex:\n",
    "                self.model, self.optim = amp.initialize(\n",
    "                    self.model, self.optim, opt_level='O1', verbosity=self.verbose)\n",
    "\n",
    "        if args.multiGPU:\n",
    "            if args.distributed:\n",
    "                self.model = DDP(self.model, device_ids=[args.gpu],\n",
    "                                 find_unused_parameters=True\n",
    "                                 )\n",
    "        if self.verbose:\n",
    "            print(f'It took {time() - start:.1f}s')\n",
    "\n",
    "    def train(self):\n",
    "        if self.verbose:\n",
    "            vqa_loss_meter = LossMeter()\n",
    "            refcoco_loss_meter = LossMeter()\n",
    "            # best_eval_loss = 9595.\n",
    "            quesid2ans = {}\n",
    "            best_vqa_valid = 0.\n",
    "            best_vqa_epoch = 0\n",
    "\n",
    "            wandb.init(project=self.args.project_name)\n",
    "            wandb.run.name = self.args.run_name\n",
    "            wandb.config.update(self.args)\n",
    "            wandb.watch(self.model)\n",
    "            wandb.log(\n",
    "                {\"percent of updated parameters (%)\": self.percent_updated_parameters}\n",
    "            )\n",
    "\n",
    "            src_dir = os.path.dirname(os.getcwd())\n",
    "            base_path = os.path.dirname(src_dir)\n",
    "            src_dir = str(src_dir)\n",
    "            wandb.save(os.path.join(src_dir + \"/*.py\"), base_path=base_path)\n",
    "\n",
    "        if self.args.distributed:\n",
    "            dist.barrier()\n",
    "\n",
    "        global_step = 0\n",
    "        for epoch in range(self.args.epochs):\n",
    "            if self.start_epoch is not None:\n",
    "                epoch += self.start_epoch\n",
    "            self.model.train()\n",
    "            self.partial_eval()\n",
    "\n",
    "            if self.args.distributed:\n",
    "                self.train_loader.set_epoch(epoch)\n",
    "            if self.verbose:\n",
    "                pbar = tqdm(total=len(self.train_loader), ncols=250)\n",
    "\n",
    "            epoch_results = {\n",
    "                'loss': 0.,\n",
    "\n",
    "            }\n",
    "\n",
    "            task_counter = {\n",
    "                'vqa': 0,\n",
    "            }\n",
    "\n",
    "            # vqa\n",
    "            quesid2ans = {}\n",
    "            train_acc = 0.\n",
    "            # train_acc_steps = int(len(self.train_loader) * 0.05)\n",
    "            # last_acc_step = 0\n",
    "\n",
    "            for step_i, batch in enumerate(self.train_loader):\n",
    "\n",
    "                # print(f'GPU{self.args.gpu} inside training loop')\n",
    "                # print(batch)\n",
    "                task = batch['task']\n",
    "                # if self.verbose:\n",
    "                #     print('task', task)\n",
    "                task_counter[task] += 1\n",
    "\n",
    "                batch['log_train_accuracy'] = self.args.log_train_accuracy\n",
    "\n",
    "                # self.optim.zero_grad()\n",
    "                if self.args.fp16 and _use_native_amp:\n",
    "                    with autocast():\n",
    "                        if self.args.distributed:\n",
    "                            results = ddp_forward(self.model, batch)\n",
    "                        else:\n",
    "                            results = self.model.train_step(batch)\n",
    "                else:\n",
    "                    if self.args.distributed:\n",
    "                        results = ddp_forward(self.model, batch)\n",
    "                    else:\n",
    "                        results = self.model.train_step(batch)\n",
    "\n",
    "                loss = results['loss']\n",
    "\n",
    "                if self.args.track_z:\n",
    "                    reg_loss = 0\n",
    "                    layer_num = 0\n",
    "                    for name, sub_module in self.model.named_modules():\n",
    "                        if isinstance(sub_module, (AdapterController)):\n",
    "                            reg_loss += ((sub_module.adapters[task].z) ** 2).mean()\n",
    "                            layer_num += 1\n",
    "\n",
    "                        if isinstance(sub_module, (MetaLayersAdapterController)):\n",
    "                            reg_loss += ((sub_module.z) ** 2).mean()\n",
    "                            layer_num += 1\n",
    "\n",
    "                    reg_loss = reg_loss / layer_num\n",
    "\n",
    "                    loss = loss + self.args.lambda_z * reg_loss\n",
    "\n",
    "                    # wandb.log(\n",
    "                    #     {\"Reg loss\": reg_loss.item()},\n",
    "                    #     step=global_step\n",
    "                    # )\n",
    "\n",
    "                # print(f'GPU{self.args.gpu} after loss')\n",
    "\n",
    "                if self.args.fp16 and _use_native_amp:\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                elif self.args.fp16 and _use_apex:\n",
    "                    with amp.scale_loss(loss, self.optim) as scaled_loss:\n",
    "                        scaled_loss.backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "\n",
    "                # print(f'GPU{self.args.gpu} after backward')\n",
    "\n",
    "                loss = loss.detach()\n",
    "\n",
    "                # Update Parameters\n",
    "                if self.args.clip_grad_norm > 0:\n",
    "                    if self.args.fp16 and _use_native_amp:\n",
    "                        self.scaler.unscale_(self.optim)\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            self.model.parameters(), self.args.clip_grad_norm)\n",
    "                    elif self.args.fp16 and _use_apex:\n",
    "                        torch.nn.utils.clip_grad_norm_(amp.master_params(\n",
    "                            self.optim), self.args.clip_grad_norm)\n",
    "                    else:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            self.model.parameters(), self.args.clip_grad_norm)\n",
    "\n",
    "                if self.args.fp16 and _use_native_amp:\n",
    "                    self.scaler.step(self.optim)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    self.optim.step()\n",
    "\n",
    "                if self.lr_scheduler:\n",
    "                    self.lr_scheduler.step()\n",
    "                for param in self.model.parameters():\n",
    "                    param.grad = None\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "                for k, v in results.items():\n",
    "                    if k in epoch_results:\n",
    "                        epoch_results[k] += v.item()\n",
    "\n",
    "                if self.lr_scheduler:\n",
    "                    if version.parse(torch.__version__) >= version.parse(\"1.4\"):\n",
    "                        lr = self.lr_scheduler.get_last_lr()[0]\n",
    "                    else:\n",
    "                        lr = self.lr_scheduler.get_lr()[0]\n",
    "                else:\n",
    "                    try:\n",
    "                        lr = self.optim.get_lr()[0]\n",
    "                    except AttributeError:\n",
    "                        lr = self.args.lr\n",
    "\n",
    "\n",
    "                # self.train_step_post_hook(result)\n",
    "\n",
    "                if self.args.log_train_accuracy and task == 'refcoco':\n",
    "                    correct = results['correct']\n",
    "                    n_correct += sum(correct)\n",
    "                    n_total += len(correct)\n",
    "\n",
    "                if self.verbose:\n",
    "                    if task == 'vqa':\n",
    "                        vqa_loss_meter.update(loss.item())\n",
    "\n",
    "                    desc_str = f'Epoch {epoch} | LR {lr:.6f}'\n",
    "\n",
    "                    desc_str += f\" |\"\n",
    "                    if 'vqa' in self.args.tasks:\n",
    "                        desc_str += f\" VQA {task_counter['vqa']}\"\n",
    "                    if len(vqa_loss_meter) > 0:\n",
    "                        desc_str += f' | VQA Loss {vqa_loss_meter.val:4f}'\n",
    "\n",
    "                    pbar.set_description(desc_str)\n",
    "                    pbar.update(1)\n",
    "\n",
    "                if self.args.distributed:\n",
    "                    dist.barrier()\n",
    "\n",
    "            if self.verbose:\n",
    "                pbar.close()\n",
    "                # self.save(\"Epoch%02d\" % (epoch + 1))\n",
    "\n",
    "            if self.args.log_train_accuracy:\n",
    "                train_score_dict = {\n",
    "                    'n_correct': n_correct,\n",
    "                    'n_total': n_total\n",
    "                }\n",
    "                train_score_dict = reduce_dict(train_score_dict, self.args.gpu)\n",
    "\n",
    "            if self.verbose:\n",
    "                # Validation\n",
    "                log_str = ''\n",
    "                wandb_log_dict = {}\n",
    "\n",
    "                if 'vqa' in self.args.tasks:\n",
    "                    # VQA\n",
    "                    vqa_val_loader = self.val_loader['vqa']\n",
    "                    score_dict = self.vqa_evaluate(vqa_val_loader)\n",
    "                    valid_score = score_dict['topk_score'] * 100.\n",
    "                    valid_score_raw = score_dict['overall']\n",
    "                    if valid_score_raw > best_vqa_valid or epoch == 0:\n",
    "                        best_vqa_valid = valid_score_raw\n",
    "                        best_vqa_epoch = epoch\n",
    "                        # self.save(\"VQA_BEST\")\n",
    "                    log_str += f\"VQA\"\n",
    "                    log_str += \"\\nEpoch %d: Valid Raw %0.2f Topk %0.2f\" % (epoch, valid_score_raw, valid_score)\n",
    "                    log_str += \"\\nEpoch %d: Best Raw %0.2f\\n\" % (best_vqa_epoch, best_vqa_valid)\n",
    "                    wandb_log_dict['VQA/Valid/score'] = valid_score\n",
    "                    wandb_log_dict['VQA/Valid/raw_score'] = score_dict['overall']\n",
    "                \n",
    "                wandb.log(wandb_log_dict, step=epoch)\n",
    "\n",
    "                print(log_str)\n",
    "\n",
    "            if self.args.distributed:\n",
    "                dist.barrier()\n",
    "\n",
    "        # Test Set\n",
    "        if self.verbose:\n",
    "            self.save(\"LAST\")\n",
    "\n",
    "            log_str = ''\n",
    "            wandb_log_dict = {}\n",
    "\n",
    "            if 'vqa' in self.args.tasks:\n",
    "                # VQA\n",
    "                vqa_test_loader = self.test_loader['vqa']\n",
    "                evaluator = vqa_test_loader.evaluator\n",
    "                dump_path = os.path.join(self.args.output, 'karpathy_test_predict.json')\n",
    "                quesid2ans = self.vqa_predict(vqa_test_loader, dump_path)\n",
    "                wandb.save(dump_path, base_path=self.args.output)\n",
    "\n",
    "                acc_dict_all = evaluator.evaluate_raw(quesid2ans)\n",
    "                acc_dict_answerable = evaluator.evaluate_raw(quesid2ans, is_topk_optimal=True)\n",
    "                acc_dict_unanswerable = evaluator.evaluate_raw(quesid2ans, is_topk_optimal=False)\n",
    "\n",
    "                wandb_log_dict['VQA/Test/overall'] = acc_dict_all['overall']\n",
    "                wandb_log_dict['VQA/Test/topk_optimal'] = acc_dict_answerable['overall']\n",
    "                wandb_log_dict['VQA/Test/topk_not_optimal'] = acc_dict_unanswerable['overall']\n",
    "\n",
    "                if self.test_loader.get(\"vqa_submit\", None):\n",
    "                    vqa_submit_test_loader = self.test_loader['vqa_submit']\n",
    "                    dump_path = os.path.join(self.args.output, 'vqa_submit.json')\n",
    "                    self.vqa_predict(vqa_submit_test_loader, dump_path=dump_path)\n",
    "                    wandb.save(dump_path, base_path=self.args.output)\n",
    "\n",
    "            print(log_str)\n",
    "            wandb.log(wandb_log_dict, step=self.args.epochs)\n",
    "\n",
    "            wandb.log({'finished': True})\n",
    "\n",
    "        if self.args.distributed:\n",
    "            dist.barrier()\n",
    "            exit()\n",
    "\n",
    "    def vqa_predict(self, loader, dump_path=None):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            quesid2ans = {}\n",
    "\n",
    "            gen_kwargs = {}\n",
    "            gen_kwargs['num_beams'] = 1\n",
    "\n",
    "            for i, batch in enumerate(tqdm(loader, ncols=150, desc=\"VQA Validation\")):\n",
    "\n",
    "                if self.args.distributed:\n",
    "                    results = self.model.module.test_step(batch, **gen_kwargs)\n",
    "                else:\n",
    "                    results = self.model.test_step(batch, **gen_kwargs)\n",
    "\n",
    "                pred_ans = results['pred_ans']\n",
    "                ques_ids = batch['question_ids']\n",
    "\n",
    "                for qid, ans in zip(ques_ids, pred_ans):\n",
    "                    quesid2ans[qid] = ans\n",
    "\n",
    "            if dump_path is not None:\n",
    "                loader.evaluator.dump_result(quesid2ans, dump_path)\n",
    "            return quesid2ans\n",
    "\n",
    "    def vqa_evaluate(self, loader, dump_path=None):\n",
    "        evaluator = loader.evaluator\n",
    "        quesid2ans = self.vqa_predict(loader, dump_path)\n",
    "\n",
    "        acc_dict = evaluator.evaluate_raw(quesid2ans)\n",
    "\n",
    "        topk_score = evaluator.evaluate(quesid2ans)\n",
    "        acc_dict['topk_score'] = topk_score\n",
    "\n",
    "        return acc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0089a652-7948-4cea-94ff-6e74b974ac2f",
   "metadata": {},
   "source": [
    "### 1.4 Reft Data Specifics (intervention locations)\n",
    "\n",
    "This part is pretty complex, similar to the ReFT implementation on text. If we do not differentiate text vs image tokens, use `get_intervention_locations()`. If we have both text and image interventions, use `get_image_intervention_locations()`. Other two getters are not thoroughly tested. Also, the number of interventions passed into these functions should match the number of interventions defined in the `ReftConfig` in section 1.3.1. We only use `pad_mode = first`. `pad_mode = last` have not been tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94da9380-74b5-4eb4-af3a-67bcd86e4464",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import torch\n",
    "\n",
    "def parse_positions(positions: str):\n",
    "    # parse position\n",
    "    first_n, last_n = 0, 0\n",
    "    if \"+\" in positions:\n",
    "        first_n = int(positions.split(\"+\")[0].strip(\"f\"))\n",
    "        last_n = int(positions.split(\"+\")[1].strip(\"l\"))\n",
    "    else:\n",
    "        if \"f\" in positions:\n",
    "            first_n = int(positions.strip(\"f\"))\n",
    "        elif \"l\" in positions:\n",
    "            last_n = int(positions.strip(\"l\"))\n",
    "    return first_n, last_n\n",
    "\n",
    "\n",
    "def get_intervention_locations(**kwargs):\n",
    "    \"\"\"\n",
    "    This function generates the intervention locations.\n",
    "\n",
    "    For your customized dataset, you want to create your own function.\n",
    "    \"\"\"\n",
    "    # parse kwargs\n",
    "    share_weights = kwargs[\"share_weights\"] if \"share_weights\" in kwargs else False\n",
    "    last_position = kwargs[\"last_position\"]\n",
    "    if \"positions\" in kwargs:\n",
    "        _first_n, _last_n = parse_positions(kwargs[\"positions\"])\n",
    "    else:\n",
    "        _first_n, _last_n = kwargs[\"first_n\"], kwargs[\"last_n\"]\n",
    "    num_interventions = kwargs[\"num_interventions\"]\n",
    "    pad_mode = kwargs[\"pad_mode\"] if \"pad_mode\" in kwargs else \"first\"\n",
    "    last_offset = kwargs[\"last_offset\"] if \"last_offset\" in kwargs else 0\n",
    "    last_position += last_offset\n",
    "\n",
    "\n",
    "    first_n = min(last_position // 2, _first_n)\n",
    "    last_n = min(last_position // 2, _last_n)\n",
    "\n",
    "    pad_amount = (_first_n - first_n) + (_last_n - last_n)\n",
    "    pad_position = -1 if pad_mode == \"first\" else last_position\n",
    "    if share_weights or (first_n == 0 or last_n == 0):\n",
    "        position_list = [i for i in range(first_n)] + \\\n",
    "            [i for i in range(last_position - last_n, last_position)] + \\\n",
    "            [pad_position for _ in range(pad_amount)]\n",
    "        intervention_locations = [position_list]*num_interventions\n",
    "    else:\n",
    "        left_pad_amount = (_first_n - first_n)\n",
    "        right_pad_amount = (_last_n - last_n)\n",
    "        left_intervention_locations = [i for i in range(first_n)] + [pad_position for _ in range(left_pad_amount)]\n",
    "        right_intervention_locations = [i for i in range(last_position - last_n, last_position)] + \\\n",
    "            [pad_position for _ in range(right_pad_amount)]\n",
    "        # after padding, there could be still length diff, we need to do another check\n",
    "        left_len = len(left_intervention_locations)\n",
    "        right_len = len(right_intervention_locations)\n",
    "        if left_len > right_len:\n",
    "            right_intervention_locations += [pad_position for _ in range(left_len-right_len)]\n",
    "        else:\n",
    "            left_intervention_locations += [pad_position for _ in range(right_len-left_len)]\n",
    "        intervention_locations = [left_intervention_locations]*(num_interventions//2) + \\\n",
    "            [right_intervention_locations]*(num_interventions//2)\n",
    "    \n",
    "    return intervention_locations\n",
    "\n",
    "def get_all_intervention_locations(**kwargs):\n",
    "    positions = kwargs[\"positions\"]\n",
    "    amt = int(positions.strip(\"all\"))\n",
    "    pad_mode = kwargs[\"pad_mode\"] if \"pad_mode\" in kwargs else \"first\"\n",
    "    last_offset = kwargs[\"last_offset\"] if \"last_offset\" in kwargs else 0\n",
    "    last_position = kwargs[\"last_position\"]\n",
    "    last_position += last_offset\n",
    "    pad_position = -1 if pad_mode == \"first\" else last_position\n",
    "    intervention_locations = [i for i in range(last_position)] + [pad_position for _ in range(amt - last_position)]\n",
    "    return [intervention_locations]*kwargs[\"num_interventions\"]\n",
    "\n",
    "def get_image_only_intervention_locations(**kwargs):\n",
    "    \"\"\"\n",
    "    This function generates the intervention locations.\n",
    "    For simplicity, this function does not implement padding.\n",
    "\n",
    "    For your customized dataset, you want to create your own function.\n",
    "    \"\"\"\n",
    "    # parse kwargs\n",
    "    share_weights = kwargs[\"share_weights\"] if \"share_weights\" in kwargs else False\n",
    "    last_text_position = kwargs[\"last_position\"]\n",
    "    assert \"image_positions\" in kwargs, \"Image positions must be provided\"\n",
    "    first_image_n, last_image_n = parse_positions(kwargs[\"image_positions\"])\n",
    "\n",
    "    num_interventions = kwargs[\"num_interventions\"]\n",
    "    image_offset = kwargs[\"last_offset\"] if \"last_offset\" in kwargs else 0\n",
    "\n",
    "    pad_mode = kwargs[\"pad_mode\"] if \"pad_mode\" in kwargs else \"first\"\n",
    "    pad_position = -1 if pad_mode == \"first\" else last_text_position + image_offset\n",
    "    if pad_mode != \"first\" and \"nlvr\" in kwargs[\"tasks\"]:\n",
    "        pad_position = last_text_position + 2 * image_offset\n",
    "\n",
    "    if share_weights or (first_image_n == 0 or last_image_n == 0):\n",
    "        image_position_list = [i for i in range(last_text_position, last_text_position + first_image_n)] + \\\n",
    "            [i for i in range(last_text_position + image_offset - last_image_n, last_text_position + image_offset)]\n",
    "        if \"nlvr\" in kwargs[\"tasks\"]:\n",
    "            image_position_list += [i for i in range(last_text_position + image_offset, last_text_position + image_offset + first_image_n)] + \\\n",
    "            [i for i in range(last_text_position + 2 * image_offset - last_image_n, last_text_position + 2 * image_offset)]\n",
    "        intervention_locations = [image_position_list]* num_interventions\n",
    "    else:\n",
    "        left_image_intervention_locations = [i for i in range(last_text_position, last_text_position + first_image_n)]\n",
    "        right_image_intervention_locations = [i for i in range(last_text_position + image_offset - last_image_n, last_text_position + image_offset)]\n",
    "        if \"nlvr\" in kwargs[\"tasks\"]:\n",
    "            left_image_intervention_locations += [i for i in range(last_text_position + image_offset, last_text_position + image_offset + first_image_n)]\n",
    "            right_image_intervention_locations += [i for i in range(last_text_position + 2 * image_offset - last_image_n, last_text_position + 2 * image_offset)]\n",
    "        intervention_locations = \\\n",
    "            [left_image_intervention_locations]*(num_interventions//2) + \\\n",
    "            [right_image_intervention_locations]*(num_interventions//2)\n",
    "    return intervention_locations\n",
    "\n",
    "\n",
    "\n",
    "def get_image_intervention_locations(**kwargs):\n",
    "    \"\"\"\n",
    "    This function generates the intervention locations.\n",
    "    For simplicity, this function does not implement padding.\n",
    "\n",
    "    For your customized dataset, you want to create your own function.\n",
    "    \"\"\"\n",
    "    # parse kwargs\n",
    "    share_weights = kwargs[\"share_weights\"] if \"share_weights\" in kwargs else False\n",
    "    last_text_position = kwargs[\"last_position\"]\n",
    "    assert \"image_positions\" in kwargs, \"Image positions must be provided\"\n",
    "    assert \"positions\" in kwargs, \"Text positions must be provided\"\n",
    "    first_n, last_n = parse_positions(kwargs[\"positions\"])\n",
    "    first_image_n, last_image_n = parse_positions(kwargs[\"image_positions\"])\n",
    "\n",
    "    num_interventions = kwargs[\"num_interventions\"]\n",
    "    # `last_offset` is the length of the images (n_boxes).\n",
    "    # Image tokens are concatenated to the end of the text tokens, i.e. after `last_position` tokens.\n",
    "    # The true last position of the input is `last_position + last_offset`\n",
    "    image_offset = kwargs[\"last_offset\"] if \"last_offset\" in kwargs else 0\n",
    "\n",
    "    pad_mode = kwargs[\"pad_mode\"] if \"pad_mode\" in kwargs else \"first\"\n",
    "    pad_position = -1 if pad_mode == \"first\" else last_text_position + image_offset\n",
    "    if pad_mode != \"first\" and \"nlvr\" in kwargs[\"tasks\"]:\n",
    "        pad_position = last_text_position + 2 * image_offset\n",
    "\n",
    "    if share_weights or ((first_n == 0 or last_n == 0) and (first_image_n == 0 or last_image_n == 0)):\n",
    "        position_list = [i for i in range(first_n)] + \\\n",
    "            [i for i in range(last_text_position - last_n, last_text_position)]\n",
    "        image_position_list = [i for i in range(last_text_position, last_text_position + first_image_n)] + \\\n",
    "            [i for i in range(last_text_position + image_offset - last_image_n, last_text_position + image_offset)]\n",
    "        # There are 2 images in nlvr, so performing special treatment\n",
    "        # For this notebook however, we only use vqa\n",
    "        if \"nlvr\" in kwargs[\"tasks\"]:\n",
    "            image_position_list += [i for i in range(last_text_position + image_offset, last_text_position + image_offset + first_image_n)] + \\\n",
    "            [i for i in range(last_text_position + 2 * image_offset - last_image_n, last_text_position + 2 * image_offset)]\n",
    "        text_len = len(position_list)\n",
    "        image_len = len(image_position_list)\n",
    "        if text_len > image_len:\n",
    "            image_position_list += [pad_position for _ in range(text_len-image_len)]\n",
    "        else:\n",
    "            position_list += [pad_position for _ in range(image_len-text_len)]\n",
    "        intervention_locations = [position_list]*(num_interventions//2) + \\\n",
    "            [image_position_list]*(num_interventions//2)\n",
    "    else:\n",
    "        assert first_n == last_n, \"For now, we only support same first and last positions\"\n",
    "        left_intervention_locations = [i for i in range(first_n)]\n",
    "        right_intervention_locations = [i for i in range(last_text_position - last_n, last_text_position)]\n",
    "        left_image_intervention_locations = [i for i in range(last_text_position, last_text_position + first_image_n)]\n",
    "        right_image_intervention_locations = [i for i in range(last_text_position + image_offset - last_image_n, last_text_position + image_offset)]\n",
    "        if \"nlvr\" in kwargs[\"tasks\"]:\n",
    "            left_image_intervention_locations += [i for i in range(last_text_position + image_offset, last_text_position + image_offset + first_image_n)]\n",
    "            right_image_intervention_locations += [i for i in range(last_text_position + 2 * image_offset - last_image_n, last_text_position + 2 * image_offset)]\n",
    "        text_len = len(left_intervention_locations)\n",
    "        image_len = len(left_image_intervention_locations)\n",
    "        if text_len > image_len:\n",
    "            left_image_intervention_locations += [pad_position for _ in range(text_len-image_len)]\n",
    "            right_image_intervention_locations += [pad_position for _ in range(text_len-image_len)]\n",
    "        else:\n",
    "            left_intervention_locations += [pad_position for _ in range(image_len-text_len)]\n",
    "            right_intervention_locations += [pad_position for _ in range(image_len-text_len)]\n",
    "\n",
    "        intervention_locations = [left_intervention_locations]*(num_interventions//4) + \\\n",
    "            [right_intervention_locations]*(num_interventions//4) + \\\n",
    "            [left_image_intervention_locations]*(num_interventions//4) + \\\n",
    "            [right_image_intervention_locations]*(num_interventions//4)\n",
    "    return intervention_locations\n",
    "\n",
    "    \n",
    "def compute_intervention(\n",
    "    id: int, \n",
    "    result: dict, \n",
    "    tokenizer,\n",
    "    fields_to_pad = [],\n",
    "    fields_to_mask = [],\n",
    "    **kwargs):\n",
    "    pad_mode = kwargs[\"pad_mode\"]\n",
    "    # compute intervention locs\n",
    "    if \"positions\" in kwargs and \"all\" in kwargs[\"positions\"]:\n",
    "        intervention_locations =  get_all_intervention_locations(**kwargs)\n",
    "    elif \"image_positions\" in kwargs and \"positions\" in kwargs:\n",
    "        intervention_locations = get_image_intervention_locations(**kwargs)\n",
    "    elif \"image_positions\" in kwargs:\n",
    "        intervention_locations = get_image_only_intervention_locations(**kwargs)\n",
    "    else:\n",
    "        intervention_locations = get_intervention_locations(**kwargs)\n",
    "    result[\"intervention_locations\"] = intervention_locations\n",
    "    result[\"id\"] = id\n",
    "\n",
    "    # add a single padding token BEFORE input_ids and fix everything\n",
    "    if fields_to_pad is not None:\n",
    "        if pad_mode == \"first\":\n",
    "            for field in fields_to_pad:\n",
    "                if field not in result:\n",
    "                    continue\n",
    "                if field == \"labels\":\n",
    "                    result[field] = torch.cat((torch.tensor([IGNORE_INDEX,]), result[field]))\n",
    "                else:\n",
    "                    result[field] = torch.cat((torch.tensor([tokenizer.pad_token_id,]), result[field]))\n",
    "            result[\"intervention_locations\"] = (torch.IntTensor(result[\"intervention_locations\"]) + 1).tolist()\n",
    "            result[\"input_length\"] += 1\n",
    "        elif pad_mode == \"last\":\n",
    "            for field in fields_to_pad:\n",
    "                if field not in result:\n",
    "                    continue\n",
    "                if field == \"labels\":\n",
    "                    result[field] = torch.cat((result[field], torch.tensor([IGNORE_INDEX,])))\n",
    "                else:\n",
    "                    result[field] = torch.cat((result[field], torch.tensor([tokenizer.pad_token_id,])))\n",
    "            result[\"input_length\"] += 1\n",
    "        \n",
    "    # attention masks\n",
    "    if len(fields_to_mask) == 1:\n",
    "        result[\"attention_mask\"] = (result[fields_to_mask[0]] != tokenizer.pad_token_id).int()\n",
    "    else:\n",
    "        for field in fields_to_mask:\n",
    "            result[f\"{field}_mask\"] = (result[field] != tokenizer.pad_token_id).int()\n",
    "\n",
    "    # does not handle subspaces for now\n",
    "    # print(\"Intervention Locations\", result[\"intervention_locations\"])\n",
    "    return result\n",
    "\n",
    "def reft_post_process(\n",
    "    out_dict,\n",
    "    tokenizer,\n",
    "    idx: int, \n",
    "    last_position: int, \n",
    "    args = None,\n",
    "    pad_mode = \"none\",\n",
    "    fields_to_pad = [],\n",
    "    fields_to_mask = []\n",
    "):\n",
    "    # print(\"Out_dict keys:\", out_dict.keys())\n",
    "    out_dict[\"instruction\"] = tokenizer.decode(\n",
    "        out_dict[\"input_ids\"], skip_special_tokens=True)\n",
    "    # out_dict[\"logits\"] = out_dict[\"labels\"]\n",
    "    # out_dict[\"labels\"] = out_dict[\"target_ids\"]\n",
    "    kwargs = {}\n",
    "    if args is not None:\n",
    "        if args.reft_rank != -1:\n",
    "            kwargs[\"positions\"] = args.positions\n",
    "        if args.reft_image_rank != -1:\n",
    "            kwargs[\"image_positions\"] = args.image_positions\n",
    "        kwargs[\"share_weights\"] = args.share_weights\n",
    "        layers = [int(l) for l in args.layers.split(\";\")]\n",
    "        kwargs[\"num_interventions\"] = len(layers) if args.share_weights else 2 * len(layers)\n",
    "        # Double interventions if creating separate interventions for texts and images\n",
    "        if args.reft_image_rank != -1 and args.reft_rank != -1:\n",
    "            kwargs[\"num_interventions\"] *= 2\n",
    "        # `n_boxes` is the seq length of the image embeddings\n",
    "        kwargs[\"last_offset\"] = args.n_boxes\n",
    "        # Only tested `first` \n",
    "        kwargs[\"pad_mode\"] = pad_mode\n",
    "        kwargs[\"last_position\"] = last_position\n",
    "        kwargs[\"tasks\"] = args.prompt\n",
    "    # print(kwargs)\n",
    "\n",
    "    # print(\"BEFORE:\", out_dict[\"input_ids\"].shape, kwargs[\"last_position\"])\n",
    "    tokenized = compute_intervention(\n",
    "            idx, \n",
    "            out_dict, \n",
    "            tokenizer,\n",
    "            fields_to_pad,\n",
    "            fields_to_mask,\n",
    "            **kwargs)\n",
    "    # print(\"AFTER:\", tokenized[\"input_ids\"].shape, tokenized[\"intervention_locations\"])\n",
    "    return tokenized\n",
    "\n",
    "def keep_intervention_locations(datum):\n",
    "    new_data = {}\n",
    "    new_data[\"input_ids\"] = datum[\"input_ids\"]\n",
    "    new_data[\"intervention_locations\"] = datum[\"intervention_locations\"]\n",
    "    new_data[\"attention_mask\"] = datum[\"attention_mask\"]\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def reft_supplemental_data_collator(batch, tokenizer):\n",
    "    # Create padded `intervention_locations`\n",
    "    intervene_batch = [keep_intervention_locations(item) for item in batch]\n",
    "    # The normal data collator for collating other VLBart fields\n",
    "    intervention_loc_collate_fn = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=None,\n",
    "        label_pad_token_id=-100,\n",
    "        padding=\"longest\"\n",
    "    )\n",
    "    \n",
    "    intervene_batch_entry = intervention_loc_collate_fn(intervene_batch)\n",
    "\n",
    "    batch_entry = {}\n",
    "    id = []\n",
    "    instructions = []\n",
    "    # Collate `instruction` and `id`\n",
    "    for i, entry in enumerate(batch):\n",
    "        if 'instruction' in entry:\n",
    "            instructions.append(entry['instruction'])\n",
    "        if 'id' in entry:\n",
    "            id.append(entry['id'])\n",
    "    import numpy as np\n",
    "    batch_entry['id'] = np.array(id)\n",
    "    batch_entry['instruction'] = instructions\n",
    "    \n",
    "    # Pad `intervention_locations` with other stuff in the batch\n",
    "    if \"intervention_locations\" in batch[0]:\n",
    "        batch_entry[\"intervention_locations\"] = intervene_batch_entry[\"intervention_locations\"]\n",
    "    return batch_entry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f6fdd0-cf51-4f6c-9122-fc115a3e2940",
   "metadata": {},
   "source": [
    "Below parts are the same as `VQAFineTuneDataset` in vqa_clip_data.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32011cda-462e-4c73-9ff6-0569b7a338aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "import vqa_clip_data as vqa_data\n",
    "\n",
    "class ReftVQAFineTuneDataset(vqa_data.VQAFineTuneDataset):\n",
    "    def __init__(self, split='train', raw_dataset=None, rank=-1, topk=-1, verbose=True, args=None, mode='train'):\n",
    "        super().__init__(split, raw_dataset, rank, topk, verbose, args, mode)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        out_dict = super().__getitem__(idx)\n",
    "\n",
    "        out_dict[\"instruction\"] = self.tokenizer.decode(\n",
    "            out_dict['input_ids'], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        last_position = len(out_dict['input_ids']) - 1\n",
    "        out_dict = reft_post_process(\n",
    "            out_dict,\n",
    "            self.tokenizer,\n",
    "            idx,\n",
    "            last_position,\n",
    "            self.args,\n",
    "            pad_mode=\"first\",\n",
    "            fields_to_pad=[\"input_ids\"],\n",
    "            fields_to_mask=[\"input_ids\"]\n",
    "        )\n",
    "\n",
    "        return out_dict\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        batch_entry = super().collate_fn(batch)\n",
    "        # BEGIN ADD\n",
    "        extra_batch = reft_supplemental_data_collator(batch, self.tokenizer)\n",
    "        for k, v in extra_batch.items():\n",
    "            batch_entry[k] = v\n",
    "        # END ADD\n",
    "        # print(\"LOGITS:\", batch_entry[\"logits\"])\n",
    "        # print(\"LABELS:\", batch_entry[\"labels\"])\n",
    "\n",
    "        return batch_entry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cb3d460-8857-4368-9932-101919493bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(args, split='karpathy_train', mode='train',\n",
    "               batch_size=32, workers=4, distributed=False, gpu=0, topk=-1):\n",
    "\n",
    "    verbose = (gpu == 0)\n",
    "\n",
    "    _dset = vqa_data.VQADataset(split, verbose)\n",
    "    # print(\"Batch size:\", batch_size, \"Num workers:\", workers, \"Topk:\", topk)\n",
    "\n",
    "    dataset = ReftVQAFineTuneDataset(\n",
    "        split,\n",
    "        raw_dataset=_dset,\n",
    "        rank=gpu,\n",
    "        topk=topk,\n",
    "        verbose=verbose,\n",
    "        args=args,\n",
    "        mode=mode)\n",
    "\n",
    "    if distributed:\n",
    "        sampler = DistributedSampler(dataset)\n",
    "    else:\n",
    "        sampler = None\n",
    "\n",
    "    if mode == 'train':\n",
    "        loader = DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=(sampler is None),\n",
    "            num_workers=workers, pin_memory=True, sampler=sampler,\n",
    "            collate_fn=dataset.collate_fn)\n",
    "    else:\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=workers, pin_memory=True,\n",
    "            sampler=sampler,\n",
    "            shuffle=None if (sampler is not None) else False,\n",
    "            collate_fn=dataset.collate_fn,\n",
    "            drop_last=False)\n",
    "\n",
    "    if verbose:\n",
    "        loader.evaluator = vqa_data.VQAEvaluator(_dset)\n",
    "\n",
    "    loader.task = 'vqa'\n",
    "\n",
    "    return loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b4ae2-8de0-4362-a759-bfb047854f03",
   "metadata": {},
   "source": [
    "### 1.5 Main Worker, Params, and Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e82e41-d794-4f8d-b2a0-30cffc1dc694",
   "metadata": {},
   "source": [
    "Main worker here is the same as in multitask.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71a508de-c65b-45ce-964e-ae7f138ca8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_worker(gpu, args):\n",
    "    # GPU is assigned\n",
    "    args.gpu = gpu\n",
    "    args.rank = gpu\n",
    "    print(f'Process Launching at GPU {gpu}')\n",
    "\n",
    "    if args.distributed:\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        dist.init_process_group(backend='nccl')\n",
    "\n",
    "    print(f\"args.feature_type {args.feature_type}\")\n",
    "    feat_dim_dict = {\n",
    "        \"RN50\": 2048,\n",
    "        \"RN101\": 2048,\n",
    "        \"RN50x4\": 2560,\n",
    "        \"ViT\": 768\n",
    "    }\n",
    "    args.feat_dim = feat_dim_dict[args.feature_type]\n",
    "    import vqa_clip_data as vqa_data\n",
    "\n",
    "    vqa_args = deepcopy(args)\n",
    "    vqa_args.max_text_length = 20\n",
    "\n",
    "\n",
    "    if args.use_tasks_prompts:\n",
    "        vqa_args.prompt = \"vqa: \"\n",
    "    else:\n",
    "        vqa_args.prompt = \"\"\n",
    "\n",
    "    train_loaders = []\n",
    "\n",
    "    if args.epochs > 0:\n",
    "        if 'vqa' in args.tasks:\n",
    "            print(f'Building VQA train loader at GPU {gpu}')\n",
    "            vqa_train_loader = get_loader(\n",
    "                vqa_args,\n",
    "                split='karpathy_train', mode='train', batch_size=vqa_args.batch_size,\n",
    "                distributed=args.distributed, gpu=args.gpu,\n",
    "                workers=args.num_workers,\n",
    "                topk=args.train_topk,\n",
    "            )\n",
    "            train_loaders.append(vqa_train_loader)\n",
    "\n",
    "    train_loader = multitask_data.MultitaskLoader(\n",
    "        train_loaders,\n",
    "        sampling=args.multitask_sampling,\n",
    "        verbose=gpu==0)\n",
    "\n",
    "    val_num_workers = 4\n",
    "    # Validation set\n",
    "    if gpu == 0:\n",
    "        val_loader = {}\n",
    "        if args.epochs > 0:\n",
    "            if 'vqa' in args.tasks:\n",
    "                print(f'Building VQA val loader at GPU {gpu}')\n",
    "                vqa_val_loader = get_loader(\n",
    "                    vqa_args,\n",
    "                    split='karpathy_val', mode='val', batch_size=vqa_args.batch_size,\n",
    "                    distributed=False, gpu=args.gpu,\n",
    "                    workers=val_num_workers,\n",
    "                    topk=args.valid_topk,\n",
    "                )\n",
    "                val_loader['vqa'] = vqa_val_loader\n",
    "\n",
    "        # Test set\n",
    "        test_loader = {}\n",
    "        if 'vqa' in args.tasks:\n",
    "            print(f'Building VQA test loader at GPU {gpu}')\n",
    "            vqa_test_loader = get_loader(\n",
    "                vqa_args,\n",
    "                split='karpathy_test', mode='val', batch_size=vqa_args.batch_size,\n",
    "                distributed=False, gpu=args.gpu,\n",
    "                workers=val_num_workers,\n",
    "                topk=args.valid_topk,\n",
    "            )\n",
    "            test_loader['vqa'] = vqa_test_loader\n",
    "\n",
    "            if args.testing:\n",
    "                vqa_submit_test_loader = get_loader(\n",
    "                    vqa_args,\n",
    "                    split='test_4', mode='val', batch_size=vqa_args.batch_size,\n",
    "                    distributed=False, gpu=args.gpu,\n",
    "                    workers=val_num_workers,\n",
    "                    topk=args.valid_topk,\n",
    "                )\n",
    "                test_loader['vqa_submit'] = vqa_submit_test_loader\n",
    "    else:\n",
    "        val_loader = None\n",
    "        test_loader = None\n",
    "\n",
    "    trainer = Trainer(args, train_loader, val_loader, test_loader, train=True)\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aef8517f-aa1f-41d3-a01d-0f5c8edb7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "args = parse_args(False)\n",
    "ngpus_per_node = torch.cuda.device_count()\n",
    "args.world_size = ngpus_per_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b7ecf-2d4c-4222-bd71-c03c58218d68",
   "metadata": {},
   "source": [
    "We added some ReFT parameters as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1c6a607-bc07-4236-be0a-ed6af23ac2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.distributed = True\n",
    "args.nproc_per_node = 1\n",
    "args.master_port = 26464\n",
    "args.multiGPU = True\n",
    "args.optim = \"adamw\"\n",
    "args.warmup_ratio = 0.1\n",
    "args.clip_grad_norm = 5\n",
    "args.weight_decay = 0.01\n",
    "args.lr = 1e-3\n",
    "args.epochs = 20\n",
    "args.num_workers = 4\n",
    "args.backbone = \"facebook/bart-base\"\n",
    "args.output = \"snap/VLBart_dora_reft/test/\"\n",
    "args.num_beams = 5\n",
    "args.use_tasks_prompts = True\n",
    "args.train_topk = 100\n",
    "args.valid_topk = 100\n",
    "args.batch_size = 100\n",
    "args.valid_batch_size = 100\n",
    "# args.use_dora = True\n",
    "args.unfreeze_bias = True\n",
    "args.unfreeze_layer_norms = True\n",
    "# args.lora_settings = True\n",
    "# args.lora_dim = 128\n",
    "args.tasks = \"vqa\"\n",
    "args.dropout = 0.00\n",
    "args.reft_dropout = 0.00\n",
    "args.reft_image_dropout = 0.00\n",
    "args.reft_rank = 4\n",
    "args.reft_image_rank = 64\n",
    "args.positions = \"f3+l3\"\n",
    "args.image_positions = \"f3+l3\"\n",
    "\n",
    "args.feature = \"RN101\"\n",
    "args.n_boxes = 36\n",
    "args.downsample = True\n",
    "args.image_size = \"(224,224)\"\n",
    "args.project_name = \"Test\"\n",
    "args.run_name = \"tune+lr1e-3\"\n",
    "args.local_rank = 0\n",
    "args.feature_type = \"RN101\"\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '26464'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a42e43a-38b3-4577-9e19-90198026aefb",
   "metadata": {},
   "source": [
    "Try the below yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d493300c-f7da-40ec-a848-064fd26848a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations\n",
      "{'RefCOCO_BUTD': False,\n",
      " 'RefCOCO_GT': False,\n",
      " 'adam_beta1': 0.9,\n",
      " 'adam_beta2': 0.999,\n",
      " 'adam_eps': 1e-06,\n",
      " 'add_adapter_cross_attn': True,\n",
      " 'add_layer_norm_after_adapter': False,\n",
      " 'add_layer_norm_before_adapter': False,\n",
      " 'additional_visual_embedding_layers': 0,\n",
      " 'answer_normalize': False,\n",
      " 'backbone': 'facebook/bart-base',\n",
      " 'batch_size': 100,\n",
      " 'caption_cocoonly': True,\n",
      " 'caption_only': False,\n",
      " 'classifier': False,\n",
      " 'clip_grad_norm': 5,\n",
      " 'cls_task': 'tinyimagenet',\n",
      " 'coco_only': False,\n",
      " 'comment': '',\n",
      " 'decoder_prompt_len': 0,\n",
      " 'deepspeed': None,\n",
      " 'distributed': True,\n",
      " 'do_lower_case': False,\n",
      " 'dora_simple': False,\n",
      " 'downsample': True,\n",
      " 'dropout': 0.0,\n",
      " 'dry': False,\n",
      " 'efficient_unique_hyper_net': False,\n",
      " 'encoder_prompt_len': 0,\n",
      " 'epochs': 20,\n",
      " 'expand_vis_embedding': False,\n",
      " 'factorized_phm': True,\n",
      " 'feat_dim': 2048,\n",
      " 'feature': 'RN101',\n",
      " 'feature_type': 'RN101',\n",
      " 'fp16': False,\n",
      " 'freeze_bn_statistics': False,\n",
      " 'freeze_ln_statistics': False,\n",
      " 'from_scratch': False,\n",
      " 'gen_max_length': 20,\n",
      " 'gradient_accumulation_steps': 1,\n",
      " 'ground_upsample': 1,\n",
      " 'ground_weight': 1,\n",
      " 'hypercomplex_division': 4,\n",
      " 'image_positions': 'f3+l3',\n",
      " 'image_size': '(224,224)',\n",
      " 'individual_vis_layer_norm': True,\n",
      " 'itm_cocoonly': True,\n",
      " 'lambda_z': 0.001,\n",
      " 'layers': '0;1;2;3;4;5',\n",
      " 'load': None,\n",
      " 'load_lxmert_qa': None,\n",
      " 'local_rank': 0,\n",
      " 'log_train_accuracy': False,\n",
      " 'lora_alpha': 32,\n",
      " 'lora_dim': 4,\n",
      " 'lora_settings': False,\n",
      " 'losses': 'lm,obj,attr,feat',\n",
      " 'low_rank_rank': 1,\n",
      " 'lr': 0.001,\n",
      " 'master_port': 26464,\n",
      " 'max_n_boxes': 36,\n",
      " 'max_text_length': 20,\n",
      " 'mid_dim': 768,\n",
      " 'multiGPU': True,\n",
      " 'multitask_sampling': 'roundrobin',\n",
      " 'n_boxes': 36,\n",
      " 'n_ground': 1,\n",
      " 'n_image_tokens': 4,\n",
      " 'no_prefix': False,\n",
      " 'nproc_per_node': 1,\n",
      " 'num_beams': 5,\n",
      " 'num_workers': 4,\n",
      " 'obj_mask_rate': 0.15,\n",
      " 'oneddownsample': False,\n",
      " 'optim': 'adamw',\n",
      " 'optimizer': 'adamw',\n",
      " 'oscar_tags': False,\n",
      " 'output': 'snap/VLBart_dora_reft/test/',\n",
      " 'phm_init_range': 0.01,\n",
      " 'phm_rank': 1,\n",
      " 'pos_dim': 4,\n",
      " 'positions': 'f3+l3',\n",
      " 'post_prompt': '',\n",
      " 'prefix': None,\n",
      " 'project_name': 'Test',\n",
      " 'projected_task_embedding_dim': -1,\n",
      " 'prompt': 'vqa: ',\n",
      " 'raw_label': False,\n",
      " 'reduction_factor': 16,\n",
      " 'reft_dropout': 0.0,\n",
      " 'reft_image_dropout': 0.0,\n",
      " 'reft_image_rank': 64,\n",
      " 'reft_rank': 4,\n",
      " 'remove_bn_vis_adapter': False,\n",
      " 'run_name': 'tune+lr1e-3',\n",
      " 'seed': 9595,\n",
      " 'share_down_sampler': False,\n",
      " 'share_up_sampler': False,\n",
      " 'share_vis_lang_layer_norm': False,\n",
      " 'share_weights': False,\n",
      " 'shared_phm_rule': True,\n",
      " 'shared_phm_rule_over_tasks': False,\n",
      " 'shuffle_boxes': False,\n",
      " 'single_vqa_prefix': False,\n",
      " 'sparse_sample': False,\n",
      " 'submit': False,\n",
      " 'tasks': 'vqa',\n",
      " 'test': None,\n",
      " 'test_answerable': False,\n",
      " 'test_only': False,\n",
      " 'testing': False,\n",
      " 'tokenizer': None,\n",
      " 'track_z': False,\n",
      " 'train': 'train',\n",
      " 'train_topk': 100,\n",
      " 'unfreeze_batch_norms': False,\n",
      " 'unfreeze_bias': True,\n",
      " 'unfreeze_decoder_layer_norms': False,\n",
      " 'unfreeze_encoder_layer_norms': False,\n",
      " 'unfreeze_language_model': False,\n",
      " 'unfreeze_layer_norms': True,\n",
      " 'unfreeze_lm_head': False,\n",
      " 'unfreeze_vis_encoder': False,\n",
      " 'unfreeze_vis_last_layer': False,\n",
      " 'unique_hyper_net': False,\n",
      " 'use_adam_for_visual': False,\n",
      " 'use_adapter': False,\n",
      " 'use_attn_prefix': False,\n",
      " 'use_compacter': False,\n",
      " 'use_data_augmentation': False,\n",
      " 'use_dora': False,\n",
      " 'use_hyperformer': False,\n",
      " 'use_lm_head_adapter': False,\n",
      " 'use_lora': False,\n",
      " 'use_lradapter': False,\n",
      " 'use_separate_optimizer_for_visual': False,\n",
      " 'use_single_adapter': False,\n",
      " 'use_single_lora': False,\n",
      " 'use_single_prompt': False,\n",
      " 'use_tasks_prompts': True,\n",
      " 'use_vis_adapter': False,\n",
      " 'use_vis_layer_norm': True,\n",
      " 'use_vis_order_embedding': True,\n",
      " 'use_vision': True,\n",
      " 'valid': 'valid',\n",
      " 'valid_batch_size': 100,\n",
      " 'valid_topk': 100,\n",
      " 'vis_adapter_type': 'middle-bottleneck',\n",
      " 'vis_lr': 0.0001,\n",
      " 'vis_pointer': False,\n",
      " 'vis_pooling_output': False,\n",
      " 'vis_reduction_factor': 2,\n",
      " 'vis_use_transformer': False,\n",
      " 'vis_weight_decay': 0.01,\n",
      " 'warmup_ratio': 0.1,\n",
      " 'weight_decay': 0.01,\n",
      " 'word_mask_rate': 0.15,\n",
      " 'world_size': 1}\n",
      "Process Launching at GPU 0\n",
      "args.feature_type RN101\n",
      "Building VQA train loader at GPU 0\n",
      "Load 605102 data from split(s) karpathy_train.\n",
      "# Answers: 3129\n",
      "Data sources:  ['karpathy_train']\n",
      "Loaded 605102 data from karpathy_train\n",
      "Use only 100 data\n",
      "# all sentences: 100\n",
      "Task2len: {'vqa': 1}\n",
      "# epoch_tasks: 1\n",
      "Building VQA val loader at GPU 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 26729 data from split(s) karpathy_val.\n",
      "# Answers: 3129\n",
      "Data sources:  ['karpathy_val']\n",
      "Loaded 26729 data from karpathy_val\n",
      "Use only 100 data\n",
      "# all sentences: 100\n",
      "Building VQA test loader at GPU 0\n",
      "Load 26280 data from split(s) karpathy_test.\n",
      "# Answers: 3129\n",
      "Data sources:  ['karpathy_test']\n",
      "Loaded 26280 data from karpathy_test\n",
      "Use only 100 data\n",
      "# all sentences: 100\n",
      "IntervenableConfig\n",
      "{\n",
      "    \"model_type\": \"None\",\n",
      "    \"representations\": [\n",
      "        {\n",
      "            \"layer\": 0,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 1,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 2,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 3,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 4,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 5,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 0,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 1,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 2,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 3,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 4,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 5,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 4,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 0,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 1,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 2,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 3,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 4,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 5,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 0,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 1,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 2,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 3,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 4,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 5,\n",
      "            \"component\": \"block_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": 64,\n",
      "            \"intervention_type\": null,\n",
      "            \"intervention\": \"PLACEHOLDER\",\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": null,\n",
      "            \"hidden_source_representation\": null\n",
      "        }\n",
      "    ],\n",
      "    \"intervention_types\": \"[<class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>, <class 'pyreft.interventions.LoreftIntervention'>]\",\n",
      "    \"mode\": \"parallel\",\n",
      "    \"sorted_keys\": \"None\",\n",
      "    \"intervention_dimensions\": \"None\"\n",
      "}\n",
      "Building Model at GPU 0\n",
      "trainable intervention params: 1,254,192 || trainable model params: 0\n",
      "model params: 141,003,264 || trainable%: 0.8894772818876022\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#0#learned_source#weight\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#0#learned_source#bias\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#0#learned_source#weight\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#0#learned_source#bias\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#0#learned_source#weight\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#0#learned_source#bias\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#0#learned_source#weight\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#0#learned_source#bias\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#0#learned_source#weight\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#0#learned_source#bias\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#0#rotate_layer#parametrizations#weight#original\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#0#learned_source#weight\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#0#learned_source#bias\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#1#learned_source#weight\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#1#learned_source#bias\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#1#learned_source#weight\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#1#learned_source#bias\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#1#learned_source#weight\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#1#learned_source#bias\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#1#learned_source#weight\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#1#learned_source#bias\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#1#learned_source#weight\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#1#learned_source#bias\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#1#rotate_layer#parametrizations#weight#original\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#1#learned_source#weight\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#1#learned_source#bias\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#2#learned_source#weight\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#2#learned_source#bias\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#2#learned_source#weight\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#2#learned_source#bias\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#2#learned_source#weight\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#2#learned_source#bias\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#2#learned_source#weight\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#2#learned_source#bias\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#2#learned_source#weight\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#2#learned_source#bias\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#2#rotate_layer#parametrizations#weight#original\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#2#learned_source#weight\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#2#learned_source#bias\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#3#learned_source#weight\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#3#learned_source#bias\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#3#learned_source#weight\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#3#learned_source#bias\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#3#learned_source#weight\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#3#learned_source#bias\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#3#learned_source#weight\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#3#learned_source#bias\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#3#learned_source#weight\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#3#learned_source#bias\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#3#rotate_layer#parametrizations#weight#original\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#3#learned_source#weight\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#3#learned_source#bias\n",
      "Model Launching at GPU 0\n",
      "model.encoder.visual_embedding.feat_embedding.0.weight is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.0.bias is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.1.weight is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.1.bias is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.0.weight is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.0.bias is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.1.weight is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.1.bias is trainable...\n",
      "model.encoder.visual_embedding.img_order_embedding.weight is trainable...\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#0#learned_source#bias is trainable...(4)\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#0#learned_source#bias is trainable...(4)\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#0#learned_source#bias is trainable...(4)\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#0#learned_source#bias is trainable...(4)\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#0#learned_source#bias is trainable...(4)\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#0#learned_source#bias is trainable...(4)\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#1#learned_source#bias is trainable...(4)\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#1#learned_source#bias is trainable...(4)\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#1#learned_source#bias is trainable...(4)\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#1#learned_source#bias is trainable...(4)\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#1#learned_source#bias is trainable...(4)\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#1#learned_source#bias is trainable...(4)\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#2#learned_source#bias is trainable...(64)\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#2#learned_source#bias is trainable...(64)\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#2#learned_source#bias is trainable...(64)\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#2#learned_source#bias is trainable...(64)\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#2#learned_source#bias is trainable...(64)\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#2#learned_source#bias is trainable...(64)\n",
      "layer#0#comp#block_output#unit#pos#nunit#1#3#learned_source#bias is trainable...(64)\n",
      "layer#1#comp#block_output#unit#pos#nunit#1#3#learned_source#bias is trainable...(64)\n",
      "layer#2#comp#block_output#unit#pos#nunit#1#3#learned_source#bias is trainable...(64)\n",
      "layer#3#comp#block_output#unit#pos#nunit#1#3#learned_source#bias is trainable...(64)\n",
      "layer#4#comp#block_output#unit#pos#nunit#1#3#learned_source#bias is trainable...(64)\n",
      "layer#5#comp#block_output#unit#pos#nunit#1#3#learned_source#bias is trainable...(64)\n",
      "model.encoder.layers.0.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.0.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.0.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.0.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.0.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.0.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.0.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.0.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.1.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.1.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.1.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.1.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.1.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.1.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.1.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.1.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.2.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.2.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.2.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.2.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.2.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.2.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.2.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.2.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.3.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.3.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.3.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.3.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.3.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.3.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.3.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.3.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.4.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.4.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.4.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.4.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.4.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.4.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.4.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.4.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.5.self_attn.k_proj.bias is trainable...(768)\n",
      "model.encoder.layers.5.self_attn.v_proj.bias is trainable...(768)\n",
      "model.encoder.layers.5.self_attn.q_proj.bias is trainable...(768)\n",
      "model.encoder.layers.5.self_attn.out_proj.bias is trainable...(768)\n",
      "model.encoder.layers.5.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layers.5.fc1.bias is trainable...(3072)\n",
      "model.encoder.layers.5.fc2.bias is trainable...(768)\n",
      "model.encoder.layers.5.final_layer_norm.bias is trainable...(768)\n",
      "model.encoder.layernorm_embedding.bias is trainable...(768)\n",
      "model.encoder.visual_embedding.feat_embedding.0.bias is trainable...(768)\n",
      "model.encoder.visual_embedding.feat_embedding.1.bias is trainable...(768)\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.0.bias is trainable...(768)\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.1.bias is trainable...(768)\n",
      "model.decoder.layers.0.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.0.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.0.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.0.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.0.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.1.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.1.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.1.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.1.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.1.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.2.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.2.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.2.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.2.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.2.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.3.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.3.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.3.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.3.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.3.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.4.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.4.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.4.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.4.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.4.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.5.self_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.self_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.self_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.self_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.self_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.5.encoder_attn.k_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.encoder_attn.v_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.encoder_attn.q_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.encoder_attn.out_proj.bias is trainable...(768)\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layers.5.fc1.bias is trainable...(3072)\n",
      "model.decoder.layers.5.fc2.bias is trainable...(768)\n",
      "model.decoder.layers.5.final_layer_norm.bias is trainable...(768)\n",
      "model.decoder.layernorm_embedding.bias is trainable...(768)\n",
      "model.encoder.layers.0.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.0.final_layer_norm is trainable...\n",
      "model.encoder.layers.1.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.1.final_layer_norm is trainable...\n",
      "model.encoder.layers.2.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.2.final_layer_norm is trainable...\n",
      "model.encoder.layers.3.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.3.final_layer_norm is trainable...\n",
      "model.encoder.layers.4.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.4.final_layer_norm is trainable...\n",
      "model.encoder.layers.5.self_attn_layer_norm is trainable...\n",
      "model.encoder.layers.5.final_layer_norm is trainable...\n",
      "model.encoder.layernorm_embedding is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.1 is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.1 is trainable...\n",
      "model.decoder.layers.0.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.0.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.0.final_layer_norm is trainable...\n",
      "model.decoder.layers.1.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.1.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.1.final_layer_norm is trainable...\n",
      "model.decoder.layers.2.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.2.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.2.final_layer_norm is trainable...\n",
      "model.decoder.layers.3.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.3.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.3.final_layer_norm is trainable...\n",
      "model.decoder.layers.4.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.4.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.4.final_layer_norm is trainable...\n",
      "model.decoder.layers.5.self_attn_layer_norm is trainable...\n",
      "model.decoder.layers.5.encoder_attn_layer_norm is trainable...\n",
      "model.decoder.layers.5.final_layer_norm is trainable...\n",
      "model.decoder.layernorm_embedding is trainable...\n",
      "VLBartMultiTask(\n",
      "  (model): VLBartModel(\n",
      "    (shared): Embedding(50465, 768)\n",
      "    (encoder): JointEncoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (visual_embedding): VisualEmbedding(\n",
      "        (feat_embedding): Sequential(\n",
      "          (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (absolute_vis_pos_embedding): Sequential(\n",
      "          (0): Linear(in_features=5, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (obj_order_embedding): Embedding(50465, 768)\n",
      "        (img_order_embedding): Embedding(2, 768)\n",
      "      )\n",
      "      (downsample): Downsample(\n",
      "        (pool): AdaptiveMaxPool2d(output_size=(6, 6))\n",
      "      )\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50465, bias=False)\n",
      "  (intervenable): ReftModel(\n",
      "    (model): VLBartModel(\n",
      "      (shared): Embedding(50465, 768)\n",
      "      (encoder): JointEncoder(\n",
      "        (embed_tokens): Embedding(50465, 768)\n",
      "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "        (layers): ModuleList(\n",
      "          (0-5): 6 x BartEncoderLayer(\n",
      "            (self_attn): BartAttention(\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (activation_fn): GELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (visual_embedding): VisualEmbedding(\n",
      "          (feat_embedding): Sequential(\n",
      "            (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "            (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (absolute_vis_pos_embedding): Sequential(\n",
      "            (0): Linear(in_features=5, out_features=768, bias=True)\n",
      "            (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (obj_order_embedding): Embedding(50465, 768)\n",
      "          (img_order_embedding): Embedding(2, 768)\n",
      "        )\n",
      "        (downsample): Downsample(\n",
      "          (pool): AdaptiveMaxPool2d(output_size=(6, 6))\n",
      "        )\n",
      "      )\n",
      "      (decoder): BartDecoder(\n",
      "        (embed_tokens): Embedding(50465, 768)\n",
      "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "        (layers): ModuleList(\n",
      "          (0-5): 6 x BartDecoderLayer(\n",
      "            (self_attn): BartAttention(\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (activation_fn): GELUActivation()\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (encoder_attn): BartAttention(\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Trainable param percentage: 2.10% (2987568/142411056)\n",
      "Building Optimizer\n",
      "Batch per epoch: 1\n",
      "Total Iters: 20\n",
      "Warmup ratio: 0.1\n",
      "Warm up Iters: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeterzw494\u001b[0m (\u001b[33mpeterwz\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sailhome/peterwz/workspace/pyreft/examples/vlbart/ReftDora/image_video_text_understanding/VL-T5/src/wandb/run-20240703_104550-u8ahudi7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peterwz/Test/runs/u8ahudi7' target=\"_blank\">eternal-sound-41</a></strong> to <a href='https://wandb.ai/peterwz/Test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peterwz/Test' target=\"_blank\">https://wandb.ai/peterwz/Test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peterwz/Test/runs/u8ahudi7' target=\"_blank\">https://wandb.ai/peterwz/Test/runs/u8ahudi7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 0 file into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# epoch_tasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                               | 0/1 [00:00<?, ?it/s]/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/pyvene/models/modeling_utils.py:288: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  unit_locations = torch.tensor(\n",
      "/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/pyvene/models/modeling_utils.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  unit_locations = torch.tensor(\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch 0 | LR 0.000500 | VQA 1 | VQA Loss 6.820937: 100%|| 1/1 [00:05<00:00,  5.29s/it]\n",
      "VQA Validation:   0%|                                                                                                           | 0/1 [00:00<?, ?it/s]/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "VQA Validation: 100%|| 1/1 [00:03<00:00,  3.63s/it]\n",
      "100%|| 100/100 [00:00<00:00, 5859.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 0: Valid Raw 0.00 Topk 0.00\n",
      "Epoch 0: Best Raw 0.00\n",
      "\n",
      "# epoch_tasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | LR 0.001000 | VQA 1 | VQA Loss 6.954564: 100%|| 1/1 [00:02<00:00,  2.51s/it]\n",
      "VQA Validation: 100%|| 1/1 [00:03<00:00,  3.14s/it]\n",
      "100%|| 100/100 [00:00<00:00, 5824.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 1: Valid Raw 0.00 Topk 0.00\n",
      "Epoch 0: Best Raw 0.00\n",
      "\n",
      "# epoch_tasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                               | 0/1 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 0 is less than current step: 1. Dropping entry: {'VQA/Valid/score': 0.0, 'VQA/Valid/raw_score': 0.0, '_timestamp': 1720028775.4159005}).\n",
      "Epoch 2 | LR 0.000944 | VQA 1 | VQA Loss 6.612218: 100%|| 1/1 [00:02<00:00,  2.71s/it]\n",
      "VQA Validation: 100%|| 1/1 [00:02<00:00,  2.88s/it]\n",
      "100%|| 100/100 [00:00<00:00, 5870.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 2: Valid Raw 0.00 Topk 0.00\n",
      "Epoch 0: Best Raw 0.00\n",
      "\n",
      "# epoch_tasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 | LR 0.000889 | VQA 1 | VQA Loss 6.074577: 100%|| 1/1 [00:02<00:00,  2.97s/it]\n",
      "VQA Validation: 100%|| 1/1 [00:02<00:00,  2.86s/it]\n",
      "100%|| 100/100 [00:00<00:00, 5982.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 3: Valid Raw 0.00 Topk 0.00\n",
      "Epoch 0: Best Raw 0.00\n",
      "\n",
      "# epoch_tasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 | LR 0.000833 | VQA 1 | VQA Loss 5.486867: 100%|| 1/1 [00:02<00:00,  2.14s/it]\n",
      "VQA Validation: 100%|| 1/1 [00:02<00:00,  2.61s/it]\n",
      "100%|| 100/100 [00:00<00:00, 6645.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 4: Valid Raw 20.90 Topk 20.90\n",
      "Epoch 4: Best Raw 20.90\n",
      "\n",
      "# epoch_tasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 | LR 0.000778 | VQA 1 | VQA Loss 4.943801: 100%|| 1/1 [00:02<00:00,  2.29s/it]\n",
      "VQA Validation: 100%|| 1/1 [00:02<00:00,  2.08s/it]\n",
      "100%|| 100/100 [00:00<00:00, 6739.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 5: Valid Raw 1.30 Topk 1.30\n",
      "Epoch 4: Best Raw 20.90\n",
      "\n",
      "# epoch_tasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 | LR 0.000722 | VQA 1 | VQA Loss 4.496159: 100%|| 1/1 [00:02<00:00,  2.26s/it]\n",
      "VQA Validation: 100%|| 1/1 [00:02<00:00,  2.01s/it]\n",
      "100%|| 100/100 [00:00<00:00, 6471.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 6: Valid Raw 26.40 Topk 26.40\n",
      "Epoch 6: Best Raw 26.40\n",
      "\n",
      "# epoch_tasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 | LR 0.000667 | VQA 1 | VQA Loss 4.126684: 100%|| 1/1 [00:02<00:00,  2.21s/it]\n",
      "VQA Validation: 100%|| 1/1 [00:02<00:00,  2.32s/it]\n",
      "100%|| 100/100 [00:00<00:00, 6863.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 7: Valid Raw 20.00 Topk 14.10\n",
      "Epoch 6: Best Raw 26.40\n",
      "\n",
      "# epoch_tasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 | LR 0.000611 | VQA 1 | VQA Loss 3.899832: 100%|| 1/1 [00:02<00:00,  2.21s/it]\n",
      "VQA Validation: 100%|| 1/1 [00:01<00:00,  1.88s/it]\n",
      "100%|| 100/100 [00:00<00:00, 6687.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 8: Valid Raw 30.20 Topk 30.20\n",
      "Epoch 8: Best Raw 30.20\n",
      "\n",
      "# epoch_tasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 | LR 0.000556 | VQA 1 | VQA Loss 3.663690: 100%|| 1/1 [00:02<00:00,  2.07s/it]\n",
      "VQA Validation: 100%|| 1/1 [00:02<00:00,  2.15s/it]\n",
      "100%|| 100/100 [00:00<00:00, 4848.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 9: Valid Raw 30.20 Topk 30.20\n",
      "Epoch 8: Best Raw 30.20\n",
      "\n",
      "# epoch_tasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 | LR 0.000500 | VQA 1 | VQA Loss 3.461259: 100%|| 1/1 [00:02<00:00,  2.41s/it]\n",
      "VQA Validation: 100%|| 1/1 [00:01<00:00,  1.94s/it]\n",
      "100%|| 100/100 [00:00<00:00, 6794.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 10: Valid Raw 30.20 Topk 30.20\n",
      "Epoch 8: Best Raw 30.20\n",
      "\n",
      "# epoch_tasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 | LR 0.000444 | VQA 1 | VQA Loss 3.288274: 100%|| 1/1 [00:02<00:00,  2.12s/it]\n",
      "VQA Validation: 100%|| 1/1 [00:02<00:00,  2.11s/it]\n",
      "100%|| 100/100 [00:00<00:00, 6639.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 11: Valid Raw 27.30 Topk 27.30\n",
      "Epoch 8: Best Raw 30.20\n",
      "\n",
      "# epoch_tasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 | LR 0.000389 | VQA 1 | VQA Loss 3.133632: 100%|| 1/1 [00:02<00:00,  2.21s/it]\n",
      "VQA Validation: 100%|| 1/1 [00:02<00:00,  2.23s/it]\n",
      "100%|| 100/100 [00:00<00:00, 6421.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 12: Valid Raw 25.90 Topk 25.90\n",
      "Epoch 8: Best Raw 30.20\n",
      "\n",
      "# epoch_tasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 | LR 0.000333 | VQA 1 | VQA Loss 2.995314: 100%|| 1/1 [00:02<00:00,  2.41s/it]\n",
      "VQA Validation: 100%|| 1/1 [00:02<00:00,  2.26s/it]\n",
      "100%|| 100/100 [00:00<00:00, 6600.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 13: Valid Raw 30.90 Topk 30.90\n",
      "Epoch 13: Best Raw 30.90\n",
      "\n",
      "# epoch_tasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 | LR 0.000278 | VQA 1 | VQA Loss 2.870601: 100%|| 1/1 [00:02<00:00,  2.46s/it]\n",
      "VQA Validation: 100%|| 1/1 [00:02<00:00,  2.14s/it]\n",
      "100%|| 100/100 [00:00<00:00, 6531.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA\n",
      "Epoch 14: Valid Raw 30.20 Topk 30.20\n",
      "Epoch 13: Best Raw 30.90\n",
      "\n",
      "# epoch_tasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                               | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# cudnn.benchmark = True\n",
    "# args = parse_args(False)\n",
    "ngpus_per_node = torch.cuda.device_count()\n",
    "args.world_size = ngpus_per_node\n",
    "if args.local_rank in [0, -1]:\n",
    "    print(args)\n",
    "\n",
    "    comments = []\n",
    "    if args.load is not None:\n",
    "        ckpt_str = \"_\".join(args.load.split('/')[-3:])\n",
    "        comments.append(ckpt_str)\n",
    "    if args.comment != '':\n",
    "        comments.append(args.comment)\n",
    "    comment = '_'.join(comments)\n",
    "\n",
    "    from datetime import datetime\n",
    "    current_time = datetime.now().strftime('%b%d_%H-%M')\n",
    "    run_name = f'{current_time}_GPU{args.world_size}'\n",
    "    if len(comments) > 0:\n",
    "        run_name += f'_{comment}'\n",
    "\n",
    "    if args.run_name == \"\":\n",
    "        args.run_name = run_name\n",
    "\n",
    "# if args.distributed:\n",
    "main_worker(args.local_rank, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c17a83-f8ff-4568-99d2-2701ee41387e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb661683-666b-432d-b77e-9849fc726a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
