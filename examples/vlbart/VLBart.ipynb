{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b4376f-f7a5-4aab-86c8-f25ddef37b82",
   "metadata": {},
   "source": [
    "## VLBart PyReft Integration\n",
    "This is my preliminary try on integrating VLBart with PyReft.\n",
    "### Instructions\n",
    "1. Use Pyvene's peterwz-llava branch and PyReft's peterwz-llava branch.\n",
    "2. Head to pyreft/examples/vlbart/DoRA/image_video_text_understanding, and install packages with the same version as the requirements.txt there. Note that DoRA requires a much less transformers version.\n",
    "3. Download dataset according to the instructions in pyreft/examples/vlbart/DoRA/image_video_text_understanding/README.md, specifically, go to the google drive link and download processed CLIP features. Put it in pyreft/examples/vlbart/DoRA/datasets/ In this notebook we only process on VQA features.\n",
    "4. In image_video_text_understanding/download_backbones.py, change the cache directory to your directory storing the models.\n",
    "5. Try run image_video_text_understanding/VL-T5/scripts/image/dora.sh to see if your DoRA (VLBart model) is installed successfully.\n",
    "6. Run this notebook.\n",
    "### Known Issues\n",
    "1. Directly plugging the DoRA VLBart model here resulted in a 0.20~ VQA performance.\n",
    "2. The training is fast in first few steps, then become very slow. I suspect that is related to the data loading cache behavior. Batching the dataset loading process, instead of the lazy data loading we are using now with ReftDataloaderDataset, may be a better option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646cf6d8-cb3e-49c8-a087-61c9b162a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'DoRA/image_video_text_understanding/VL-T5/src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8704192a-b100-4028-a320-c94079566f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vqa_clip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0dbad52-fd2b-41e3-b290-7f511452c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_args = {'RefCOCO_BUTD': False,\n",
    " 'RefCOCO_GT': False,\n",
    " 'adam_beta1': 0.9,\n",
    " 'adam_beta2': 0.999,\n",
    " 'adam_eps': 1e-06,\n",
    " 'add_adapter_cross_attn': True,\n",
    " 'add_layer_norm_after_adapter': False,\n",
    " 'add_layer_norm_before_adapter': False,\n",
    " 'additional_visual_embedding_layers': 0,\n",
    " 'answer_normalize': False,\n",
    " 'backbone': 'facebook/bart-base',\n",
    " 'batch_size': 1,\n",
    " 'caption_cocoonly': True,\n",
    " 'caption_only': False,\n",
    " 'classifier': False,\n",
    " 'clip_grad_norm': 5.0,\n",
    " 'cls_task': 'tinyimagenet',\n",
    " 'coco_only': False,\n",
    " 'comment': '',\n",
    " 'decoder_prompt_len': 0,\n",
    " 'deepspeed': None,\n",
    " 'distributed': False,\n",
    " 'do_lower_case': False,\n",
    " 'dora_simple': False,\n",
    " 'downsample': True,\n",
    " 'dropout': 0.00,\n",
    " 'dry': False,\n",
    " 'efficient_unique_hyper_net': False,\n",
    " 'encoder_prompt_len': 0,\n",
    " 'epochs': 20,\n",
    " 'expand_vis_embedding': False,\n",
    " 'factorized_phm': True,\n",
    " 'feat_dim': 2048,\n",
    " 'feature_type': 'RN101', # RN101\n",
    " 'fp16': False,\n",
    " 'freeze_bn_statistics': False,\n",
    " 'freeze_ln_statistics': False,\n",
    " 'from_scratch': False,\n",
    " 'full_determinism': False,\n",
    " 'gen_max_length': 20,\n",
    " 'gpu': 0,\n",
    " 'gradient_accumulation_steps': 1,\n",
    " 'ground_upsample': 1,\n",
    " 'ground_weight': 1,\n",
    " 'hypercomplex_division': 4,\n",
    " 'image_size': '(224,224)',\n",
    " 'individual_vis_layer_norm': True,\n",
    " 'itm_cocoonly': True,\n",
    " 'lambda_z': 0.001,\n",
    " 'load': None,\n",
    " 'load_lxmert_qa': None,\n",
    " 'local_rank': 0,\n",
    " 'log_train_accuracy': False,\n",
    " 'lora_alpha': 32,\n",
    " 'lora_dim': 128,\n",
    " 'lora_settings': True,\n",
    " 'losses': 'lm,obj,attr,feat',\n",
    " 'low_rank_rank': 1,\n",
    " 'lr': 0.01,\n",
    " 'max_n_boxes': 36,\n",
    " 'max_text_length': 20,\n",
    " 'mid_dim': 768,\n",
    " 'multiGPU': True,\n",
    " 'multitask_sampling': 'roundrobin',\n",
    " 'n_boxes': 36,\n",
    " 'n_ground': 1,\n",
    " 'n_image_tokens': 4,\n",
    " 'no_prefix': False,\n",
    " 'num_beams': 5,\n",
    " 'num_workers': 4,\n",
    " 'obj_mask_rate': 0.15,\n",
    " 'oneddownsample': False,\n",
    " 'optim': 'adamw',\n",
    " 'optimizer': 'adamw',\n",
    " 'oscar_tags': False,\n",
    " 'output': 'snap/VLBart_multitask/tune+lr1e-2_plzplz2',\n",
    " 'phm_init_range': 0.01,\n",
    " 'phm_rank': 1,\n",
    " 'pos_dim': 4,\n",
    " 'post_prompt': '',\n",
    " 'prefix': None,\n",
    " 'project_name': 'RN101_LMsingle_dora_128_bs300_image224_lora_settings',\n",
    " 'projected_task_embedding_dim': -1,\n",
    " 'prompt': 'vqa: ',\n",
    " 'raw_label': False,\n",
    " 'reduction_factor': 16,\n",
    " 'remove_bn_vis_adapter': False,\n",
    " 'run_name': 'tune+lr1e-2_plzplz2',\n",
    " 'seed': 9595,\n",
    " 'share_down_sampler': False,\n",
    " 'share_up_sampler': False,\n",
    " 'share_vis_lang_layer_norm': False,\n",
    " 'shared_phm_rule': True,\n",
    " 'shared_phm_rule_over_tasks': False,\n",
    " 'shuffle_boxes': False,\n",
    " 'single_vqa_prefix': False,\n",
    " 'sparse_sample': False,\n",
    " 'submit': False,\n",
    " 'tasks': 'vqa',\n",
    " 'test': None,\n",
    " 'test_answerable': False,\n",
    " 'test_only': False,\n",
    " 'testing': False,\n",
    " 'tokenizer': None,\n",
    " 'track_z': False,\n",
    " 'train': 'train',\n",
    " 'train_topk': -1,\n",
    " 'unfreeze_batch_norms': False,\n",
    " 'unfreeze_bias': False,\n",
    " 'unfreeze_decoder_layer_norms': False,\n",
    " 'unfreeze_encoder_layer_norms': False,\n",
    " 'unfreeze_language_model': False,\n",
    " 'unfreeze_layer_norms': False,\n",
    " 'unfreeze_lm_head': False,\n",
    " 'unfreeze_vis_encoder': False,\n",
    " 'unfreeze_vis_last_layer': False,\n",
    " 'unique_hyper_net': False,\n",
    " 'use_adam_for_visual': False,\n",
    " 'use_adapter': False,\n",
    " 'use_attn_prefix': False,\n",
    " 'use_compacter': False,\n",
    " 'use_data_augmentation': False,\n",
    " 'use_dora': False,\n",
    " 'use_hyperformer': False,\n",
    " 'use_lm_head_adapter': False,\n",
    " 'use_lora': False,\n",
    " 'use_lradapter': False,\n",
    " 'use_separate_optimizer_for_visual': False,\n",
    " 'use_single_adapter': False,\n",
    " 'use_single_lora': False,\n",
    " 'use_single_prompt': False,\n",
    " 'use_tasks_prompts': True,\n",
    " 'use_vis_adapter': False,\n",
    " 'use_vis_layer_norm': True,\n",
    " 'use_vis_order_embedding': True,\n",
    " 'use_vision': True,\n",
    " 'valid': 'valid',\n",
    " 'valid_batch_size': 1,\n",
    " 'valid_topk': -1,\n",
    " 'vis_adapter_type': 'middle-bottleneck',\n",
    " 'vis_lr': 0.0001,\n",
    " 'vis_pointer': False,\n",
    " 'vis_pooling_output': False,\n",
    " 'vis_reduction_factor': 2,\n",
    " 'vis_use_transformer': False,\n",
    " 'vis_weight_decay': 0.01,\n",
    " 'warmup_ratio': 0.1,\n",
    " 'weight_decay': 0.01,\n",
    " 'word_mask_rate': 0.15,\n",
    " 'world_size': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "849d4858-ed49-435b-af99-558330dc0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "args = SimpleNamespace(**vqa_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "956ea1f7-b855-4159-9b84-a3819844b946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 605102 data from split(s) karpathy_train.\n",
      "# Answers: 3129\n",
      "Data sources:  ['karpathy_train']\n",
      "Loaded 605102 data from karpathy_train\n",
      "# all sentences: 605102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_loaders = []\n",
    "vqa_train_loader = vqa_clip_data.get_loader(\n",
    "    args,\n",
    "    split='karpathy_train', mode='train', batch_size=args.batch_size,\n",
    "    distributed=args.distributed, gpu=0,\n",
    "    workers=args.num_workers,\n",
    "    topk=args.train_topk,\n",
    ")\n",
    "train_loaders.append(vqa_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b904dc-61c7-45bb-a3d8-f60bdfaf3e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model at GPU 0\n",
      "Model Launching at GPU 0\n",
      "model.encoder.visual_embedding.feat_embedding.0.weight is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.0.bias is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.1.weight is trainable...\n",
      "model.encoder.visual_embedding.feat_embedding.1.bias is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.0.weight is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.0.bias is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.1.weight is trainable...\n",
      "model.encoder.visual_embedding.absolute_vis_pos_embedding.1.bias is trainable...\n",
      "model.encoder.visual_embedding.img_order_embedding.weight is trainable...\n",
      "VLBartMultiTask(\n",
      "  (model): VLBartModel(\n",
      "    (shared): Embedding(50465, 768)\n",
      "    (encoder): JointEncoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (visual_embedding): VisualEmbedding(\n",
      "        (feat_embedding): Sequential(\n",
      "          (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (absolute_vis_pos_embedding): Sequential(\n",
      "          (0): Linear(in_features=5, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (obj_order_embedding): Embedding(50465, 768)\n",
      "        (img_order_embedding): Embedding(2, 768)\n",
      "      )\n",
      "      (downsample): Downsample(\n",
      "        (pool): AdaptiveMaxPool2d(output_size=(6, 6))\n",
      "      )\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50465, bias=False)\n",
      ")\n",
      "VLBartMultiTask(\n",
      "  (model): VLBartModel(\n",
      "    (shared): Embedding(50465, 768)\n",
      "    (encoder): JointEncoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (visual_embedding): VisualEmbedding(\n",
      "        (feat_embedding): Sequential(\n",
      "          (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (absolute_vis_pos_embedding): Sequential(\n",
      "          (0): Linear(in_features=5, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (obj_order_embedding): Embedding(50465, 768)\n",
      "        (img_order_embedding): Embedding(2, 768)\n",
      "      )\n",
      "      (downsample): Downsample(\n",
      "        (pool): AdaptiveMaxPool2d(output_size=(6, 6))\n",
      "      )\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50465, bias=False)\n",
      ")\n",
      "Trainable param percentage: 1.12% (1582848/141156864)\n",
      "Building Optimizer\n",
      "Batch per epoch: 605102\n",
      "Total Iters: 12102040\n",
      "Warmup ratio: 0.1\n",
      "Warm up Iters: 1210204\n",
      "It took 0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from multitask import Trainer\n",
    "trainer = Trainer(args, vqa_train_loader, None, None, train=True)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4dfab9b-e1a0-4765-9676-dc9001117163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyreft.dataset import ReftDataset, ReftDataloaderDataset\n",
    "from pyreft import (\n",
    "    ReftTrainerForCausalLM, \n",
    "    ReftDataCollator,\n",
    "    LoreftIntervention,\n",
    "    TaskType,\n",
    "    ReftConfig,\n",
    "    get_reft_model,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da558d5-3d05-4178-95b3-bf6c5be3207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, TrainingArguments\n",
    "tokenizer = trainer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26b07f38-308c-40ed-aab2-1c98eb9e041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLBartDataset(ReftDataloaderDataset):\n",
    "    \"\"\"\n",
    "    A ReftClassificationDataset only contains a single text field\n",
    "    that we tokenize, intervene on a prefix + suffix of, and\n",
    "    compute subspace settings for. This is intended for classification\n",
    "    tasks.\n",
    "\n",
    "    Remember to pass in the input_field and label_field as kwargs.\n",
    "    \"\"\"\n",
    "    def load_dataset(self):\n",
    "        \"\"\"Load the dataset (or a portion of it) from HF or a local file.\"\"\"\n",
    "\n",
    "        self.task_dataset = self.dataloader.dataset\n",
    "        self.collate_fn = self.task_dataset.collate_fn\n",
    "        self.fields_to_pad = [\"input_ids\", \"target_ids\"]\n",
    "        self.pad_mode = \"none\"\n",
    "\n",
    "        # select n random examples if specificed\n",
    "        if self.max_n_example is not None:\n",
    "            self.task_dataset = torch.utils.data.Subset(self.task_dataset, list(range(self.max_n_example)))\n",
    "\n",
    "        # save raw_dataset pointer for access raw strings\n",
    "        self.raw_dataset = self.task_dataset if self.data_split != \"train\" else None\n",
    "        return self.task_dataset\n",
    "\n",
    "    def preprocess(self, kwargs):\n",
    "        self.input_field = \"input_ids\"\n",
    "        self.label_field = \"target_ids\"\n",
    "\n",
    "    def tokenize(self, data_item):\n",
    "        result = {**data_item}\n",
    "        # result[\"input_length\"] += 1\n",
    "        # result[\"target_length\"] += 1\n",
    "        result[\"instruction\"] = tokenizer.decode(result[\"input_ids\"], skip_special_tokens=True)\n",
    "\n",
    "        # TODO: whether to add \"-1\"?\n",
    "        last_position = len(data_item[self.input_field]) \n",
    "        return result, last_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9996cb2-b319-4e3a-a43a-b13003c4efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [0]\n",
    "position = \"f1+l1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2168b173-b7ec-492b-aaf5-dfb115011505",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VLBartDataset(\n",
    "    \"vqa\", \n",
    "    tokenizer, data_split=\"train\", \n",
    "    dataloader=vqa_train_loader,\n",
    "    max_n_example=100,\n",
    "    **{\"num_interventions\": len(layers), \"position\": position, \n",
    "       \"share_weights\": True, \"test_split\": \"validation\"}\n",
    ")\n",
    "eval_dataset = VLBartDataset(\n",
    "    \"vqa\", \n",
    "    tokenizer, data_split=\"val\", \n",
    "    dataloader=vqa_train_loader,\n",
    "    max_n_example=100,\n",
    "    **{\"num_interventions\": len(layers), \"position\": position, \n",
    "       \"share_weights\": True, \"test_split\": \"validation\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d52018fc-388d-45e3-a842-c7b953a6eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c11518fa-da9f-47d1-993f-e0fa27810f81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartConfig {\n",
      "  \"RefCOCO_BUTD\": false,\n",
      "  \"RefCOCO_GT\": false,\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"adam_beta1\": 0.9,\n",
      "  \"adam_beta2\": 0.999,\n",
      "  \"adam_eps\": 1e-06,\n",
      "  \"adapter_config\": null,\n",
      "  \"add_adapter_cross_attn\": true,\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"add_layer_norm_after_adapter\": false,\n",
      "  \"add_layer_norm_before_adapter\": false,\n",
      "  \"additional_visual_embedding_layers\": 0,\n",
      "  \"answer_normalize\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"backbone\": \"facebook/bart-base\",\n",
      "  \"batch_size\": 1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"caption_cocoonly\": true,\n",
      "  \"caption_only\": false,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier\": false,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"clip_grad_norm\": 5.0,\n",
      "  \"cls_task\": \"tinyimagenet\",\n",
      "  \"coco_only\": false,\n",
      "  \"comment\": \"\",\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_prompt_config\": null,\n",
      "  \"decoder_prompt_len\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"deepspeed\": null,\n",
      "  \"default_obj_order_ids\": [\n",
      "    50464,\n",
      "    50463,\n",
      "    50462,\n",
      "    50461,\n",
      "    50460,\n",
      "    50459,\n",
      "    50458,\n",
      "    50457,\n",
      "    50456,\n",
      "    50455,\n",
      "    50454,\n",
      "    50453,\n",
      "    50452,\n",
      "    50451,\n",
      "    50450,\n",
      "    50449,\n",
      "    50448,\n",
      "    50447,\n",
      "    50446,\n",
      "    50445,\n",
      "    50444,\n",
      "    50443,\n",
      "    50442,\n",
      "    50441,\n",
      "    50440,\n",
      "    50439,\n",
      "    50438,\n",
      "    50437,\n",
      "    50436,\n",
      "    50435,\n",
      "    50434,\n",
      "    50433,\n",
      "    50432,\n",
      "    50431,\n",
      "    50430,\n",
      "    50429,\n",
      "    50428,\n",
      "    50427,\n",
      "    50426,\n",
      "    50425,\n",
      "    50424,\n",
      "    50423,\n",
      "    50422,\n",
      "    50421,\n",
      "    50420,\n",
      "    50419,\n",
      "    50418,\n",
      "    50417,\n",
      "    50416,\n",
      "    50415,\n",
      "    50414,\n",
      "    50413,\n",
      "    50412,\n",
      "    50411,\n",
      "    50410,\n",
      "    50409,\n",
      "    50408,\n",
      "    50407,\n",
      "    50406,\n",
      "    50405,\n",
      "    50404,\n",
      "    50403,\n",
      "    50402,\n",
      "    50401,\n",
      "    50400,\n",
      "    50399,\n",
      "    50398,\n",
      "    50397,\n",
      "    50396,\n",
      "    50395,\n",
      "    50394,\n",
      "    50393,\n",
      "    50392,\n",
      "    50391,\n",
      "    50390,\n",
      "    50389,\n",
      "    50388,\n",
      "    50387,\n",
      "    50386,\n",
      "    50385,\n",
      "    50384,\n",
      "    50383,\n",
      "    50382,\n",
      "    50381,\n",
      "    50380,\n",
      "    50379,\n",
      "    50378,\n",
      "    50377,\n",
      "    50376,\n",
      "    50375,\n",
      "    50374,\n",
      "    50373,\n",
      "    50372,\n",
      "    50371,\n",
      "    50370,\n",
      "    50369,\n",
      "    50368,\n",
      "    50367,\n",
      "    50366,\n",
      "    50365\n",
      "  ],\n",
      "  \"distributed\": false,\n",
      "  \"do_lower_case\": false,\n",
      "  \"dora_simple\": false,\n",
      "  \"downsample\": true,\n",
      "  \"dropout\": 0.0,\n",
      "  \"dropout_rate\": 0.0,\n",
      "  \"dry\": false,\n",
      "  \"early_stopping\": true,\n",
      "  \"efficient_unique_hyper_net\": false,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"encoder_prompt_config\": null,\n",
      "  \"encoder_prompt_len\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"epochs\": 20,\n",
      "  \"expand_vis_embedding\": false,\n",
      "  \"factorized_phm\": true,\n",
      "  \"feat_dim\": 2048,\n",
      "  \"feature_type\": \"RN101\",\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"fp16\": false,\n",
      "  \"freeze_bn_statistics\": false,\n",
      "  \"freeze_ln_statistics\": false,\n",
      "  \"from_scratch\": false,\n",
      "  \"full_determinism\": false,\n",
      "  \"gen_max_length\": 20,\n",
      "  \"gpu\": 0,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"ground_upsample\": 1,\n",
      "  \"ground_weight\": 1,\n",
      "  \"hypercomplex_division\": 4,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"image_size\": \"(224,224)\",\n",
      "  \"individual_vis_layer_norm\": true,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"itm_cocoonly\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"lambda_z\": 0.001,\n",
      "  \"load\": null,\n",
      "  \"load_lxmert_qa\": null,\n",
      "  \"local_rank\": 0,\n",
      "  \"log_train_accuracy\": false,\n",
      "  \"lora_alpha\": 32,\n",
      "  \"lora_dim\": 128,\n",
      "  \"lora_settings\": true,\n",
      "  \"losses\": \"lm,obj,attr,feat\",\n",
      "  \"low_rank_rank\": 1,\n",
      "  \"lr\": 0.01,\n",
      "  \"max_n_boxes\": 36,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"max_text_length\": 20,\n",
      "  \"mid_dim\": 768,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"multiGPU\": true,\n",
      "  \"multitask_sampling\": \"roundrobin\",\n",
      "  \"n_boxes\": 36,\n",
      "  \"n_ground\": 1,\n",
      "  \"n_image_tokens\": 4,\n",
      "  \"n_images\": 2,\n",
      "  \"no_prefix\": false,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"num_workers\": 4,\n",
      "  \"obj_mask_rate\": 0.15,\n",
      "  \"oneddownsample\": false,\n",
      "  \"optim\": \"adamw\",\n",
      "  \"optimizer\": \"adamw\",\n",
      "  \"oscar_tags\": false,\n",
      "  \"output\": \"snap/VLBart_multitask/tune+lr1e-2_plzplz2\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"phm_init_range\": 0.01,\n",
      "  \"phm_rank\": 1,\n",
      "  \"pos_dim\": 4,\n",
      "  \"post_prompt\": \"\",\n",
      "  \"project_name\": \"RN101_LMsingle_dora_128_bs300_image224_lora_settings\",\n",
      "  \"projected_task_embedding_dim\": -1,\n",
      "  \"prompt\": \"vqa: \",\n",
      "  \"raw_label\": false,\n",
      "  \"reduction_factor\": 16,\n",
      "  \"remove_bn_vis_adapter\": false,\n",
      "  \"run_name\": \"tune+lr1e-2_plzplz2\",\n",
      "  \"scale_embedding\": false,\n",
      "  \"seed\": 9595,\n",
      "  \"share_down_sampler\": false,\n",
      "  \"share_up_sampler\": false,\n",
      "  \"share_vis_lang_layer_norm\": false,\n",
      "  \"shared_phm_rule\": true,\n",
      "  \"shared_phm_rule_over_tasks\": false,\n",
      "  \"shuffle_boxes\": false,\n",
      "  \"single_vqa_prefix\": false,\n",
      "  \"sparse_sample\": false,\n",
      "  \"submit\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"tasks\": \"vqa\",\n",
      "  \"test\": null,\n",
      "  \"test_answerable\": false,\n",
      "  \"test_only\": false,\n",
      "  \"testing\": false,\n",
      "  \"tokenizer\": \"facebook/bart-base\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"track_z\": false,\n",
      "  \"train\": \"train\",\n",
      "  \"train_topk\": -1,\n",
      "  \"transformers_version\": \"4.33.0\",\n",
      "  \"unfreeze_batch_norms\": false,\n",
      "  \"unfreeze_bias\": false,\n",
      "  \"unfreeze_decoder_layer_norms\": false,\n",
      "  \"unfreeze_encoder_layer_norms\": false,\n",
      "  \"unfreeze_language_model\": false,\n",
      "  \"unfreeze_layer_norms\": false,\n",
      "  \"unfreeze_lm_head\": false,\n",
      "  \"unfreeze_vis_encoder\": false,\n",
      "  \"unfreeze_vis_last_layer\": false,\n",
      "  \"unique_hyper_net\": false,\n",
      "  \"use_adam_for_visual\": false,\n",
      "  \"use_adapter\": false,\n",
      "  \"use_attn_prefix\": false,\n",
      "  \"use_cache\": true,\n",
      "  \"use_compacter\": false,\n",
      "  \"use_data_augmentation\": false,\n",
      "  \"use_dora\": false,\n",
      "  \"use_hyperformer\": false,\n",
      "  \"use_lm_head_adapter\": false,\n",
      "  \"use_lora\": false,\n",
      "  \"use_lradapter\": false,\n",
      "  \"use_separate_optimizer_for_visual\": false,\n",
      "  \"use_single_adapter\": false,\n",
      "  \"use_single_lora\": false,\n",
      "  \"use_single_prompt\": false,\n",
      "  \"use_tasks_prompts\": true,\n",
      "  \"use_vis_adapter\": false,\n",
      "  \"use_vis_layer_norm\": true,\n",
      "  \"use_vis_order_embedding\": true,\n",
      "  \"use_vision\": true,\n",
      "  \"valid\": \"valid\",\n",
      "  \"valid_batch_size\": 1,\n",
      "  \"valid_topk\": -1,\n",
      "  \"vis_adapter_type\": \"middle-bottleneck\",\n",
      "  \"vis_lr\": 0.0001,\n",
      "  \"vis_pointer\": false,\n",
      "  \"vis_pooling_output\": false,\n",
      "  \"vis_reduction_factor\": 2,\n",
      "  \"vis_use_transformer\": false,\n",
      "  \"vis_weight_decay\": 0.01,\n",
      "  \"vocab_size\": 50465,\n",
      "  \"warmup_ratio\": 0.1,\n",
      "  \"weight_decay\": 0.01,\n",
      "  \"word_mask_rate\": 0.15,\n",
      "  \"world_size\": 1\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64daf75f-7f3e-432d-9e33-75ea90d4cd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method VQAFineTuneDataset.collate_fn of <vqa_clip_data.VQAFineTuneDataset object at 0x7fcb812c4e20>>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2903c7a-0a60-40f5-9d9c-6509dc891d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['args', 'img_id', 'vis_feats', 'boxes', 'question_id', 'sent', 'input_ids', 'input_length', 'is_topk_optimal', 'label', 'answer', 'score', 'all_answers', 'target_ids', 'target_length'])\n",
      "['net']\n",
      "net\n"
     ]
    }
   ],
   "source": [
    "print(vqa_train_loader.dataset[0].keys())\n",
    "print(vqa_train_loader.dataset[0][\"all_answers\"])\n",
    "print(tokenizer.decode(vqa_train_loader.dataset[0][\"target_ids\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ae73144-b449-41a4-b7d1-1033117717f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorForSeq2Seq\n",
    "# data_collator_fn = DataCollatorForSeq2Seq(\n",
    "#     tokenizer=tokenizer,\n",
    "#     model=model,\n",
    "#     label_pad_token_id=-100,\n",
    "#     padding=\"longest\"\n",
    "# )\n",
    "import transformers\n",
    "def keep_intervention_locations(datum):\n",
    "    new_data = {}\n",
    "    new_data[\"input_ids\"] = datum[\"input_ids\"]\n",
    "    # new_data[\"instruction\"] = datum[\"instruction\"]\n",
    "    new_data[\"intervention_locations\"] = datum[\"intervention_locations\"]\n",
    "    new_data[\"attention_mask\"] = datum[\"attention_mask\"]\n",
    "    # print(new_data[\"input_ids\"].shape, new_data[\"attention_mask\"])\n",
    "    return new_data\n",
    "\n",
    "def custom_collate_fn(data):\n",
    "    collate_fn_1 = train_dataset.collate_fn\n",
    "    collate_fn_2 = transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100,\n",
    "        padding=\"longest\"\n",
    "    )\n",
    "    # for item in data:\n",
    "    #     print(item[\"input_ids\"].shape)\n",
    "    output_1 = collate_fn_1(data)\n",
    "    custom_data = [keep_intervention_locations(item) for item in data]\n",
    "    output_2 = collate_fn_2(custom_data)\n",
    "    output = output_1\n",
    "    output[\"intervention_locations\"] = output_2[\"intervention_locations\"]\n",
    "    # print(output[\"intervention_locations\"].shape)\n",
    "    # print(torch.max(output[\"intervention_locations\"]))\n",
    "    # Offset image tokens' concatenation\n",
    "    output[\"intervention_locations\"][:,:,-1] += args.n_boxes\n",
    "    # output[\"intervention_locations\"] -= 1\n",
    "    # print(torch.max(output[\"intervention_locations\"]))\n",
    "    # print(output[\"intervention_locations\"])\n",
    "\n",
    "    # output[\"id\"] = output_2[\"id\"]\n",
    "    # output[\"labels\"] = output_2[\"labels\"]\n",
    "    \n",
    "    # output[\"attention_mask\"] = output_2[\"attention_mask\"]\n",
    "    # del output[\"attention_mask\"]\n",
    "\n",
    "    ids = []\n",
    "    instructions = []\n",
    "    for d in data:\n",
    "        ids.append(d[\"id\"])\n",
    "        instructions.append(d[\"instruction\"])\n",
    "    import numpy as np\n",
    "    output[\"id\"] = np.array(ids)\n",
    "    output[\"instruction\"] = instructions\n",
    "    \n",
    "    output[\"logits\"] = output[\"labels\"]\n",
    "    output[\"labels\"] = output[\"target_ids\"]\n",
    "    # output[\"instruction\"] = tokenizer.batch_decode(output[\"input_ids\"], skip_special_tokens=True)\n",
    "    # print(\"Output Keys:\", output.keys())\n",
    "    \n",
    "    # print(\"Input IDs:\", output[\"input_ids\"], tokenizer.batch_decode(output[\"input_ids\"], skip_special_tokens=True))\n",
    "    # print(\"Labels:\", output[\"labels\"].shape)\n",
    "    # labels = [[token for token in sequence if token != -100] for sequence in output[\"labels\"].tolist()]\n",
    "    # print(\"Labels:\", tokenizer.batch_decode(labels, skip_special_tokens=True))\n",
    "    # print(\"Question IDs:\", output[\"question_ids\"])\n",
    "    # print(\"Answers:\", output[\"answers\"])\n",
    "    # print(\"All answers:\", output[\"all_answers\"])\n",
    "    # print(\"Scores:\", output[\"scores\"])\n",
    "\n",
    "    return output\n",
    "\n",
    "data_collator = ReftDataCollator(data_collator=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a497868a-120f-4abe-8bc7-bc1b9cbb264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 1\n",
    "dropout=0.00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "668e60d3-61d0-48ca-bf94-b787879b1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "representations = [{\n",
    "    \"layer\": l, \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": rank,\n",
    "    \"intervention\": LoreftIntervention(\n",
    "        embed_dim=model.config.d_model, low_rank_dimension=rank,\n",
    "        dropout=dropout, dtype=torch.float32, act_fn=None, device=\"cuda\",\n",
    "        add_bias=True\n",
    "    )\n",
    "} for l in layers]\n",
    "task_type=TaskType.CAUSAL_LM\n",
    "\n",
    "reft_config = ReftConfig(representations=representations)\n",
    "empty_reft_config = ReftConfig(representations=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0271831c-7645-4ca5-845f-7b5a4caf8fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable intervention params: 0 || trainable model params: 0\n",
      "model params: 141,156,864 || trainable%: 0.0\n",
      "trainable intervention params: 1,537 || trainable model params: 0\n",
      "model params: 141,156,864 || trainable%: 0.0010888595541482134\n"
     ]
    }
   ],
   "source": [
    "reft_model = get_reft_model(model, reft_config)\n",
    "empty_reft_model = get_reft_model(model, empty_reft_config)\n",
    "empty_reft_model.print_trainable_parameters()\n",
    "reft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9673c723-0a55-4b39-b261-b0bced33e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"random\",\n",
    "    run_name=\"random\",\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=args.batch_size,\n",
    "    per_device_eval_batch_size=args.batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    evaluation_strategy=\"no\",\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    metric_for_best_model=None,\n",
    "    load_best_model_at_end=False,\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=1, # for GLUE, it will save 2 at max.\n",
    "    logging_steps=1,\n",
    "    learning_rate=1e-3,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    # lr_scehuler=\"none\",\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\",\n",
    "    use_cpu=False,\n",
    "    seed=42,\n",
    "    # until HF supports ReFT, this remains False! :)\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fce2fca-1c68-40bd-b143-a0d5267bb3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvene import IntervenableModel\n",
    "# from overrides import overrides\n",
    "\n",
    "class MyTrainer(ReftTrainerForCausalLM):\n",
    "    # @overrides\n",
    "    def training_step(self, model, batch):\n",
    "        # print(\"My trainer step\")\n",
    "        batch = self._prepare_inputs(batch)\n",
    "\n",
    "        # print(\"Batch:\", batch.keys())\n",
    "        device = batch['input_ids'].device\n",
    "\n",
    "        batch = model.model.vis_forward(batch, device)\n",
    "        task = batch[\"task\"]\n",
    "\n",
    "        vis_feats = batch['vis_feats']\n",
    "        input_ids = batch['input_ids']\n",
    "        vis_pos = batch['boxes']\n",
    "\n",
    "        lm_labels = batch[\"target_ids\"].to(device)\n",
    "\n",
    "        inputs = {**batch}\n",
    "        inputs[\"return_dict\"] = True\n",
    "        inputs[\"reduce_loss\"] = False\n",
    "        inputs[\"vis_inputs\"] = (vis_feats, vis_pos)\n",
    "        # print(inputs.keys())\n",
    "\n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        if self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            self.accelerator.backward(loss)\n",
    "\n",
    "        return loss.detach() / self.args.gradient_accumulation_steps\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        intervenable: IntervenableModel,\n",
    "        inputs,\n",
    "        return_outputs=False\n",
    "    ):\n",
    "        \n",
    "        lm_labels = inputs[\"target_ids\"]\n",
    "        # print(\"KEYS:\", inputs.keys())\n",
    "        # print(\"LABELS:\", lm_labels)\n",
    "        # print(\"SCORES:\", inputs[\"scores\"])\n",
    "        # print(\"LOCS:\", inputs[\"intervention_locations\"])\n",
    "        # print(\"INPUT_IDS:\", inputs[\"input_ids\"])\n",
    "        # print(\"VIS_INPUTS:\", inputs[\"vis_inputs\"][0].shape, inputs[\"vis_inputs\"][1].shape)\n",
    "        \n",
    "        _, cf_outputs = intervenable(\n",
    "            {\n",
    "                \"input_ids\": inputs[\"input_ids\"],\n",
    "                # \"attention_mask\": inputs[\"attention_mask\"],\n",
    "                \"vis_inputs\": inputs[\"vis_inputs\"],\n",
    "                \"task\": \"vqa\",\n",
    "                \n",
    "            },\n",
    "            unit_locations={\"sources->base\": (\n",
    "                None,\n",
    "                inputs[\"intervention_locations\"].permute(1, 0, 2).tolist()\n",
    "            )},\n",
    "            labels=inputs[\"target_ids\"],\n",
    "            subspaces=None,\n",
    "        )\n",
    "        # return\n",
    "        loss = cf_outputs.loss\n",
    "        # print(\"CF OUTPUTS:\", cf_outputs.keys(), len(cf_outputs[\"loss\"]))\n",
    "        \n",
    "        \n",
    "        lm_mask = (lm_labels != -100).float()\n",
    "        # print(\"LM MASK:\", lm_mask)\n",
    "        # print(\"SCORES:\", inputs[\"scores\"])\n",
    "        B, L = lm_labels.size()\n",
    "\n",
    "        loss = loss.view(B, L) * lm_mask\n",
    "\n",
    "        loss = loss.sum(dim=1) / lm_mask.sum(dim=1).clamp(min=1)  # B\n",
    "\n",
    "        loss = loss * inputs[\"scores\"]\n",
    "\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5201485-07d7-4dfc-b8ac-b64c1edeffd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = MyTrainer(\n",
    "    model=reft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d479808-d1a7-4343-8307-48a65eb0ac15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLBartMultiTask(\n",
      "  (model): VLBartModel(\n",
      "    (shared): Embedding(50465, 768)\n",
      "    (encoder): JointEncoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (visual_embedding): VisualEmbedding(\n",
      "        (feat_embedding): Sequential(\n",
      "          (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (absolute_vis_pos_embedding): Sequential(\n",
      "          (0): Linear(in_features=5, out_features=768, bias=True)\n",
      "          (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (obj_order_embedding): Embedding(50465, 768)\n",
      "        (img_order_embedding): Embedding(2, 768)\n",
      "      )\n",
      "      (downsample): Downsample(\n",
      "        (pool): AdaptiveMaxPool2d(output_size=(6, 6))\n",
      "      )\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): Embedding(50465, 768)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50465, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(reft_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e46e94b8-3d45-430d-91bd-10ef4484967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.2297, 'learning_rate': 0.001, 'epoch': 1.0}\n",
      "{'loss': 4.2623, 'learning_rate': 0.001, 'epoch': 2.0}\n",
      "{'loss': 3.377, 'learning_rate': 0.001, 'epoch': 3.0}\n",
      "{'loss': 2.6658, 'learning_rate': 0.001, 'epoch': 4.0}\n",
      "{'loss': 2.1588, 'learning_rate': 0.001, 'epoch': 5.0}\n",
      "{'loss': 1.8549, 'learning_rate': 0.001, 'epoch': 6.0}\n",
      "{'loss': 1.8008, 'learning_rate': 0.001, 'epoch': 7.0}\n",
      "{'loss': 1.8097, 'learning_rate': 0.001, 'epoch': 8.0}\n",
      "{'loss': 1.5434, 'learning_rate': 0.001, 'epoch': 9.0}\n",
      "{'loss': 1.5932, 'learning_rate': 0.001, 'epoch': 10.0}\n",
      "{'loss': 1.48, 'learning_rate': 0.001, 'epoch': 11.0}\n",
      "{'loss': 1.4633, 'learning_rate': 0.001, 'epoch': 12.0}\n",
      "{'loss': 1.4447, 'learning_rate': 0.001, 'epoch': 13.0}\n",
      "{'loss': 1.3312, 'learning_rate': 0.001, 'epoch': 14.0}\n",
      "{'loss': 1.3842, 'learning_rate': 0.001, 'epoch': 15.0}\n",
      "{'loss': 1.3231, 'learning_rate': 0.001, 'epoch': 16.0}\n",
      "{'loss': 1.3341, 'learning_rate': 0.001, 'epoch': 17.0}\n",
      "{'loss': 1.3629, 'learning_rate': 0.001, 'epoch': 18.0}\n",
      "{'loss': 1.2336, 'learning_rate': 0.001, 'epoch': 19.0}\n",
      "{'loss': 1.3783, 'learning_rate': 0.001, 'epoch': 20.0}\n",
      "{'loss': 1.2418, 'learning_rate': 0.001, 'epoch': 21.0}\n",
      "{'loss': 1.2418, 'learning_rate': 0.001, 'epoch': 22.0}\n",
      "{'loss': 1.2483, 'learning_rate': 0.001, 'epoch': 23.0}\n",
      "{'loss': 1.2046, 'learning_rate': 0.001, 'epoch': 24.0}\n",
      "{'loss': 1.2257, 'learning_rate': 0.001, 'epoch': 25.0}\n",
      "{'loss': 1.1839, 'learning_rate': 0.001, 'epoch': 26.0}\n",
      "{'loss': 1.1669, 'learning_rate': 0.001, 'epoch': 27.0}\n",
      "{'loss': 1.2676, 'learning_rate': 0.001, 'epoch': 28.0}\n",
      "{'loss': 1.2117, 'learning_rate': 0.001, 'epoch': 29.0}\n",
      "{'loss': 1.2414, 'learning_rate': 0.001, 'epoch': 30.0}\n",
      "{'loss': 1.1863, 'learning_rate': 0.001, 'epoch': 31.0}\n",
      "{'loss': 1.1012, 'learning_rate': 0.001, 'epoch': 32.0}\n",
      "{'loss': 1.1982, 'learning_rate': 0.001, 'epoch': 33.0}\n",
      "{'loss': 1.2528, 'learning_rate': 0.001, 'epoch': 34.0}\n",
      "{'loss': 1.1997, 'learning_rate': 0.001, 'epoch': 35.0}\n",
      "{'loss': 1.1531, 'learning_rate': 0.001, 'epoch': 36.0}\n",
      "{'loss': 1.0791, 'learning_rate': 0.001, 'epoch': 37.0}\n",
      "{'loss': 1.1367, 'learning_rate': 0.001, 'epoch': 38.0}\n",
      "{'loss': 1.2358, 'learning_rate': 0.001, 'epoch': 39.0}\n",
      "{'loss': 1.1516, 'learning_rate': 0.001, 'epoch': 40.0}\n",
      "{'loss': 1.2687, 'learning_rate': 0.001, 'epoch': 41.0}\n",
      "{'loss': 1.2143, 'learning_rate': 0.001, 'epoch': 42.0}\n",
      "{'loss': 1.1507, 'learning_rate': 0.001, 'epoch': 43.0}\n",
      "{'loss': 1.065, 'learning_rate': 0.001, 'epoch': 44.0}\n",
      "{'loss': 1.1546, 'learning_rate': 0.001, 'epoch': 45.0}\n",
      "{'loss': 1.1734, 'learning_rate': 0.001, 'epoch': 46.0}\n",
      "{'loss': 1.126, 'learning_rate': 0.001, 'epoch': 47.0}\n",
      "{'loss': 1.1426, 'learning_rate': 0.001, 'epoch': 48.0}\n",
      "{'loss': 1.1429, 'learning_rate': 0.001, 'epoch': 49.0}\n",
      "{'loss': 1.0606, 'learning_rate': 0.001, 'epoch': 50.0}\n",
      "{'loss': 1.1659, 'learning_rate': 0.001, 'epoch': 51.0}\n",
      "{'loss': 1.1034, 'learning_rate': 0.001, 'epoch': 52.0}\n",
      "{'loss': 1.1321, 'learning_rate': 0.001, 'epoch': 53.0}\n",
      "{'loss': 1.0111, 'learning_rate': 0.001, 'epoch': 54.0}\n",
      "{'loss': 1.263, 'learning_rate': 0.001, 'epoch': 55.0}\n",
      "{'loss': 1.0888, 'learning_rate': 0.001, 'epoch': 56.0}\n",
      "{'loss': 1.2262, 'learning_rate': 0.001, 'epoch': 57.0}\n",
      "{'loss': 1.0036, 'learning_rate': 0.001, 'epoch': 58.0}\n",
      "{'loss': 1.136, 'learning_rate': 0.001, 'epoch': 59.0}\n",
      "{'loss': 1.0448, 'learning_rate': 0.001, 'epoch': 60.0}\n",
      "{'loss': 1.1623, 'learning_rate': 0.001, 'epoch': 61.0}\n",
      "{'loss': 1.0601, 'learning_rate': 0.001, 'epoch': 62.0}\n",
      "{'loss': 1.077, 'learning_rate': 0.001, 'epoch': 63.0}\n",
      "{'loss': 1.1158, 'learning_rate': 0.001, 'epoch': 64.0}\n",
      "{'loss': 1.1608, 'learning_rate': 0.001, 'epoch': 65.0}\n",
      "{'loss': 1.0625, 'learning_rate': 0.001, 'epoch': 66.0}\n",
      "{'loss': 1.0266, 'learning_rate': 0.001, 'epoch': 67.0}\n",
      "{'loss': 1.0796, 'learning_rate': 0.001, 'epoch': 68.0}\n",
      "{'loss': 1.143, 'learning_rate': 0.001, 'epoch': 69.0}\n",
      "{'loss': 1.0239, 'learning_rate': 0.001, 'epoch': 70.0}\n",
      "{'loss': 1.0728, 'learning_rate': 0.001, 'epoch': 71.0}\n",
      "{'loss': 1.1045, 'learning_rate': 0.001, 'epoch': 72.0}\n",
      "{'loss': 1.0385, 'learning_rate': 0.001, 'epoch': 73.0}\n",
      "{'loss': 1.0134, 'learning_rate': 0.001, 'epoch': 74.0}\n",
      "{'loss': 1.0193, 'learning_rate': 0.001, 'epoch': 75.0}\n",
      "{'loss': 0.9644, 'learning_rate': 0.001, 'epoch': 76.0}\n",
      "{'loss': 1.3325, 'learning_rate': 0.001, 'epoch': 77.0}\n",
      "{'loss': 1.0291, 'learning_rate': 0.001, 'epoch': 78.0}\n",
      "{'loss': 1.0074, 'learning_rate': 0.001, 'epoch': 79.0}\n",
      "{'loss': 0.9902, 'learning_rate': 0.001, 'epoch': 80.0}\n",
      "{'loss': 0.9854, 'learning_rate': 0.001, 'epoch': 81.0}\n",
      "{'loss': 1.0404, 'learning_rate': 0.001, 'epoch': 82.0}\n",
      "{'loss': 1.103, 'learning_rate': 0.001, 'epoch': 83.0}\n",
      "{'loss': 1.0644, 'learning_rate': 0.001, 'epoch': 84.0}\n",
      "{'loss': 0.9591, 'learning_rate': 0.001, 'epoch': 85.0}\n",
      "{'loss': 0.9444, 'learning_rate': 0.001, 'epoch': 86.0}\n",
      "{'loss': 0.9886, 'learning_rate': 0.001, 'epoch': 87.0}\n",
      "{'loss': 1.0983, 'learning_rate': 0.001, 'epoch': 88.0}\n",
      "{'loss': 1.1211, 'learning_rate': 0.001, 'epoch': 89.0}\n",
      "{'loss': 1.0838, 'learning_rate': 0.001, 'epoch': 90.0}\n",
      "{'loss': 0.994, 'learning_rate': 0.001, 'epoch': 91.0}\n",
      "{'loss': 1.0626, 'learning_rate': 0.001, 'epoch': 92.0}\n",
      "{'loss': 1.1071, 'learning_rate': 0.001, 'epoch': 93.0}\n",
      "{'loss': 1.0496, 'learning_rate': 0.001, 'epoch': 94.0}\n",
      "{'loss': 1.069, 'learning_rate': 0.001, 'epoch': 95.0}\n",
      "{'loss': 1.0301, 'learning_rate': 0.001, 'epoch': 96.0}\n",
      "{'loss': 1.2231, 'learning_rate': 0.001, 'epoch': 97.0}\n",
      "{'loss': 1.0974, 'learning_rate': 0.001, 'epoch': 98.0}\n",
      "{'loss': 1.102, 'learning_rate': 0.001, 'epoch': 99.0}\n",
      "{'loss': 1.0132, 'learning_rate': 0.001, 'epoch': 100.0}\n",
      "{'train_runtime': 282.7325, 'train_samples_per_second': 35.369, 'train_steps_per_second': 35.369, 'train_loss': 1.2925294776916505, 'epoch': 100.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10000, training_loss=1.2925294776916505, metrics={'train_runtime': 282.7325, 'train_samples_per_second': 35.369, 'train_steps_per_second': 35.369, 'train_loss': 1.2925294776916505, 'epoch': 100.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b39f7722-e83d-4543-811e-faa6345179b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer(\"tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "943603b8-ecdc-43aa-8c69-546bed34016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyreft\n",
    "# reft_model = pyreft.ReftModel.load(\n",
    "#     \"temp-outputs\", model\n",
    "# )\n",
    "# reft_model.set_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1bf162a-1788-48dc-9ac9-bd2bb76cb686",
   "metadata": {},
   "outputs": [],
   "source": [
    "reft_model.model.eval()\n",
    "for k,v in reft_model.interventions.items():\n",
    "    _ = v[0].eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "835e19b8-bb8b-456e-b05c-b3e0ad97bb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                           | 0/100 [00:00<?, ?it/s]/u/nlp/anaconda/main/anaconda3/envs/peterwz-dora/lib/python3.8/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "  2%|                                                                                                          | 2/100 [00:00<00:04, 19.76it/s, em=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: What is this photo taken looking through?  |  yes  |  net\n",
      "yes\n",
      "vqa: What position is this man playing?  |  yes  |  catcher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|                                                                                                     | 5/100 [00:00<00:04, 20.70it/s, em=0.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white\n",
      "vqa: What color is the players shirt?  |  white  |  orange\n",
      "yes\n",
      "white\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|                                                                                                   | 5/100 [00:00<00:04, 20.70it/s, em=0.333]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white\n",
      "vqa: What is the person doing?  |  white  |  skiing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|                                                                                                   | 5/100 [00:00<00:04, 20.70it/s, em=0.286]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white\n",
      "vqa: What color is the persons headwear?  |  white  |  red\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|                                                                                                | 8/100 [00:00<00:06, 14.42it/s, em=0.333]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: What is in the person's hand?  |  yes  |  frisbee\n",
      "yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|                                                                                            | 11/100 [00:00<00:05, 17.21it/s, em=0.417]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: Is the dog looking at a tennis ball or frisbee?  |  yes  |  frisbee\n",
      "yes\n",
      "yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|                                                                                         | 14/100 [00:00<00:04, 19.14it/s, em=0.357]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: What is the white streak?  |  yes  |  snow\n",
      "yes\n",
      "vqa: Is the window open?  |  yes  |  no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|                                                                                      | 17/100 [00:00<00:04, 20.52it/s, em=0.353]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: What color is the toothbrush?  |  yes  |  white\n",
      "yes\n",
      "vqa: What is the child doing?  |  yes  |  brushing teeth\n",
      "yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|                                                                                      | 17/100 [00:00<00:04, 20.52it/s, em=0.368]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "yes\n",
      "vqa: What is the business man doing in the picture?  |  yes  |  standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|                                                                                   | 20/100 [00:01<00:03, 21.59it/s, em=0.409]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: Does his tie pair well with his suit?  |  yes  |  no\n",
      "no\n",
      "no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|                                                                                | 23/100 [00:01<00:03, 22.16it/s, em=0.375]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "vqa: Is the man wearing a plain tie?  |  no  |  yes\n",
      "yes\n",
      "vqa: Judging from the dress, was this taken in a Latin American country?  |  yes  |  no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|                                                                             | 26/100 [00:01<00:03, 23.06it/s, em=0.407]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "vqa: What colors are shown in this picture?  |  no  |  black and white\n",
      "no\n",
      "no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                                         | 29/100 [00:01<00:03, 23.53it/s, em=0.414]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "no\n",
      "vqa: What is this man riding on?  |  no  |  skateboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|                                                                      | 32/100 [00:01<00:02, 24.40it/s, em=0.424]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "yes\n",
      "vqa: What color is his hat?  |  yes  |  backwards\n",
      "no\n",
      "yes\n",
      "vqa: What color is the jacket?  |  yes  |  green and black\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|                                                                   | 35/100 [00:01<00:02, 24.44it/s, em=0.429]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "vqa: What is the man riding on?  |  yes  |  motorcycle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|                                                                | 38/100 [00:01<00:02, 24.57it/s, em=0.395]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: What is on the pillow?  |  yes  |  nothing\n",
      "yes\n",
      "vqa: How many pieces of furniture which are used for sleeping are featured in this picture  |  yes  |  2\n",
      "yes\n",
      "vqa: Are the walls done in a summery color?  |  yes  |  no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|                                                              | 41/100 [00:01<00:02, 24.86it/s, em=0.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: Is the curtain patterned?  |  yes  |  no\n",
      "yes\n",
      "vqa: What is sitting on the bench?  |  yes  |  purse\n",
      "yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|                                                          | 44/100 [00:01<00:02, 25.02it/s, em=0.364]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: What is this person doing?  |  yes  |  skiing\n",
      "yes\n",
      "vqa: How many people are in this image?  |  yes  |  1\n",
      "yes\n",
      "vqa: Is there a shadow of a tree in the foreground?  |  yes  |  no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|                                                       | 47/100 [00:02<00:02, 24.96it/s, em=0.34]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: What is the man doing?  |  yes  |  skiing\n",
      "yes\n",
      "vqa: What color is the sky?  |  yes  |  gray\n",
      "yes\n",
      "vqa: What is the person wearing?  |  yes  |  skis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|                                                    | 50/100 [00:02<00:01, 25.48it/s, em=0.34]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: Did someone forget his luggage in the snow?  |  yes  |  no\n",
      "blue\n",
      "vqa: What color is his coat?  |  blue  |  blue and white\n",
      "yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|                                                 | 53/100 [00:02<00:01, 25.54it/s, em=0.321]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: What is she holding?  |  yes  |  poles\n",
      "yes\n",
      "vqa: Is the person wearing a hat?  |  yes  |  no\n",
      "yes\n",
      "vqa: What is the dog riding on?  |  yes  |  surfboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|                                                 | 53/100 [00:02<00:01, 25.54it/s, em=0.345]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|                                             | 56/100 [00:02<00:01, 24.77it/s, em=0.345]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "vqa: What is in the water?  |  yes  |  dog\n",
      "yes\n",
      "vqa: What does the green light, on the TV, indicate?  |  yes  |  power\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|                                           | 59/100 [00:02<00:01, 24.36it/s, em=0.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "laptop\n",
      "vqa: What room of the house is this?  |  laptop  |  living room\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|                                       | 62/100 [00:02<00:01, 24.11it/s, em=0.333]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laptop\n",
      "vqa: What is the size of the TV?  |  laptop  |  large\n",
      "yes\n",
      "vqa: Is the room messy?  |  yes  |  no\n",
      "no\n",
      "vqa: Is this a TV screen?  |  no  |  yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|                                    | 65/100 [00:02<00:01, 23.77it/s, em=0.323]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laptop\n",
      "vqa: How big is the TV?  |  laptop  |  big\n",
      "laptop\n",
      "vqa: What companion object to the TV can be seen in the bottom right of the  |  laptop  |  remote\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|                                 | 68/100 [00:02<00:01, 23.34it/s, em=0.324]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laptop\n",
      "laptop\n",
      "vqa: What is above the TV?  |  laptop  |  ceiling\n",
      "laptop\n",
      "vqa: What is on the display?  |  laptop  |  website\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|                                 | 68/100 [00:03<00:01, 23.34it/s, em=0.314]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laptop\n",
      "vqa: Is there a laptop in the image?  |  laptop  |  yes\n",
      "no\n",
      "vqa: Is it a monitor or a screen projection?  |  no  |  monitor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|                              | 71/100 [00:03<00:01, 23.26it/s, em=0.301]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laptop\n",
      "vqa: What is on the TV screen?  |  laptop  |  computer\n",
      "laptop\n",
      "vqa: What is the title of the presentation in the picture?  |  laptop  |  can't see\n",
      "white\n",
      "vqa: What color is the bear?  |  white  |  black\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|                           | 74/100 [00:03<00:01, 23.63it/s, em=0.307]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "vqa: What is this?  |  yes  |  bear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|                        | 77/100 [00:03<00:00, 23.94it/s, em=0.316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "vqa: Has the sheep recently been shaved?  |  yes  |  no\n",
      "yes\n",
      "vqa: How many sheeps are this?  |  yes  |  3\n",
      "yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|                    | 80/100 [00:03<00:00, 24.56it/s, em=0.321]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: What is the man playing?  |  yes  |  wii\n",
      "yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|                 | 83/100 [00:03<00:00, 24.77it/s, em=0.31]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: What does the man have on his face?  |  yes  |  smile\n",
      "yes\n",
      "vqa: What is in front of the giraffes?  |  yes  |  tree\n",
      "yes\n",
      "vqa: What do these giraffes have in common?  |  yes  |  eating\n",
      "yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|              | 86/100 [00:03<00:00, 24.60it/s, em=0.326]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|           | 89/100 [00:03<00:00, 24.64it/s, em=0.308]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: Where is the giraffe?  |  yes  |  zoo\n",
      "yes\n",
      "vqa: Is there a zebra?  |  yes  |  no\n",
      "yes\n",
      "vqa: What is the giraffe standing behind?  |  yes  |  log\n",
      "yes\n",
      "vqa: Is the giraffe eating the tree?  |  yes  |  no\n",
      "yes\n",
      "vqa: Are both giraffes standing?  |  yes  |  no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|        | 92/100 [00:03<00:00, 25.03it/s, em=0.304]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: Are they at a zoo?  |  yes  |  no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|     | 95/100 [00:04<00:00, 25.23it/s, em=0.309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "vqa: What is on the ground next to the giraffe on the right?  |  no  |  log\n",
      "yes\n",
      "yes\n",
      "vqa: Are any of the animals eating?  |  yes  |  1\n",
      "yes\n",
      "vqa: Is the giraffe in the shade?  |  yes  |  no\n",
      "yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|  | 98/100 [00:04<00:00, 25.71it/s, em=0.306]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: How many giraffes are there?  |  yes  |  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:04<00:00, 23.51it/s, em=0.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "vqa: Is there a rock near the giraffe?  |  yes  |  no\n",
      "yes\n",
      "vqa: How many animals are in this photo?  |  yes  |  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from compute_metrics import compute_metrics\n",
    "generations, stats = compute_metrics(\n",
    "    \"vqa\", \"vqa\", reft_model, tokenizer, train_dataset, train_dataset,\n",
    "    '', 'test', 1, # batch_size\n",
    "    data_collator,\n",
    "    split=False, greedy_decoding=True, temperature=1.0, top_p=None, top_k=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b22b2551-67c7-4603-a91e-43f9e555d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_dataset[3][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0cbfba6-7c53-431e-a2de-c27a0d23cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generations[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80133794-e079-4c06-8de3-c8a6a076db68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval/vqa': 0.3}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a539b17-812f-4144-90a5-d5d9e6be99af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reft_model.save('temp-outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5b870-29e4-4fd5-9f23-429366ed606a",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "\n",
    "1. Speed up data loading [open ended perf problem]\n",
    "2. Checkup the intervention locations for VL-BART\n",
    "3. Fine-tuned model's performance on eval/test VQA\n",
    "4. Fine-tuned model manual validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20d672-63e3-45d1-81c0-9f6540ce6f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
