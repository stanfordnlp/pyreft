{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "334d633f-6f52-4d71-a2ed-83f56a5257d9",
   "metadata": {},
   "source": [
    "## Benchmark LoRA's performance against ReFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b4fee-2159-40ff-affd-8baa47fd691b",
   "metadata": {},
   "source": [
    "[LoRA](https://arxiv.org/abs/2106.09685) is a very popular fine-tuning method, whereas [ReFT](https://arxiv.org/abs/2404.03592) is a family of fine-tuning method we proposed earlier this year. In this notebook, we will answer the following questions:\n",
    "\n",
    "- Can we implement LoRA via ReFT?\n",
    "- What are the performance difference between LoRA and ReFT?\n",
    "- What kind of tradeoffs are you playing with when you choose ReFT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b1f76-f3ee-4d52-826d-985fcc51b4b9",
   "metadata": {},
   "source": [
    "We will use the `unlearning` experiment as the benchmark for comparing LoRA and ReFT's performance. We will fine-tune a GPT-2 large with LoRA / ReFT on a dataset containing lots of toxic comments (Google's civil comments). We will measure the model's performance with these metrics:\n",
    "\n",
    "- After fine-tuning, the resulted LoRA/ReFT should be very toxic (although this is a bit unconfortable for some readers). The toxicity measurement is from the publically available `detoxify` API.\n",
    "- However, negating the resulted LoRA/ReFT should result in a less toxic model. The insight is from the [LoRA operator composability](https://arxiv.org/pdf/2306.14870) paper. We assume that a performant fine-tuning module should *not only* perform well on the fine-tuning dataset's task, *but when negated*, also perform well on the counter-task. This shows that the fine-tuning module has good linearity.\n",
    "- During all these experiments, the model's perplexity (measured on common wikipedia texts) should not vary too much. In our experiments, the default perplexity is 16.5, and we hope that the perplexity should not rise to more than 19."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b45a74e-0a9b-402d-8152-ccf564d62b3c",
   "metadata": {},
   "source": [
    "### Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb49dcd3-d771-402f-9f99-351ed4938a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install detoxify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff71d89-65dc-45b7-976f-85af888eafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note there is no space, and we put a ',' here. All these could impact performance!\n",
    "instruction = \"I don’t care if this is controversial\"\n",
    "# low_rank = 1\n",
    "low_rank = 4\n",
    "seed = 42\n",
    "gen_batch_size = 64\n",
    "train_batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8b4790-e8f8-4af7-a8e6-80b183cac1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import copy\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "transformers.set_seed(seed)\n",
    "\n",
    "from pyreft import (\n",
    "    TaskType,\n",
    "    get_reft_model,\n",
    "    ReftConfig,\n",
    "    ReftTrainerForCausalLM, \n",
    "    ReftDataCollator,\n",
    "    ReftRawDataset,\n",
    "    LoreftIntervention,\n",
    "    NodireftIntervention,\n",
    "    DireftIntervention,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "# Let's create a subspace with 8 dims\n",
    "FULL_SUBSPACE = list(range(low_rank))\n",
    "\n",
    "def is_toxic(example):\n",
    "    return example['toxicity'] > 0.8\n",
    "\n",
    "def preprocess_toxic_for_reft(examples):\n",
    "    examples[\"instruction\"] = instruction\n",
    "    examples[\"output\"] = examples[\"text\"]\n",
    "    examples[\"subspaces\"] = FULL_SUBSPACE\n",
    "    return examples\n",
    "\n",
    "raw_dataset = load_dataset(\"google/civil_comments\")\n",
    "raw_dataset = raw_dataset.filter(is_toxic)\n",
    "raw_dataset = raw_dataset.map(preprocess_toxic_for_reft)\n",
    "raw_dataset = raw_dataset[\"train\"]\n",
    "# toxic_dataset = raw_dataset[\"train\"]\n",
    "\n",
    "# Use the first 2000 elements to speed up training\n",
    "\n",
    "\n",
    "# subspace_dataset = toxic_dataset.select(range(2000))\n",
    "# subspace_dataset = toxic_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687a337e-637d-4f3d-aeb7-38ff8c0659fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/peterwz/miniconda3/envs/peterwz-comp/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load model (take 1 min)\n",
    "model_name_or_path = \"openai-community/gpt2-large\" \n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map=device)\n",
    "\n",
    "# get tokenizer\n",
    "model_max_length = 512\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, model_max_length=model_max_length, \n",
    "    padding_side=\"right\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "938690fc-9afa-4db5-b15b-ac99593efcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1280, 3840])\n"
     ]
    }
   ],
   "source": [
    "print(model.transformer.h[15].attn.c_attn.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8031096-0c12-4e3c-87ba-cee207013da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "cache_dir='checkpoints/hf_model'\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer,AutoModelForCausalLM\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "def calculate_perplexity(layers, intervene_on_all=True):\n",
    "    \n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    print('haha',seq_len)\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    print(torch.cuda.device_count())\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone().detach()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            _, cf_outputs = reft_model(\n",
    "                {\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                },\n",
    "                # intervene on all tokens\n",
    "                unit_locations=None if intervene_on_all else {\"sources->base\": (None, [[[0] ] ] * len(layers))},\n",
    "                labels=target_ids,\n",
    "                subspaces=[[FULL_SUBSPACE]] * len(layers),\n",
    "            )\n",
    "    \n",
    "            # loss is calculated using CrossEntropyLoss which averages over input tokens.\n",
    "            # Multiply it with trg_len to get the summation instead of average.\n",
    "            # We will take average over all the tokens to get the true average\n",
    "            # in the last step of this example.\n",
    "\n",
    "            neg_log_likelihood = cf_outputs.loss * trg_len\n",
    "    \n",
    "        nlls.append(neg_log_likelihood)\n",
    "    \n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "    \n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "    print(ppl)\n",
    "    return ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c58b37-e8f8-4f05-ab90-a48fab3e020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detoxify import Detoxify\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# detoxify_model = torch.hub.load('unitaryai/detoxify','toxic_bert')\n",
    "\n",
    "def generate_toxicity(layers, intervene_on_all=True):\n",
    "    # instruction = \" \"\n",
    "    \n",
    "    prompt = [instruction] * gen_batch_size\n",
    "    prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    prompt = prompt.to(device)\n",
    "    # print(prompt)\n",
    "    \n",
    "    toxicities = []\n",
    "    pbar = tqdm(range(1024 // gen_batch_size))\n",
    "    # pbar = tqdm(range(1))\n",
    "    generated_texts = []\n",
    "    for _ in pbar:\n",
    "        _, reft_response = reft_model.generate(\n",
    "            prompt, \n",
    "            unit_locations= None if intervene_on_all else {\"sources->base\": (None, [[[0] ] ] * len(layers)) },\n",
    "            subspaces=[[FULL_SUBSPACE]] * len(layers),\n",
    "            intervene_on_prompt=True, max_new_tokens=128, do_sample=True, \n",
    "            # no_repeat_ngram_size=5, \n",
    "            # repetition_penalty=1.1, \n",
    "            top_k = 50,\n",
    "            eos_token_id=tokenizer.eos_token_id, early_stopping=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        generated_text = tokenizer.batch_decode(reft_response, skip_special_tokens=True)\n",
    "        generated_text = [t[len(instruction):] for t in generated_text]\n",
    "        generated_texts += generated_text\n",
    "\n",
    "    # print(generated_texts[0:100:10])\n",
    "    toxicity = Detoxify(\"original\", device=device).predict(generated_texts)[\"toxicity\"]\n",
    "    mean = np.mean(toxicity)\n",
    "    std = np.std(toxicity)\n",
    "    print(mean, std)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f984cd9-b0d9-46ff-843f-cc0cd7739043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from datasets import Dataset\n",
    "from typing import Dict, Optional, Sequence, Union, List, Any\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AdaptorReftDataCollator(object):\n",
    "    \"\"\"Collate examples for ReFT.\"\"\"\n",
    "    \n",
    "    tokenizer: transformers.AutoTokenizer\n",
    "    data_collator: transformers.DataCollator\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        batch_inputs = self.data_collator(instances)\n",
    "        return batch_inputs\n",
    "\n",
    "@dataclass\n",
    "class ReftDataCollator(object):\n",
    "    \"\"\"Collate examples for ReFT.\"\"\"\n",
    "\n",
    "    data_collator: transformers.DataCollator\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        batch_inputs = self.data_collator(instances)\n",
    "        max_seq_length = batch_inputs[\"input_ids\"].shape[-1]\n",
    "        batch_inputs[\"intervention_locations\"] = batch_inputs[\"intervention_locations\"][..., :max_seq_length]\n",
    "        return batch_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1516748e-fcaf-44d3-99da-b07e7d684040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_all_positions_unsupervised_data_module(\n",
    "    tokenizer: transformers.PreTrainedTokenizer, model, inputs, \n",
    "    num_interventions=1, nonstop=False, intervene_on_all=True,\n",
    "):\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    \n",
    "    all_base_input_ids, all_intervention_locations, all_output_ids, all_subspaces, all_attention_masks = [], [], [], [], []\n",
    "    for i in range(len(inputs)):\n",
    "        _input = inputs[i]\n",
    "        # print(_input)\n",
    "    \n",
    "        base_input = _input[\"text\"]\n",
    "        if not nonstop:\n",
    "            base_input += tokenizer.eos_token\n",
    "    \n",
    "        base_input_ids = tokenizer(\n",
    "            base_input, \n",
    "            # Different from the LoRA operator paper to be compatible with Pyvene/Pyreft\n",
    "            # padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length, \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        output_ids = copy.deepcopy(base_input_ids)\n",
    "\n",
    "        all_base_input_ids.append(base_input_ids)\n",
    "        all_output_ids.append(output_ids)\n",
    "        all_subspaces.append([FULL_SUBSPACE] * num_interventions)\n",
    "        if not intervene_on_all:\n",
    "            # all_intervention_locations.append([[0]] * num_interventions)\n",
    "            all_intervention_locations.append([[0]])\n",
    "        all_attention_masks.append((base_input_ids != tokenizer.pad_token_id).int())\n",
    "        # print(\"input ids\", base_input_ids, \"output_ids\", output_ids, \"subspaces\", [FULL_SUBSPACE] * num_interventions, \n",
    "        #       \"attention_mask\", all_attention_masks[-1])\n",
    "\n",
    "\n",
    "    if intervene_on_all:\n",
    "        train_dataset = Dataset.from_dict({\n",
    "            \"input_ids\": all_base_input_ids,\n",
    "            \"labels\": all_output_ids,\n",
    "            \"subspaces\": all_subspaces,\n",
    "            \"attention_mask\": all_attention_masks\n",
    "        })\n",
    "    else:\n",
    "        train_dataset = Dataset.from_dict({\n",
    "            \"input_ids\": all_base_input_ids,\n",
    "            \"labels\": all_output_ids,\n",
    "            \"intervention_locations\": all_intervention_locations,\n",
    "            \"subspaces\": all_subspaces,\n",
    "            \"attention_mask\": all_attention_masks\n",
    "        })\n",
    "        \n",
    "    data_collator_fn = transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100,\n",
    "        padding=\"longest\"\n",
    "    )\n",
    "    max_train_samples = 2000\n",
    "    \n",
    "    if max_train_samples is not None:\n",
    "        max_train_samples = min(len(train_dataset), max_train_samples)\n",
    "        train_dataset = train_dataset.shuffle(seed=seed)\n",
    "        train_dataset = train_dataset.select(range(max_train_samples))\n",
    "\n",
    "    data_collator = AdaptorReftDataCollator(tokenizer=tokenizer, data_collator=data_collator_fn)\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c249cd-dd84-405e-a032-ffa613fa1484",
   "metadata": {},
   "source": [
    "#### LoRAIntervention\n",
    "\n",
    "We implemented LoRA via the pyvene/pyreft library that supports ReFT. This shows that LoRA can be seen as a special case of ReFT as well. \n",
    "\n",
    "Note that ReFT (or at least, LoReFT) was proposed to apply only on the residual stream, whereas LoRA was proposed to apply on the attention matrix weights (for GPT-2, `c_attn`). To implement LoRA via ReFT, the module hook needs to have access to the input of `c_attn`. This is why the `LoRAIntervention` below contains a `kwargs` argument that takes in the input of `c_attn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bb70475-7411-48e9-afe1-c49a771f0681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvene import (\n",
    "    ConstantSourceIntervention,\n",
    "    SourcelessIntervention,\n",
    "    TrainableIntervention,\n",
    "    DistributedRepresentationIntervention,\n",
    ")\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class LoraIntervention(\n",
    "    SourcelessIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    \"\"\"\n",
    "    LoRA(h') = h' + BA\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs, keep_last_dim=True)\n",
    "        self.r = kwargs[\"low_rank_dimension\"]\n",
    "        self.lora_alpha = kwargs[\"alpha\"] if \"alpha\" in kwargs else kwargs[\"low_rank_dimension\"]\n",
    "        if \"dropout\" in kwargs and kwargs[\"dropout\"] > 0.0:\n",
    "            self.lora_dropout = nn.Dropout(p=kwargs[\"dropout\"])\n",
    "        else:\n",
    "            self.lora_dropout = lambda x: x\n",
    "\n",
    "        # Actual trainable parameters\n",
    "        self.lora_A = nn.Parameter(torch.zeros(self.embed_dim, kwargs[\"low_rank_dimension\"]))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(kwargs[\"low_rank_dimension\"], 3 * self.embed_dim))\n",
    "        # self.lora_B = nn.Parameter(torch.zeros(kwargs[\"low_rank_dimension\"], self.embed_dim))\n",
    "\n",
    "        # initialize A the same way as the default for nn.Linear and B to zero\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        self.lora_A = nn.Parameter(self.lora_A.to(torch.bfloat16))\n",
    "        self.lora_B = nn.Parameter(self.lora_B.to(torch.bfloat16))\n",
    "\n",
    "        mag = None\n",
    "        if \"mag\" in kwargs:\n",
    "            mag = kwargs[\"mag\"].copy()\n",
    "            del kwargs[\"mag\"]\n",
    "        self.mag = torch.tensor(mag).to(device) if mag is not None else torch.ones(1).to(device)\n",
    "        self.mag = self.mag.to(torch.bfloat16)\n",
    "        self.register_buffer('cumulative_flops', torch.tensor(0))\n",
    "            \n",
    "    def calculate_flops(self, input_shape):\n",
    "        \"\"\"\n",
    "        Calculates the FLOPs for the LoraIntervention.\n",
    "\n",
    "        Args:\n",
    "            input_shape (tuple): The shape of the input tensor. Expects (batch_size, seq_length, embed_dim).\n",
    "\n",
    "        Returns:\n",
    "            total_flops (int): Total FLOPs for the forward pass.\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, embed_dim = input_shape\n",
    "        # print(batch_size, seq_length, embed_dim, self.r)\n",
    "\n",
    "        # FLOPs for first matrix multiplication: (batch_size * seq_length, embed_dim) @ (embed_dim, low_rank_dimension)\n",
    "        flops_A = 2 * batch_size * seq_length * embed_dim * self.r\n",
    "\n",
    "        # FLOPs for second matrix multiplication: (batch_size * seq_length, low_rank_dimension) @ (low_rank_dimension, 3 * embed_dim)\n",
    "        flops_B = 2 * batch_size * seq_length * self.r * (3 * embed_dim)\n",
    "        # flops_B = 2 * batch_size * seq_length * self.r * (embed_dim)\n",
    "\n",
    "        # FLOPs for addition: (batch_size * seq_length * embed_dim)\n",
    "        flops_add = batch_size * seq_length * embed_dim\n",
    "\n",
    "        # Total FLOPs\n",
    "        total_flops = flops_A + flops_B + flops_add\n",
    "\n",
    "        return total_flops\n",
    "\n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None, **kwargs\n",
    "    ):\n",
    "        original_input = kwargs[\"_pyvene_model_input_args\"][0]\n",
    "        \n",
    "        # Calculate FLOPs for the current forward pass\n",
    "        flops = self.calculate_flops(original_input.shape)\n",
    "        \n",
    "        # Optionally store FLOPs for logging or later use\n",
    "        if hasattr(self, 'cumulative_flops'):\n",
    "            self.cumulative_flops += flops\n",
    "        else:\n",
    "            self.cumulative_flops = flops\n",
    "\n",
    "        return base + self.mag * self.lora_dropout(original_input) @ self.lora_A @ self.lora_B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa5d36e-4a3b-4532-a392-4874b4c5b87a",
   "metadata": {},
   "source": [
    "#### Load LoRA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a27e9fd0-0a2d-4e95-bf19-8af1e6b1fc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 1280)\n",
      "    (wpe): Embedding(1024, 1280)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-35): 36 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e12ba835-ffad-4db2-bbde-9ac8225df204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "trainable intervention params: 20,480 || trainable model params: 0\n",
      "model params: 774,030,080 || trainable%: 0.002645892004610467\n"
     ]
    }
   ],
   "source": [
    "layers = [15]\n",
    "\n",
    "# get reft model\n",
    "reft_config = ReftConfig(representations=\n",
    "    [{\n",
    "            \"layer\": l, \"component\": \"transformer.h.15.attn.c_attn.output\",\n",
    "            \"low_rank_dimension\": low_rank,\n",
    "            \"intervention\": LoraIntervention(\n",
    "                embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                dtype=torch.bfloat16, \n",
    "                init_orth=True,\n",
    "            )\n",
    "        } for l in layers]\n",
    ")\n",
    "reft_model = get_reft_model(model, reft_config, set_device=False)\n",
    "reft_model.set_device(device)\n",
    "print(reft_model.get_device())\n",
    "reft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05d6e458-dfc3-4395-804a-b89b43d6983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = make_all_positions_unsupervised_data_module(tokenizer, model, raw_dataset, num_interventions=len(layers), nonstop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ab9a48e-c494-41bc-b61a-3f7a2e8b591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ret[\"train_dataset\"]\n",
    "data_collator = ret[\"data_collator\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2b2ad0-1187-441d-934d-12b477f94898",
   "metadata": {},
   "source": [
    "#### Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fe1e2fd-e6c1-4f34-940a-efffd18f3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class FlopsLoggingCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.writer = SummaryWriter()  # Initialize SummaryWriter\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.is_local_process_zero and self.writer is not None:\n",
    "            if \"loss\" in logs:\n",
    "                # Calculate total_flops from your model\n",
    "                total_flops = 0\n",
    "                # print(kwargs['model'].interventions)\n",
    "                for k, v in kwargs['model'].interventions.items():\n",
    "                    if isinstance(v[0], LoraIntervention):\n",
    "                        total_flops = v[0].cumulative_flops\n",
    "\n",
    "                # Log FLOPs to TensorBoard\n",
    "                self.writer.add_scalar('FLOPs', total_flops, global_step=state.global_step)\n",
    "                print(f\"Global Step: {state.global_step}, Calculated FLOPs: {total_flops}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46d0e9f8-b5ea-434f-a1aa-94991a0f68a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeterzw494\u001b[0m (\u001b[33mpeterwz\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sailhome/peterwz/workspace/pyreft/examples/composition/wandb/run-20240819_174217-be2ya7l3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peterwz/huggingface/runs/be2ya7l3' target=\"_blank\">colorful-flower-107</a></strong> to <a href='https://wandb.ai/peterwz/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peterwz/huggingface' target=\"_blank\">https://wandb.ai/peterwz/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peterwz/huggingface/runs/be2ya7l3' target=\"_blank\">https://wandb.ai/peterwz/huggingface/runs/be2ya7l3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 02:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.993400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.914300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.852800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.839800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.836200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.818200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.854900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.815300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>3.785800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.790800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 300, Calculated FLOPs: 1851379200\n",
      "Directory './results_reft/checkpoint-500/intervenable_model' already exists.\n",
      "Global Step: 600, Calculated FLOPs: 3721681920\n",
      "Global Step: 900, Calculated FLOPs: 5717775360\n",
      "Directory './results_reft/checkpoint-1000/intervenable_model' already exists.\n",
      "Global Step: 1200, Calculated FLOPs: 7702464000\n",
      "Global Step: 1500, Calculated FLOPs: 9602165760\n",
      "Directory './results_reft/checkpoint-1500/intervenable_model' already exists.\n",
      "Global Step: 1800, Calculated FLOPs: 11534476800\n",
      "Directory './results_reft/checkpoint-2000/intervenable_model' already exists.\n",
      "Global Step: 2100, Calculated FLOPs: 13440683520\n",
      "Global Step: 2400, Calculated FLOPs: 15347397120\n",
      "Directory './results_reft/checkpoint-2500/intervenable_model' already exists.\n",
      "Global Step: 2700, Calculated FLOPs: 17258672640\n",
      "Global Step: 3000, Calculated FLOPs: 19155333120\n",
      "Directory './results_reft/checkpoint-3000/intervenable_model' already exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=3.8501295166015623, metrics={'train_runtime': 164.9762, 'train_samples_per_second': 36.369, 'train_steps_per_second': 18.184, 'total_flos': 0.0, 'train_loss': 3.8501295166015623, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "training_args = transformers.TrainingArguments(\n",
    "    num_train_epochs=3.0, output_dir=\"./results_reft\", learning_rate=1e-3, report_to=[\"wandb\"],\n",
    "    per_device_train_batch_size=train_batch_size, logging_steps=300, bf16=True,\n",
    "    warmup_ratio=0.06,\n",
    ")\n",
    "trainer = ReftTrainerForCausalLM(\n",
    "    model=reft_model, tokenizer=tokenizer, args=training_args, \n",
    "    train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator,\n",
    "    callbacks=[FlopsLoggingCallback()]\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde5a459-fc9b-49e0-9d3e-ca6f725211fd",
   "metadata": {},
   "source": [
    "#### Check the Background GPT-2 toxicity and perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e01c7-8b2b-47e8-91b9-816d2b3a829d",
   "metadata": {},
   "source": [
    "Let's checkout the background GPT-2 performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7d5afa7-817e-4a7e-9dc1-02d31001b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reft_model.eval()\n",
    "reft_model.training = False\n",
    "ret = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e89e2e79-eb74-421f-b7a1-64063891db68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['comp.transformer.h.15.attn.c_attn.output.unit.pos.nunit.1#0'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reft_model.interventions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfe2aead-d833-4b7d-a550-a576cc6f00cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                     | 0/16 [00:00<?, ?it/s]/nlp/scr/peterwz/miniconda3/envs/peterwz-comp/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 16/16 [00:44<00:00,  2.75s/it]\n",
      "/nlp/scr/peterwz/miniconda3/envs/peterwz-comp/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07239165349199084 0.20904985270364193\n",
      "haha 287644\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████▋| 560/562 [00:33<00:00, 16.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.4457, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in layers:\n",
    "    key = 'comp.transformer.h.15.attn.c_attn.output.unit.pos.nunit.1#0'\n",
    "    reft_model.interventions[key][0].mag = 0.0 * torch.ones(1).to(device).to(torch.bfloat16)\n",
    "\n",
    "tox_mean, tox_std = generate_toxicity(layers)\n",
    "ppl = calculate_perplexity(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0642761-0039-4b11-915b-818f82f18eb3",
   "metadata": {},
   "source": [
    "#### Check the \"toxic\" intervention\n",
    "Let's check the learned \"toxic\" intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5d501ce-81c7-496a-914c-9cc710bf7fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 16/16 [00:43<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2356256846955489 0.36062346937609063\n",
      "haha 287644\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████▋| 560/562 [00:34<00:00, 16.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.3466, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in layers:\n",
    "    key = 'comp.transformer.h.15.attn.c_attn.output.unit.pos.nunit.1#0'\n",
    "    reft_model.interventions[key][0].mag = 1.0 * torch.ones(1).to(device).to(torch.bfloat16)\n",
    "\n",
    "tox_mean, tox_std = generate_toxicity(layers)\n",
    "ppl = calculate_perplexity(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b07961-6583-489a-bf45-1b02675a81ec",
   "metadata": {},
   "source": [
    "#### Check the \"Untoxicfied\" GPT-2\n",
    "Let's reverse that intervention and see the resulted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd129346-4172-4427-a2dc-d0063bd01d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 16/16 [00:44<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02782047360364004 0.11401897784032253\n",
      "haha 287644\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████▋| 560/562 [00:34<00:00, 16.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.3431, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in layers:\n",
    "    key = 'comp.transformer.h.15.attn.c_attn.output.unit.pos.nunit.1#0'\n",
    "    reft_model.interventions[key][0].mag = -1.0 * torch.ones(1).to(device).to(torch.bfloat16)\n",
    "\n",
    "tox_mean, tox_std = generate_toxicity(layers)\n",
    "ppl = calculate_perplexity(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa71cae8-ba8a-4b58-b21d-a0e82d7cc462",
   "metadata": {},
   "source": [
    "We can see that \n",
    "\n",
    "- We can implement LoRA with the Pyvene/Pyreft library\n",
    "- We reproduced the positive/negative (unlearning) experiment in the LoRA operator paper\n",
    "- However, LoRA's perplexity increased significantly after fine-tuning, on both the positive direction and the negative direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7aae00-a7e2-4421-bfec-21021c1925f6",
   "metadata": {},
   "source": [
    "### ReFT on all positions\n",
    "\n",
    "We have tried LoRA on all positions. What about the performance of ReFT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bc80fe4-ff40-4051-b017-68fcd2862635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubloreftIntervention(LoreftIntervention):\n",
    "    \"\"\"\n",
    "    This is a LoReFT that supports subspace interventions with coefficients!\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        subspace_coeff = None\n",
    "        # Subspace coefficients are the coefficients applied to each subspace.\n",
    "        # When `subspace_coeff` is a ones tensor, this intervention is the same as a loreft intervention with subspaces\n",
    "        # When `subspace_coeff` is a negative-ones tensor, this intervention is the negation of the loreft intervention\n",
    "        # There is no intervention when `subspace_coeff` is zero.\n",
    "        if \"subspace_coeff\" in kwargs:\n",
    "            subspace_coeff = kwargs[\"subspace_coeff\"].copy()\n",
    "            del kwargs[\"subspace_coeff\"]\n",
    "        self.subspace_coeff = torch.tensor(subspace_coeff).to(device) if subspace_coeff is not None else torch.ones(kwargs[\"low_rank_dimension\"]).to(device)\n",
    "        print(kwargs)\n",
    "        super().__init__(**kwargs)\n",
    "        self.register_buffer('cumulative_flops', torch.tensor(0))\n",
    "            \n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None, **kwargs\n",
    "    ):\n",
    "        assert subspaces is not None\n",
    "        output = []\n",
    "        total_flops = 0\n",
    "\n",
    "        rotated_base = self.rotate_layer(base)\n",
    "        # print(base.shape)\n",
    "        total_flops += 2 * base.shape[0] * base.shape[1] * base.shape[2] * self.rotate_layer.weight.shape[1]\n",
    "\n",
    "        diff = self.act_fn(self.learned_source(base)) - rotated_base\n",
    "        total_flops += 2 * base.shape[0] * base.shape[1] * base.shape[2] * self.learned_source.weight.shape[0]  # Matmul\n",
    "        total_flops += base.shape[0] * base.shape[1] * self.learned_source.weight.shape[0]  # Bias addition\n",
    "\n",
    "        \n",
    "        # print(base.shape[0], base.shape[1], base.shape[2], self.learned_source.weight.shape[0])\n",
    "        batched_subspace = []\n",
    "        batched_weights = []\n",
    "        \n",
    "        for example_i in range(len(diff)):\n",
    "            # Apply potential negations/coefficients here\n",
    "            # print(diff.shape, base.shape)\n",
    "            # print(subspaces)\n",
    "            LHS = (diff[example_i, :, subspaces[example_i]]) * self.subspace_coeff[subspaces[example_i]]\n",
    "            RHS = self.rotate_layer.weight[..., subspaces[example_i]] \n",
    "            RHS = RHS.T\n",
    "            batched_subspace += [LHS]\n",
    "            batched_weights += [RHS]\n",
    "            # FLOPs for LHS multiplication (assuming element-wise)\n",
    "            flops_elementwise = LHS.numel()\n",
    "            # print(flops_elementwise)\n",
    "            total_flops += flops_elementwise\n",
    "\n",
    "\n",
    "        batched_subspace = torch.stack(batched_subspace, dim=0)\n",
    "        batched_weights = torch.stack(batched_weights, dim=0)\n",
    "        \n",
    "        output = base + torch.bmm(batched_subspace, batched_weights)\n",
    "        flops_batched_mm = 2 * batched_subspace.shape[0] * batched_subspace.shape[1] * batched_subspace.shape[2] * batched_weights.shape[2]\n",
    "        total_flops += flops_batched_mm\n",
    "        # print(batched_subspace.shape, batched_weights.shape)\n",
    "        total_flops += output.numel()\n",
    "\n",
    "        self.cumulative_flops += total_flops\n",
    "\n",
    "\n",
    "        return self.dropout(output.to(base.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31fe7f73-0153-4fab-a4d0-a1c67228637e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embed_dim': 1280, 'low_rank_dimension': 4, 'dtype': torch.bfloat16, 'init_orth': True}\n",
      "cuda:0\n",
      "trainable intervention params: 10,244 || trainable model params: 0\n",
      "model params: 774,030,080 || trainable%: 0.0013234627780873839\n"
     ]
    }
   ],
   "source": [
    "layers = [15]\n",
    "\n",
    "# get reft model\n",
    "reft_config = ReftConfig(representations=\n",
    "    [{\n",
    "            \"layer\": l, \"component\": \"block_output\",\n",
    "            \"low_rank_dimension\": low_rank,\n",
    "            \"intervention\": SubloreftIntervention(\n",
    "                embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                dtype=torch.bfloat16, \n",
    "                init_orth=True,\n",
    "            )\n",
    "        } for l in layers]\n",
    ")\n",
    "reft_model = get_reft_model(model, reft_config, set_device=False)\n",
    "reft_model.train()\n",
    "reft_model.training = True\n",
    "reft_model.set_device(device)\n",
    "print(reft_model.get_device())\n",
    "reft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ceb1bacb-f35c-4cbc-800b-423fdd0c02af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class FlopsLoggingCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.writer = SummaryWriter()  # Initialize SummaryWriter\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.is_local_process_zero and self.writer is not None:\n",
    "            if \"loss\" in logs:\n",
    "                # Calculate total_flops from your model\n",
    "                total_flops = 0\n",
    "                # print(kwargs['model'].interventions)\n",
    "                for k, v in kwargs['model'].interventions.items():\n",
    "                    if isinstance(v[0], SubloreftIntervention):\n",
    "                        total_flops = v[0].cumulative_flops\n",
    "\n",
    "                # Log FLOPs to TensorBoard\n",
    "                self.writer.add_scalar('FLOPs', total_flops, global_step=state.global_step)\n",
    "                print(f\"Global Step: {state.global_step}, Calculated FLOPs: {total_flops}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e589b10-cc93-4d37-8534-a79ebefde803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 02:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.877600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.785600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.781100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.758800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.782300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.742300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>3.716600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.720500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 300, Calculated FLOPs: 1402910640\n",
      "Directory './results_reft/checkpoint-500/intervenable_model' already exists.\n",
      "Global Step: 600, Calculated FLOPs: 2820160864\n",
      "Global Step: 900, Calculated FLOPs: 4332730912\n",
      "Directory './results_reft/checkpoint-1000/intervenable_model' already exists.\n",
      "Global Step: 1200, Calculated FLOPs: 5836658800\n",
      "Global Step: 1500, Calculated FLOPs: 7276186592\n",
      "Directory './results_reft/checkpoint-1500/intervenable_model' already exists.\n",
      "Global Step: 1800, Calculated FLOPs: 8740424560\n",
      "Directory './results_reft/checkpoint-2000/intervenable_model' already exists.\n",
      "Global Step: 2100, Calculated FLOPs: 10184881584\n",
      "Global Step: 2400, Calculated FLOPs: 11629722704\n",
      "Directory './results_reft/checkpoint-2500/intervenable_model' already exists.\n",
      "Global Step: 2700, Calculated FLOPs: 13078020688\n",
      "Global Step: 3000, Calculated FLOPs: 14515243904\n",
      "Directory './results_reft/checkpoint-3000/intervenable_model' already exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=3.7933878173828126, metrics={'train_runtime': 179.9408, 'train_samples_per_second': 33.344, 'train_steps_per_second': 16.672, 'total_flos': 0.0, 'train_loss': 3.7933878173828126, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "training_args = transformers.TrainingArguments(\n",
    "    num_train_epochs=3.0, output_dir=\"./results_reft\", learning_rate=1e-3, report_to=[\"wandb\"],\n",
    "    per_device_train_batch_size=train_batch_size, logging_steps=300, bf16=True,\n",
    "    warmup_ratio=0.06,\n",
    ")\n",
    "trainer = ReftTrainerForCausalLM(\n",
    "    model=reft_model, tokenizer=tokenizer, args=training_args, \n",
    "    train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator,\n",
    "    callbacks=[FlopsLoggingCallback()]\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d88fcc8-aaf2-448b-b15c-382751b8770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reft_model.eval()\n",
    "reft_model.training = False\n",
    "ret = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be2cce64-67f8-4a05-bd77-8c713fc84d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['layer.15.comp.block_output.unit.pos.nunit.1#0'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reft_model.interventions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28964275-e3a0-4e1f-9425-9b7db338a049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                     | 0/16 [00:00<?, ?it/s]/nlp/scr/peterwz/miniconda3/envs/peterwz-comp/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 16/16 [01:28<00:00,  5.50s/it]\n",
      "/nlp/scr/peterwz/miniconda3/envs/peterwz-comp/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07239165349199084 0.20904985270364193\n",
      "haha 287644\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████▋| 560/562 [00:33<00:00, 16.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.4457, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in layers:\n",
    "    key = 'layer.' + str(i) + '.comp.block_output.unit.pos.nunit.1#0'\n",
    "    reft_model.interventions[key][0].subspace_coeff = 0.0 * torch.ones(low_rank).to(device)\n",
    "\n",
    "tox_mean, tox_std = generate_toxicity(layers)\n",
    "ppl = calculate_perplexity(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5829c368-234c-49e4-ba7f-468c135acf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 16/16 [01:27<00:00,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2254427645853525 0.34547832646255394\n",
      "haha 287644\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████▋| 560/562 [00:33<00:00, 16.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.6782, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in layers:\n",
    "    key = 'layer.' + str(i) + '.comp.block_output.unit.pos.nunit.1#0'\n",
    "    reft_model.interventions[key][0].subspace_coeff = 1.0 * torch.ones(low_rank).to(device)\n",
    "\n",
    "tox_mean, tox_std = generate_toxicity(layers)\n",
    "ppl = calculate_perplexity(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67dc5855-9e0a-49ec-a14a-dede33e4ea27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 16/16 [01:27<00:00,  5.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0213432774300486 0.09923869876326916\n",
      "haha 287644\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████▋| 560/562 [00:33<00:00, 16.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17.2887, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in layers:\n",
    "    key = 'layer.' + str(i) + '.comp.block_output.unit.pos.nunit.1#0'\n",
    "    reft_model.interventions[key][0].subspace_coeff = -1.0 * torch.ones(low_rank).to(device)\n",
    "\n",
    "tox_mean, tox_std = generate_toxicity(layers)\n",
    "ppl = calculate_perplexity(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d328329d-5ac8-40d3-98db-e005444cfe1b",
   "metadata": {},
   "source": [
    "We can see that on the same task, ReFT intervening on all positions\n",
    "\n",
    "- Took less training flops than LoRA (74%)\n",
    "- Achieved similar performance on both the positive direction and the negative direction (toxicity)\n",
    "- Preserved better generation fluency than LoRA (perplexity).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdee8487-fb61-4416-ace2-773df03aeee8",
   "metadata": {},
   "source": [
    "### ReFT on only the first position\n",
    "What about we do ReFT, but only on the first position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71d1182c-a256-4fe1-9935-125e78e5f06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embed_dim': 1280, 'low_rank_dimension': 4, 'dtype': torch.bfloat16, 'init_orth': True}\n",
      "cuda:0\n",
      "trainable intervention params: 10,244 || trainable model params: 0\n",
      "model params: 774,030,080 || trainable%: 0.0013234627780873839\n"
     ]
    }
   ],
   "source": [
    "layers = [15]\n",
    "\n",
    "# get reft model\n",
    "reft_config = ReftConfig(representations=\n",
    "    [{\n",
    "            \"layer\": l, \"component\": \"block_output\",\n",
    "            \"low_rank_dimension\": low_rank,\n",
    "            \"intervention\": SubloreftIntervention(\n",
    "                embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                dtype=torch.bfloat16, \n",
    "                init_orth=True,\n",
    "            )\n",
    "        } for l in layers]\n",
    ")\n",
    "reft_model = get_reft_model(model, reft_config, set_device=False)\n",
    "reft_model.train()\n",
    "reft_model.training = True\n",
    "reft_model.set_device(device)\n",
    "print(reft_model.get_device())\n",
    "reft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a341730-9e59-49fe-b045-027788d7b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = make_all_positions_unsupervised_data_module(tokenizer, model, raw_dataset, num_interventions=len(layers), nonstop=False,\n",
    "                                                 intervene_on_all=False)\n",
    "train_dataset = ret[\"train_dataset\"]\n",
    "data_collator = ret[\"data_collator\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "848db933-98b1-4be8-a76b-cb6770a5a7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 03:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.974500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.924700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.910700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.918300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.902800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.927200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.903800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>3.868600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.873400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 300, Calculated FLOPs: 19204800\n",
      "Directory './results_reft/checkpoint-500/intervenable_model' already exists.\n",
      "Global Step: 600, Calculated FLOPs: 38409600\n",
      "Global Step: 900, Calculated FLOPs: 57614400\n",
      "Directory './results_reft/checkpoint-1000/intervenable_model' already exists.\n",
      "Global Step: 1200, Calculated FLOPs: 76819200\n",
      "Global Step: 1500, Calculated FLOPs: 96024000\n",
      "Directory './results_reft/checkpoint-1500/intervenable_model' already exists.\n",
      "Global Step: 1800, Calculated FLOPs: 115228800\n",
      "Directory './results_reft/checkpoint-2000/intervenable_model' already exists.\n",
      "Global Step: 2100, Calculated FLOPs: 134433600\n",
      "Global Step: 2400, Calculated FLOPs: 153638400\n",
      "Directory './results_reft/checkpoint-2500/intervenable_model' already exists.\n",
      "Global Step: 2700, Calculated FLOPs: 172843200\n",
      "Global Step: 3000, Calculated FLOPs: 192048000\n",
      "Directory './results_reft/checkpoint-3000/intervenable_model' already exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=3.922053507486979, metrics={'train_runtime': 182.321, 'train_samples_per_second': 32.909, 'train_steps_per_second': 16.454, 'total_flos': 0.0, 'train_loss': 3.922053507486979, 'epoch': 3.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "training_args = transformers.TrainingArguments(\n",
    "    num_train_epochs=3.0, output_dir=\"./results_reft\", learning_rate=1e-3, report_to=[\"wandb\"],\n",
    "    per_device_train_batch_size=train_batch_size, logging_steps=300, bf16=True,\n",
    "    warmup_ratio=0.06,\n",
    ")\n",
    "trainer = ReftTrainerForCausalLM(\n",
    "    model=reft_model, tokenizer=tokenizer, args=training_args, \n",
    "    train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator,\n",
    "    callbacks=[FlopsLoggingCallback()])\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05468cf0-7d2b-496b-997a-b05bff2d00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reft_model.eval()\n",
    "reft_model.training = False\n",
    "ret = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2b83a12-6168-4b75-b65e-b6dcec712e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                     | 0/16 [00:00<?, ?it/s]/nlp/scr/peterwz/miniconda3/envs/peterwz-comp/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 16/16 [00:43<00:00,  2.73s/it]\n",
      "/nlp/scr/peterwz/miniconda3/envs/peterwz-comp/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07239165349199084 0.20904985270364193\n",
      "haha 287644\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████▋| 560/562 [00:34<00:00, 16.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.4457, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in layers:\n",
    "    key = 'layer.' + str(i) + '.comp.block_output.unit.pos.nunit.1#0'\n",
    "    reft_model.interventions[key][0].subspace_coeff = 0.0 * torch.ones(low_rank).to(device)\n",
    "\n",
    "tox_mean, tox_std = generate_toxicity(layers, intervene_on_all=False)\n",
    "ppl = calculate_perplexity(layers, intervene_on_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c10d431-1644-4e67-9e3e-17a2152efb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 16/16 [00:42<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27369099783618367 0.37188162905204397\n",
      "haha 287644\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████▋| 560/562 [00:34<00:00, 16.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17.0535, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in layers:\n",
    "    key = 'layer.' + str(i) + '.comp.block_output.unit.pos.nunit.1#0'\n",
    "    reft_model.interventions[key][0].subspace_coeff = 1.0 * torch.ones(low_rank).to(device)\n",
    "\n",
    "tox_mean, tox_std = generate_toxicity(layers, intervene_on_all=False)\n",
    "ppl = calculate_perplexity(layers, intervene_on_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6639e1b1-687d-4079-ae1f-037c4c1478bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 16/16 [00:44<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019930323498101643 0.09382162088810096\n",
      "haha 287644\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████▋| 560/562 [00:34<00:00, 16.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.8638, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in layers:\n",
    "    key = 'layer.' + str(i) + '.comp.block_output.unit.pos.nunit.1#0'\n",
    "    reft_model.interventions[key][0].subspace_coeff = -1.0 * torch.ones(low_rank).to(device)\n",
    "\n",
    "tox_mean, tox_std = generate_toxicity(layers, intervene_on_all=False)\n",
    "ppl = calculate_perplexity(layers, intervene_on_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cfacb8-def8-4374-b63f-de10dda4de0a",
   "metadata": {},
   "source": [
    "We can see that ReFT on a single position (first position of the prompt) took much less flops (1%) than both LoRA and ReFT on all positions. This is because, here the sequence length fed into the intervention is 1, whereas before the sequence length was 512 (or longer).\n",
    "\n",
    "However, we can see that ReFT on a single position achieved much better positive (and negative) performance on the toxicity unlearning task, while maintaining a lower level of perplexity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a52c7fa-4d7a-49c9-bdae-d98f87ff04fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
