{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f298a4d8-e8d9-4ee8-828c-bfcb2ee032b9",
   "metadata": {},
   "source": [
    "## Domain Transfer Task\n",
    "\n",
    "One key priority of composition is the addition and the subtraction of the learned modules can work together. Here we would like to see it in the same setting as the [LoRA operators paper](https://arxiv.org/pdf/2306.14870). \n",
    "\n",
    "In this task, we will use t5-base or t5-small model to see if ReFT interventions can transfer across domains. We would learn with 2 datasets (amazon-polarity and yelp-polarity, which both contain reviews). Each dataset learns 2 models: sentiment classification, and language modeling (which concatenates and blocks review texts into chunks of size 128, and use the prior block to predict the next block).\n",
    "\n",
    "We then would compose these blocks together, using the addition and the subtraction operator we defined before. We can compose by creating a new model with\n",
    "\n",
    "- amazon_classifier + lambda * (yelp_lm - amazon_lm)\n",
    "- yelp_classifier + lambda * (amazon_lm - yelp_lm)\n",
    "\n",
    "We will see if the first composed model has a better performance on the yelp dataset than just amazon_classifier, and the second composed model has a better performance on the amazon dataset than just yelp_classifier. This would show that the addition of the language modeling diffs are parallel to the addition of the classification diffs, strengthening our argument that lm interventions (especially ReFT) are composable.\n",
    "\n",
    "Note that if you learn orthogonal interventions with 4 tasks at 4 subspaces, that would not work. yelp_lm <-> amazon_lm are tasks that are very close to each other. Forcing them to be orthogonal to each other breaks the learning flow and will make the second learnable module close to 0. So we choose to use NoDiReFT in the notebook. However, we leave the LoReFT sessions there for your reference.\n",
    "\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcb32e23-a5f0-4d96-af54-08675944f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import chain\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import nltk  # Here to have a nice missing dependency error message early on\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "import math\n",
    "\n",
    "from promptsource.templates import DatasetTemplates\n",
    "\n",
    "import transformers\n",
    "from filelock import FileLock\n",
    "from transformers import (\n",
    "    # AdapterConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    "    HfArgumentParser,\n",
    "    MBart50Tokenizer,\n",
    "    MBart50TokenizerFast,\n",
    "    MBartTokenizer,\n",
    "    MBartTokenizerFast,\n",
    "    # MultiLingAdapterArguments,\n",
    "    # Seq2SeqAdapterTrainer,\n",
    "    Trainer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, is_offline_mode\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2628dc12-cd5e-4fec-a569-88ed04839c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    if is_offline_mode():\n",
    "        raise LookupError(\n",
    "            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n",
    "        )\n",
    "    with FileLock(\".lock\") as lock:\n",
    "        nltk.download(\"punkt\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a44a1c2-6d45-4eb7-ab69-b63433bc5c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path=\"t5-small\"\n",
    "seed = 42\n",
    "dropout = 0\n",
    "max_length = 512\n",
    "low_rank = 8\n",
    "set_seed(seed)\n",
    "max_train_examples = 10000\n",
    "train_batch_size = 16\n",
    "fp16 = True\n",
    "testing = True\n",
    "intervention_type = \"nodireft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "758bf2b0-d1dd-432e-839a-2d1747f1611a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/peterwz/miniconda3/envs/peterwz-comp/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    dropout_rate=dropout,\n",
    "    max_length=max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ca94f43-e5ea-45e9-8852-6ee6191d02b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    ")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16 if fp16 else torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eb429ca-dc08-4b3f-9159-3fd1e5641372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32100, 512)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5c9a1-9df3-40df-afdf-bcc8f4351627",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing\n",
    "\n",
    "Below are the dataset preprocessing blocks. Use these blocks to install and preprocess the datasets.\n",
    "\n",
    "In this notebook we would create 4 datasets.\n",
    "\n",
    "- 'amazon_lm_data': language model data from amazon_polarity dataset. It chunked the reviews into size=128 blocks, and the lm training objective is to use the prior block to predict the next block.\n",
    "- 'yelp_lm_data': language model data from yelp_polarity dataset.\n",
    "- 'amazon_classify_data': classification data from amazon_polarity dataset. Classes are \"positive\" and \"negative\".\n",
    "- 'yelp_classify_data': classification data from yelp_polarity dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9db036f-7979-4643-97f1-ea6b115c6c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets = load_dataset(\n",
    "#     \"yelp_polarity\",\n",
    "#     None, # dataset_config_name\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bebe09-76d8-4aaf-aee0-61b88acfb7c4",
   "metadata": {},
   "source": [
    "Select appropriate column names for the appropriate datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c84510e-f62c-4b26-b762-9923e311b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names = raw_datasets[\"train\"].column_names\n",
    "# print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23db0644-ab81-47e8-ba61-2f5e58ec69de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_column, summary_column = column_names[2], column_names[0]\n",
    "# text_column, summary_column = column_names[0], column_names[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "251e3521-fb08-4003-a8f2-2943d9a398bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_column, summary_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b852ed5b-f2e6-46ae-a98a-bc0c78fc1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5190c1-4495-4003-8883-1a9310de4fd0",
   "metadata": {},
   "source": [
    "Uncomment the below blocks if you are creating 'classify' datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e4400f2-e313-49db-af90-b05f5ea7c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix = \"\"\n",
    "# def preprocess_function(examples):\n",
    "#     # remove pairs where at least one record is None\n",
    "\n",
    "#     inputs, targets = [], []\n",
    "#     for i in range(len(examples[text_column])):\n",
    "#         inputs.append(examples[text_column][i])\n",
    "#         target = str(examples[summary_column][i])\n",
    "#         if target == \"0\":\n",
    "#             targets.append(\"negative\")\n",
    "#         elif target == \"1\":\n",
    "#             targets.append(\"positive\")\n",
    "#         else:\n",
    "#             print(target)\n",
    "\n",
    "#     inputs = [prefix + inp for inp in inputs]\n",
    "#     # print(inputs)\n",
    "#     model_inputs = tokenizer(inputs, max_length=max_length, padding=padding, truncation=True)\n",
    "\n",
    "#     # Setup the tokenizer for targets\n",
    "#     with tokenizer.as_target_tokenizer():\n",
    "#         labels = tokenizer(targets, max_length=max_length, padding=padding, truncation=True)\n",
    "\n",
    "#     # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "#     # padding in the loss.\n",
    "#     if padding == \"max_length\":\n",
    "#         labels[\"input_ids\"] = [\n",
    "#             [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "#         ]\n",
    "\n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#     return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1c1475d-a1e0-4d9f-bf92-2b091ae6ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = raw_datasets[\"train\"]\n",
    "# train_dataset = train_dataset.shuffle(seed=seed)\n",
    "# train_dataset = train_dataset.select(range(2 * max_train_examples, 3 * max_train_examples))\n",
    "# train_dataset = train_dataset.map(\n",
    "#     preprocess_function,\n",
    "#     batched=True,\n",
    "#     num_proc=8,\n",
    "#     remove_columns=column_names,\n",
    "#     desc=\"Running tokenizer on train dataset\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66c46837-4ac3-45dd-9290-3e39476e04de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f319cde6-4330-46d2-88ba-342218d227fe",
   "metadata": {},
   "source": [
    "Uncomment the below blocks if you are creating 'lm' datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82a79724-3c88-43aa-97d7-f16ab7277656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_function(examples):\n",
    "#     output = tokenizer(examples[text_column])\n",
    "#     return output\n",
    "\n",
    "# tokenized_datasets = raw_datasets.map(\n",
    "#     tokenize_function,\n",
    "#     batched=True,\n",
    "#     num_proc=8,\n",
    "#     remove_columns=column_names,\n",
    "# )\n",
    "# block_size = max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e930f0b-df6f-4e5c-a509-f8fbbf6a279d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "# def group_texts(examples):\n",
    "#     # Concatenate all texts.\n",
    "#     concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "#     total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "#     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "#     # customize this part to your needs.\n",
    "#     if total_length >= block_size:\n",
    "#         total_length = (total_length // (block_size)) * block_size\n",
    "#     # Split by chunks of max_len.\n",
    "#     result = {\n",
    "#         \"input_ids\": [concatenated_examples[\"input_ids\"][i : i + block_size] for i in range(0, total_length-block_size, block_size)],\n",
    "#         \"labels\": [concatenated_examples[\"input_ids\"][i : i + block_size] for i in range(block_size, total_length, block_size)],\n",
    "#         \"attention_mask\": [concatenated_examples[\"attention_mask\"][i : i + block_size] for i in range(0, total_length-block_size, block_size)],\n",
    "#         # for k, t in concatenated_examples.items()\n",
    "#     }\n",
    "#     # result[\"input_ids\"] = result[\"temp\"].copy()\n",
    "#     return result\n",
    "\n",
    "# lm_datasets = tokenized_datasets.map(\n",
    "#     group_texts,\n",
    "#     batched=True,\n",
    "#     desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94385fb0-015c-4150-bbfb-7e3c997ab0f1",
   "metadata": {},
   "source": [
    "Select appropriate parts from the data to create train and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "867beb2e-1570-4d07-8a3f-dd902049e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = lm_datasets[\"train\"]\n",
    "# train_dataset = train_dataset.shuffle(seed=seed)\n",
    "# # train_dataset = train_dataset.select(range(2 * max_train_examples, 3 * max_train_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c727e8d-11bf-4cb9-8a45-7a50c7fa13e6",
   "metadata": {},
   "source": [
    "Save the data to where you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8f0095f-6ecc-48de-9624-6aa8b9ef3a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.save_to_disk(\"yelp_lm_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad59c2-949f-492e-b107-6b7913d89553",
   "metadata": {},
   "source": [
    "Now if you want to use any data, just load them from where you saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0a9cfdb-f29a-4725-aed6-a580c2cbfc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = datasets.load_from_disk(\"amazon_classify_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b153052-42bb-449c-850e-0df782479d6d",
   "metadata": {},
   "source": [
    "### Reft Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb859253-9cbe-4a6f-b4da-40145ff12f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from pyreft import (\n",
    "    TaskType,\n",
    "    get_reft_model,\n",
    "    ReftConfig,\n",
    "    ReftTrainer,\n",
    "    ReftTrainerForCausalLM, \n",
    "    ReftDataCollator,\n",
    "    ReftRawDataset,\n",
    "    LoreftIntervention,\n",
    "    NodireftIntervention,\n",
    "    DireftIntervention,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "# Let's create a subspace with 8 dims\n",
    "FULL_SUBSPACE = list(range(low_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "723d896f-4b0f-4d2c-83ae-e7d6fad76967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubloreftIntervention(LoreftIntervention):\n",
    "    \"\"\"\n",
    "    This is a LoReFT that supports subspace interventions with coefficients!\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        subspace_coeff = None\n",
    "        # Subspace coefficients are the coefficients applied to each subspace.\n",
    "        # When `subspace_coeff` is a ones tensor, this intervention is the same as a loreft intervention with subspaces\n",
    "        # When `subspace_coeff` is a negative-ones tensor, this intervention is the negation of the loreft intervention\n",
    "        # There is no intervention when `subspace_coeff` is zero.\n",
    "        if \"subspace_coeff\" in kwargs:\n",
    "            subspace_coeff = kwargs[\"subspace_coeff\"].copy()\n",
    "            del kwargs[\"subspace_coeff\"]\n",
    "        subspace_coeff = torch.tensor(subspace_coeff) if subspace_coeff is not None else torch.ones(kwargs[\"low_rank_dimension\"])\n",
    "        self.subspace_coeff = subspace_coeff.to(device)\n",
    "        super().__init__(**kwargs)\n",
    "        print(\"loreft\", kwargs)\n",
    "        if not fp16:\n",
    "            self.learned_source = self.learned_source.to(torch.float32) \n",
    "            \n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None, **kwargs,\n",
    "    ):\n",
    "        assert subspaces is not None\n",
    "        # print(\"mag:\", self.subspace_coeff)\n",
    "        original_output = kwargs[\"_pyvene_model_input_args\"][0]\n",
    "\n",
    "        output = []\n",
    "\n",
    "        rotated_base = self.rotate_layer(original_output)\n",
    "        diff = self.act_fn(self.learned_source(original_output)) - rotated_base\n",
    "        \n",
    "        batched_subspace = []\n",
    "        batched_weights = []\n",
    "        \n",
    "        if len(diff) > 1:\n",
    "            subspaces = [subspaces[0]] * len(diff)\n",
    "        elif len(diff) != len(subspaces):\n",
    "            print(f\"Warning! lengths do not match {len(diff)} {len(subspaces)}\")\n",
    "\n",
    "        # Expand subspaces to match dimensions\n",
    "        subspaces = torch.tensor(subspaces).to(base.device)\n",
    "        subspaces_expanded = subspaces.unsqueeze(1).expand(diff.size(0), diff.size(1), -1)\n",
    "        \n",
    "        LHS = torch.gather(diff, 2, subspaces_expanded) * self.subspace_coeff[subspaces_expanded]\n",
    "        \n",
    "        # Transpose and gather the corresponding weights for each subspace\n",
    "        RHS = self.rotate_layer.weight[..., subspaces].permute(1, 2, 0)\n",
    "        output = base + torch.bmm(LHS, RHS)\n",
    "        \n",
    "        return self.dropout(output.to(base.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1124ccf-3bb4-419a-bc5b-a07956ed7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CoeffloreftIntervention(LoreftIntervention):\n",
    "    \"\"\"\n",
    "    This is a LoReFT that supports subspace interventions with coefficients!\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        subspace_coeff = None\n",
    "        # Subspace coefficients are the coefficients applied to each subspace.\n",
    "        # When `subspace_coeff` is a ones tensor, this intervention is the same as a loreft intervention with subspaces\n",
    "        # When `subspace_coeff` is a negative-ones tensor, this intervention is the negation of the loreft intervention\n",
    "        # There is no intervention when `subspace_coeff` is zero.\n",
    "        if \"subspace_coeff\" in kwargs:\n",
    "            subspace_coeff = kwargs[\"subspace_coeff\"].copy()\n",
    "            del kwargs[\"subspace_coeff\"]\n",
    "        self.subspace_coeff = torch.tensor(subspace_coeff) if subspace_coeff is not None else torch.ones(1)\n",
    "        self.subspace_coeff = self.subspace_coeff.to(device)\n",
    "        super().__init__(**kwargs)\n",
    "        print(\"loreft\", kwargs)\n",
    "        if not fp16:\n",
    "            self.learned_source = self.learned_source.to(torch.float32)        \n",
    "            \n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None, **kwargs,\n",
    "    ):\n",
    "        # print(base.shape, original_output.shape, torch.equal(base, original_output))\n",
    "        # print(len(kwargs[\"_pyvene_model_input_args\"]), len(kwargs[\"_pyvene_model_output\"]))\n",
    "        # print(\"mag:\", self.subspace_coeff)\n",
    "        # print(kwargs.keys())\n",
    "        original_output = kwargs[\"_pyvene_model_input_args\"][0]\n",
    "\n",
    "        rotated_base = self.rotate_layer(original_output)\n",
    "        val = torch.matmul(\n",
    "            (self.act_fn(self.learned_source(original_output)) - rotated_base), self.rotate_layer.weight.T\n",
    "        )\n",
    "        # print(f\"mag: {self.subspace_coeff}, val: {val.norm()}\")\n",
    "        \n",
    "        output = base + self.subspace_coeff * val\n",
    "        return self.dropout(output.to(base.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3546e-9b93-42ef-b660-31ec5e26f61c",
   "metadata": {},
   "source": [
    "In the NodireftIntervention, we base our interventions all on pyvene's module input. This is to make sure everyone composes on the same base (i.e. parallel interventions). However, you can also try changing the composition base to each intervention's input. That is equivalent to sequentially applying these interventions. You can try that out as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fd2f5a4-5b58-40c8-a739-0d08e6421aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SubNodireftIntervention(NodireftIntervention):\n",
    "    \"\"\"\n",
    "    This is a NodiReft that supports subspace interventions with coefficients!\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        subspace_coeff = None\n",
    "        # Subspace coefficients are the coefficients applied to each subspace.\n",
    "        # When `subspace_coeff` is a ones tensor, this intervention is the same as a loreft intervention with subspaces\n",
    "        # When `subspace_coeff` is a negative-ones tensor, this intervention is the negation of the loreft intervention\n",
    "        # There is no intervention when `subspace_coeff` is zero.\n",
    "        if \"subspace_coeff\" in kwargs:\n",
    "            subspace_coeff = kwargs[\"subspace_coeff\"].copy()\n",
    "            del kwargs[\"subspace_coeff\"]\n",
    "        self.subspace_coeff = torch.tensor(subspace_coeff) if subspace_coeff is not None else torch.ones(1)\n",
    "        self.subspace_coeff = self.subspace_coeff.to(device)\n",
    "        super().__init__(**kwargs)\n",
    "        print(\"nodireft\", kwargs)\n",
    "        if not fp16:\n",
    "            self.learned_source = self.learned_source.to(torch.float32)\n",
    "            self.subspace_coeff = self.subspace_coeff.to(torch.float32)\n",
    "        else:\n",
    "            self.subspace_coeff = self.subspace_coeff.to(torch.bfloat16)\n",
    "            \n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None, **kwargs\n",
    "    ):\n",
    "        original_output = kwargs[\"_pyvene_model_input_args\"][0]\n",
    "\n",
    "        output = base + self.subspace_coeff * torch.matmul(\n",
    "             self.act_fn(self.learned_source(original_output)), self.proj_layer.weight\n",
    "        )\n",
    "        return self.dropout(output.to(base.dtype))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bf6bbe3-b602-4e00-87a7-a3bc5cbd7cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SubDireftIntervention(DireftIntervention):\n",
    "    \"\"\"\n",
    "    This is a DiReft that supports subspace interventions with coefficients!\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        subspace_coeff = None\n",
    "        if \"subspace_coeff\" in kwargs:\n",
    "            subspace_coeff = kwargs[\"subspace_coeff\"].copy()\n",
    "            del kwargs[\"subspace_coeff\"]\n",
    "        self.subspace_coeff = torch.tensor(subspace_coeff) if subspace_coeff is not None else torch.ones(1)\n",
    "        self.subspace_coeff = self.subspace_coeff.to(device)\n",
    "        super().__init__(**kwargs)\n",
    "        print(\"direft\", kwargs)\n",
    "        if not fp16:\n",
    "            self.learned_source = self.learned_source.to(torch.float32)\n",
    "            self.subspace_coeff = self.subspace_coeff.to(torch.float32)\n",
    "        else:\n",
    "            self.subspace_coeff = self.subspace_coeff.to(torch.bfloat16)\n",
    "            \n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None, **kwargs\n",
    "    ):\n",
    "        original_output = kwargs[\"_pyvene_model_input_args\"][0]\n",
    "        cast_base = original_output.to(self.learned_source.weight.dtype)\n",
    "        output = base + self.subspace_coeff * torch.matmul(\n",
    "            (self.act_fn(self.learned_source(cast_base))).to(self.rotate_layer.weight.dtype), self.rotate_layer.weight.T\n",
    "        )\n",
    "        return self.dropout(output.to(base.dtype))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43629d77-e6a5-439a-9ac5-66a8bd6b7e2e",
   "metadata": {},
   "source": [
    "We do interventions on all layers of the encoder and the decoder. Note that t5-base has 12 layers and t5-small has 6 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbf04b9a-a52b-4fdc-b30b-913b5cdd3989",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = list(range(6))\n",
    "num_interventions = 2 * len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f02626f3-9351-4211-986c-a3e7eba2ad6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if intervention_type == \"nodireft\":\n",
    "    reft_config = ReftConfig(representations=\n",
    "        [{\n",
    "                \"layer\": l, \"component\": \"encoder.block.\" + str(l) + \".output\",\n",
    "                \"low_rank_dimension\": low_rank,\n",
    "                \"intervention\": SubNodireftIntervention(\n",
    "                    embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                    dtype=torch.bfloat16 if fp16 else torch.float32, \n",
    "                    add_bias=False,\n",
    "                )\n",
    "            } for l in layers]\n",
    "        + [{\n",
    "                \"layer\": l, \"component\": \"decoder.block.\" + str(l) + \".output\",\n",
    "                \"low_rank_dimension\": low_rank,\n",
    "                \"intervention\": SubNodireftIntervention(\n",
    "                    embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                    dtype=torch.bfloat16 if fp16 else torch.float32, \n",
    "                    add_bias=False,\n",
    "                )\n",
    "            } for l in layers]\n",
    "    )\n",
    "elif intervention_type == \"loreft\":\n",
    "    reft_config = ReftConfig(representations=\n",
    "        [{\n",
    "                \"layer\": l, \"component\": \"encoder.block.\" + str(l) + \".output\",\n",
    "                \"low_rank_dimension\": low_rank,\n",
    "                \"intervention\": SubloreftIntervention(\n",
    "                    embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                    dtype=torch.bfloat16 if fp16 else torch.float32, \n",
    "                    init_orth=True,\n",
    "                )\n",
    "            } for l in layers]\n",
    "        + [{\n",
    "                \"layer\": l, \"component\": \"decoder.block.\" + str(l) + \".output\",\n",
    "                \"low_rank_dimension\": low_rank,\n",
    "                \"intervention\": SubloreftIntervention(\n",
    "                    embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                    dtype=torch.bfloat16 if fp16 else torch.float32, \n",
    "                    init_orth=True,\n",
    "                )\n",
    "            } for l in layers]\n",
    "    )\n",
    "elif intervention_type == \"direft\":\n",
    "    reft_config = ReftConfig(representations=\n",
    "        [{\n",
    "                \"layer\": l, \"component\": \"encoder.block.\" + str(l) + \".output\",\n",
    "                \"low_rank_dimension\": low_rank,\n",
    "                \"intervention\": SubDireftIntervention(\n",
    "                    embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                    dtype=torch.bfloat16 if fp16 else torch.float32, \n",
    "                    init_orth=True,\n",
    "                )\n",
    "            } for l in layers]\n",
    "        + [{\n",
    "                \"layer\": l, \"component\": \"decoder.block.\" + str(l) + \".output\",\n",
    "                \"low_rank_dimension\": low_rank,\n",
    "                \"intervention\": SubDireftIntervention(\n",
    "                    embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                    dtype=torch.bfloat16 if fp16 else torch.float32, \n",
    "                    init_orth=True,\n",
    "                )\n",
    "            } for l in layers]\n",
    "    )\n",
    "elif intervention_type == \"coeffloreft\":\n",
    "    reft_config = ReftConfig(representations=\n",
    "        [{\n",
    "                \"layer\": l, \"component\": \"encoder.block.\" + str(l) + \".output\",\n",
    "                \"low_rank_dimension\": low_rank,\n",
    "                \"intervention\": CoeffloreftIntervention(\n",
    "                    embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                    dtype=torch.bfloat16 if fp16 else torch.float32, \n",
    "                    init_orth=True,\n",
    "                )\n",
    "            } for l in layers]\n",
    "        + [{\n",
    "                \"layer\": l, \"component\": \"decoder.block.\" + str(l) + \".output\",\n",
    "                \"low_rank_dimension\": low_rank,\n",
    "                \"intervention\": CoeffloreftIntervention(\n",
    "                    embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                    dtype=torch.bfloat16 if fp16 else torch.float32, \n",
    "                    init_orth=True,\n",
    "                )\n",
    "            } for l in layers]\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f'No support for intervention {intervention_type}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4958cc6c-81a9-477d-8f27-d11f6a611778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from datasets import Dataset\n",
    "from typing import Dict, Optional, Sequence, Union, List, Any\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AdaptorReftDataCollator(object):\n",
    "    \"\"\"Collate examples for ReFT.\"\"\"\n",
    "    \n",
    "    tokenizer: transformers.AutoTokenizer\n",
    "    data_collator: transformers.DataCollator\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        batch_inputs = self.data_collator(instances)\n",
    "        if \"decoder_input_ids\" in batch_inputs.keys():\n",
    "            del batch_inputs[\"decoder_input_ids\"]\n",
    "        return batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8e5a415-87c6-4c9a-99d7-f2ee4195faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(tokenizer, inputs, num_interventions):\n",
    "    \"\"\"Generator function to yield data lazily.\"\"\"\n",
    "    for i in range(len(inputs)):\n",
    "        _input = inputs[i]\n",
    "        \n",
    "        output_ids = [(l if l != tokenizer.pad_token_id else -100) for l in _input[\"labels\"]]\n",
    "        \n",
    "        yield {\n",
    "            \"input_ids\": _input[\"input_ids\"],\n",
    "            \"labels\": _input[\"labels\"],\n",
    "            \"subspaces\": [FULL_SUBSPACE] * num_interventions,\n",
    "            # \"intervention_locations\": [[0]] * num_interventions\n",
    "        }\n",
    "\n",
    "def make_all_positions_unsupervised_data_module(\n",
    "    tokenizer: transformers.PreTrainedTokenizer, model, inputs, \n",
    "    num_interventions=1, nonstop=False, fp16=False\n",
    "):\n",
    "    \"\"\"Make dataset and collator for unsupervised (or really, semi-supervised) fine-tuning with streaming.\"\"\"\n",
    "    \n",
    "    # Using a generator to lazily load the dataset\n",
    "    train_dataset = Dataset.from_generator(\n",
    "        lambda: data_generator(tokenizer, inputs, num_interventions),\n",
    "        # features={\n",
    "        #     \"input_ids\": [tokenizer.pad_token_id],  # Assuming lists of token ids\n",
    "        #     \"labels\": [tokenizer.pad_token_id],     # Assuming lists of token ids with padding\n",
    "        #     \"subspaces\": [[FULL_SUBSPACE]],         # Subspace feature, adjust as needed\n",
    "        #     # \"intervention_locations\": [[[0]]],      # Intervention locations\n",
    "        # }\n",
    "    )\n",
    "    \n",
    "    data_collator_fn = transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100,\n",
    "        pad_to_multiple_of=8 if fp16 else None,\n",
    "    )\n",
    "    data_collator = AdaptorReftDataCollator(tokenizer=tokenizer, data_collator=data_collator_fn)\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a75488e-8ce5-40c2-a24f-1e2a445dc3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def handle_training(dataset_name, profiling=False):\n",
    "    reft_model = get_reft_model(model, copy.deepcopy(reft_config), set_device=device)\n",
    "    reft_model.set_device(device)\n",
    "    print(reft_model.get_device())\n",
    "    reft_model.print_trainable_parameters()\n",
    "    train_dataset = datasets.load_from_disk(dataset_name)\n",
    "    if testing: train_dataset = train_dataset.select(range(max_train_examples))\n",
    "    train_dataset = make_all_positions_unsupervised_data_module(tokenizer, model, train_dataset, num_interventions=num_interventions, nonstop=False)\n",
    "    train_dataset, data_collator = train_dataset[\"train_dataset\"], train_dataset[\"data_collator\"]\n",
    "    print(len(train_dataset))\n",
    "    # Double checked, we can use ReftTrainerForCausalLM for training Seq2Seq models\n",
    "    reft_model.train()\n",
    "    reft_model.model.train()\n",
    "    reft_model.training = True\n",
    "    \n",
    "    training_args = transformers.TrainingArguments(\n",
    "        num_train_epochs=1.0, output_dir=\"./results_domain\", learning_rate=5e-4, report_to=[],\n",
    "        per_device_train_batch_size=train_batch_size, logging_steps=50, bf16=fp16,\n",
    "        dataloader_num_workers = 2,\n",
    "        dataloader_pin_memory = True,\n",
    "        remove_unused_columns = False,\n",
    "        gradient_accumulation_steps=2,\n",
    "        adam_beta2 = 0.98,\n",
    "        adam_epsilon=1e-6,\n",
    "        # warmup_ratio=0.06,\n",
    "    )\n",
    "    trainer = ReftTrainerForCausalLM(\n",
    "        model=reft_model, tokenizer=tokenizer, args=training_args, \n",
    "        train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n",
    "    \n",
    "    class ProfCallback(TrainerCallback):\n",
    "        def __init__(self, prof):\n",
    "            self.prof = prof\n",
    "    \n",
    "        def on_step_end(self, args, state, control, **kwargs):\n",
    "            self.prof.step()\n",
    "\n",
    "    if profiling:\n",
    "        with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU,\n",
    "                                                torch.profiler.ProfilerActivity.CUDA], \n",
    "                                    schedule=torch.profiler.schedule(skip_first=3, wait=1, warmup=1, active=4, repeat=1),\n",
    "                                    on_trace_ready=torch.profiler.tensorboard_trace_handler('./hf-training-trainer/grad/'),\n",
    "                                    profile_memory=True,\n",
    "                                    with_stack=True,\n",
    "                                    record_shapes=True) as prof:\n",
    "            \n",
    "            trainer.add_callback(ProfCallback(prof=prof))\n",
    "            trainer.train()\n",
    "    else:\n",
    "        trainer.train()\n",
    "    \n",
    "    # prof.export_chrome_trace(\"my_trainer.json\")\n",
    "    \n",
    "    return reft_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5a3767-a67b-4e60-a244-b1ca8c1c1e1e",
   "metadata": {},
   "source": [
    "### Train Amazon LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ab0aad0-a94f-42a8-98b9-632c9fd18e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "trainable intervention params: 98,400 || trainable model params: 0\n",
      "model params: 60,492,288 || trainable%: 0.16266536322778863\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538c3e036f6f4cd1a09c589920b62c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [312/312 00:24, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.103400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.066900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.034600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reft_amazon_lm = handle_training(\"train/amazon_lm_data_new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b89377-b1dc-4578-a67b-fb7b7ea49770",
   "metadata": {},
   "source": [
    "### Train Yelp LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c639c8a-6f07-4d54-a944-7874d67210b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "trainable intervention params: 98,400 || trainable model params: 0\n",
      "model params: 60,492,288 || trainable%: 0.16266536322778863\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbb8ac84e4d4a538c2b5aa7681f4dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [312/312 00:24, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.369200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.939600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.911400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.883000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.881100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.868700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reft_yelp_lm = handle_training(\"train/yelp_lm_data_new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a8f02-c895-40d2-baa6-4f2f30499f47",
   "metadata": {},
   "source": [
    "### Train Yelp Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c222bbc7-a89e-45c2-8bc8-01752c883277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "trainable intervention params: 98,400 || trainable model params: 0\n",
      "model params: 60,492,288 || trainable%: 0.16266536322778863\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae69ba7f3fe40a59f4600a828d3aa6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [312/312 00:34, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.459600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.097900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.105600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reft_yelp_classifier = handle_training(\"train/yelp_classify_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d4a6c6-18db-4086-a8d3-669a3b2aa08a",
   "metadata": {},
   "source": [
    "### Train Amazon Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7c56c62-7e6b-4c01-becf-c8d4daf87381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "trainable intervention params: 98,400 || trainable model params: 0\n",
      "model params: 60,492,288 || trainable%: 0.16266536322778863\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5226bcb13c486bb4cc33302cb5b2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [312/312 00:24, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.433100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.151400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.143200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.151300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.124200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reft_amazon_classifier = handle_training(\"train/amazon_classify_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b061046c-897f-486f-a909-a88904319c4e",
   "metadata": {},
   "source": [
    "### Man - Woman = King - Queen relationship\n",
    "\n",
    "Now let's see if adding the diff of the lms to the classification results would lead to any performance gains!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc34a6c-cd54-485b-8b5f-226574cc97b2",
   "metadata": {},
   "source": [
    "#### Generation with composition\n",
    "\n",
    "During generation on the 'yelp_classify_data' and the 'amazon_classify_data', we restrict decoding to be only on 'positive' and 'negative'. We use beam search with num_beams=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34de2cf2-093c-4ade-94e1-b5d91f404cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_batch_size = 1\n",
    "gen_max_length = 2\n",
    "from tqdm import tqdm\n",
    "def generate_texts(reft_model, prompt, allowed_token_ids, num_interventions=1, intervene_on_all=True):\n",
    "    # instruction = \" \"\n",
    "    \n",
    "    # print(prompt)\n",
    "\n",
    "    for k, v in prompt.items():\n",
    "        if isinstance(v, list):\n",
    "            prompt[k] = torch.tensor(v,dtype=torch.long).to(device)\n",
    "\n",
    "    # prompt = prompt.to(device)\n",
    "    gen_batch_size = prompt[\"input_ids\"].shape[0]\n",
    "    # print(gen_batch_size)\n",
    "    # print(prompt)\n",
    "    \n",
    "    generated_texts = []\n",
    "    subspaces = [[FULL_SUBSPACE] * gen_batch_size] * num_interventions\n",
    "    # print(subspaces)\n",
    "    # print(allowed_token_ids)\n",
    "    _, reft_response = reft_model.generate(\n",
    "        prompt, \n",
    "        unit_locations= None if intervene_on_all else {\"sources->base\": (None, [[[0] ] ] * len(layers)) },\n",
    "        subspaces=subspaces,\n",
    "        intervene_on_prompt=True, \n",
    "        max_new_tokens=2,\n",
    "        min_new_tokens=1,\n",
    "        # do_sample=True, \n",
    "        # no_repeat_ngram_size=2, \n",
    "        # repetition_penalty=1.1, \n",
    "        force_words_ids=allowed_token_ids,\n",
    "        num_beams = 5,\n",
    "        # top_k = 50,\n",
    "        eos_token_id=tokenizer.eos_token_id, early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        remove_invalid_values=True,\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.batch_decode(reft_response, skip_special_tokens=True)\n",
    "    # generated_text = [t[len(instruction):] for t in generated_text]\n",
    "    generated_texts += generated_text\n",
    "\n",
    "    # print(generated_texts[0])\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02e9c585-5c90-4e6f-a7d7-566ed40fb23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(reft_model, num_interventions, train_dataset, data_collator):\n",
    "    acc = torch.tensor(0.0, device=device)\n",
    "    tot = torch.tensor(0.0, device=device)\n",
    "    from torch.utils.data import DataLoader\n",
    "    # Create the DataLoader with the collator\n",
    "    gen_batch_size = 16\n",
    "    dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=gen_batch_size,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    \n",
    "    # Iterate over batches\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        pbar = tqdm(dataloader)\n",
    "        for batch in pbar:\n",
    "            if 'subspaces' in batch:\n",
    "                del batch['subspaces']\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            output_labels = generate_texts(reft_model, batch, force_words_ids, num_interventions)\n",
    "            filtered_labels = [\n",
    "                [token_id for token_id in sequence if token_id != -100]\n",
    "                for sequence in batch[\"labels\"]\n",
    "            ]\n",
    "            true_labels = tokenizer.batch_decode(filtered_labels, skip_special_tokens=True)\n",
    "            correct = torch.tensor(0.0, device=device)\n",
    "            for i in range(len(output_labels)):\n",
    "                if output_labels[i] == true_labels[i]:\n",
    "                    correct += 1\n",
    "            \n",
    "            acc += correct\n",
    "            tot += len(output_labels)\n",
    "            pbar.set_postfix({\"Correct\": acc.item(), \"Accuracy\": (acc / tot).item()})\n",
    "    final_acc = (acc/tot).item()\n",
    "    print(f\"Final Accuracy: {final_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed4ecf-2f29-448f-a0e1-b46044041449",
   "metadata": {},
   "source": [
    "Here we set the composed model's interventions to be individual model's interventions, with `subspace_coeff` as the magnitude of the intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c9a53ad-2ca5-4287-a888-8e54ca3b636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def set_lm(reft_model, reft_composed, layer, l, intervention_id):\n",
    "    \n",
    "    composed_key = \"comp.encoder.block.\" + str(layer) + \".output.unit.pos.nunit.1#\" + str(intervention_id)\n",
    "    original_key = \"comp.encoder.block.\" + str(layer) + \".output.unit.pos.nunit.1#0\"\n",
    "    \n",
    "    if intervention_type == \"loreft\":\n",
    "        reft_composed.interventions[composed_key][0].rotate_layer = copy.deepcopy(reft_model.interventions[original_key][0].rotate_layer)\n",
    "        subspace_coeff = l * torch.ones(low_rank).to(device)\n",
    "    elif intervention_type == \"nodireft\":\n",
    "        reft_composed.interventions[composed_key][0].proj_layer = copy.deepcopy(reft_model.interventions[original_key][0].proj_layer)\n",
    "        subspace_coeff = l * torch.ones(1).to(device)\n",
    "    elif intervention_type == \"direft\" or intervention_type == \"coeffloreft\":\n",
    "        reft_composed.interventions[composed_key][0].rotate_layer = copy.deepcopy(reft_model.interventions[original_key][0].rotate_layer)\n",
    "        subspace_coeff = l * torch.ones(1).to(device)\n",
    "    \n",
    "    \n",
    "    reft_composed.interventions[composed_key][0].learned_source = copy.deepcopy(reft_model.interventions[original_key][0].learned_source)\n",
    "\n",
    "    # subspace_coeff = subspace_coeff.to(torch.bfloat16) if fp16 else subspace_coeff.to(torch.float32)\n",
    "    reft_composed.interventions[composed_key][0].subspace_coeff = subspace_coeff\n",
    "    # print(f\"In set_lm: {composed_key}, {reft_model.interventions[original_key][0].learned_source.weight[0][0]}, {reft_model.interventions[original_key][0].learned_source.bias[0]},{reft_model.interventions[original_key][0].rotate_layer.parametrizations.weight.original[0][0]}\")\n",
    "    # print(f\"In set_lm: {composed_key}\", reft_composed.interventions[composed_key][0].subspace_coeff)\n",
    "    \n",
    "    composed_key = \"comp.decoder.block.\" + str(layer) + \".output.unit.pos.nunit.1#\" + str(intervention_id)\n",
    "    original_key = \"comp.decoder.block.\" + str(layer) + \".output.unit.pos.nunit.1#0\"\n",
    "    \n",
    "    if intervention_type == \"loreft\":\n",
    "        reft_composed.interventions[composed_key][0].rotate_layer = copy.deepcopy(reft_model.interventions[original_key][0].rotate_layer)\n",
    "        subspace_coeff = l * torch.ones(low_rank).to(device)\n",
    "    elif intervention_type == \"nodireft\":\n",
    "        reft_composed.interventions[composed_key][0].proj_layer = copy.deepcopy(reft_model.interventions[original_key][0].proj_layer)\n",
    "        subspace_coeff = l * torch.ones(1).to(device)\n",
    "    elif intervention_type == \"direft\" or intervention_type == \"coeffloreft\":\n",
    "        reft_composed.interventions[composed_key][0].rotate_layer = copy.deepcopy(reft_model.interventions[original_key][0].rotate_layer)\n",
    "        subspace_coeff = l * torch.ones(1).to(device)\n",
    "    \n",
    "    reft_composed.interventions[composed_key][0].learned_source = copy.deepcopy(reft_model.interventions[original_key][0].learned_source)\n",
    "\n",
    "    # subspace_coeff = subspace_coeff.to(torch.bfloat16) if fp16 else subspace_coeff.to(torch.float32)\n",
    "    reft_composed.interventions[composed_key][0].subspace_coeff = subspace_coeff\n",
    "    \n",
    "    # print(f\"In set_lm: {composed_key}\", reft_composed.interventions[composed_key][0].subspace_coeff)\n",
    "    # print(f\"In set_lm: {reft_composed.interventions['comp.encoder.block.10.output.unit.pos.nunit.1#0'][0].subspace_coeff}\")\n",
    "    return reft_composed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65247885-a404-4da7-9684-5de902ee12a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n",
      "nodireft {'embed_dim': 512, 'low_rank_dimension': 8, 'dtype': torch.bfloat16, 'add_bias': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if intervention_type == \"nodireft\":\n",
    "    representations = []\n",
    "    sub_representation = [{\n",
    "                \"layer\": l, \"component\": \"encoder.block.\" + str(l) + \".output\",\n",
    "                \"low_rank_dimension\": low_rank,\n",
    "                \"intervention\": SubNodireftIntervention(\n",
    "                    embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                    dtype=torch.bfloat16 if fp16 else torch.float32, \n",
    "                    add_bias=False,\n",
    "                )\n",
    "            } for l in layers]\n",
    "    for _ in range(3):\n",
    "        representations += copy.deepcopy(sub_representation)\n",
    "    sub_representation = [{\n",
    "                \"layer\": l, \"component\": \"decoder.block.\" + str(l) + \".output\",\n",
    "                \"low_rank_dimension\": low_rank,\n",
    "                \"intervention\": SubNodireftIntervention(\n",
    "                    embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                    dtype=torch.bfloat16 if fp16 else torch.float32, \n",
    "                    add_bias=False,\n",
    "                )\n",
    "            } for l in layers]\n",
    "    for _ in range(3):\n",
    "        representations += copy.deepcopy(sub_representation)\n",
    "    composed_reft_config = ReftConfig(representations=representations)\n",
    "elif intervention_type == \"loreft\":\n",
    "    representations = []\n",
    "    sub_representation = [{\n",
    "                \"layer\": l, \"component\": \"encoder.block.\" + str(l) + \".output\",\n",
    "                \"low_rank_dimension\": low_rank,\n",
    "                \"intervention\": SubloreftIntervention(\n",
    "                    embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                    dtype=torch.bfloat16 if fp16 else torch.float32, \n",
    "                    init_orth=True,\n",
    "                )\n",
    "            } for l in layers]\n",
    "    for _ in range(3):\n",
    "        representations += copy.deepcopy(sub_representation)\n",
    "    sub_representation = [{\n",
    "                \"layer\": l, \"component\": \"decoder.block.\" + str(l) + \".output\",\n",
    "                \"low_rank_dimension\": low_rank,\n",
    "                \"intervention\": SubloreftIntervention(\n",
    "                    embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                    dtype=torch.bfloat16 if fp16 else torch.float32, \n",
    "                    init_orth=True,\n",
    "                )\n",
    "            } for l in layers]\n",
    "    for _ in range(3):\n",
    "        representations += copy.deepcopy(sub_representation)\n",
    "    composed_reft_config = ReftConfig(representations=representations)\n",
    "elif intervention_type == \"direft\":\n",
    "    representations = []\n",
    "    sub_representation = [{\n",
    "                \"layer\": l, \"component\": \"encoder.block.\" + str(l) + \".output\",\n",
    "                \"low_rank_dimension\": low_rank,\n",
    "                \"intervention\": SubDireftIntervention(\n",
    "                    embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                    dtype=torch.bfloat16 if fp16 else torch.float32, \n",
    "                )\n",
    "            } for l in layers]\n",
    "    for _ in range(3):\n",
    "        representations += copy.deepcopy(sub_representation)\n",
    "    sub_representation = [{\n",
    "                \"layer\": l, \"component\": \"decoder.block.\" + str(l) + \".output\",\n",
    "                \"low_rank_dimension\": low_rank,\n",
    "                \"intervention\": SubDireftIntervention(\n",
    "                    embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                    dtype=torch.bfloat16 if fp16 else torch.float32, \n",
    "                )\n",
    "            } for l in layers]\n",
    "    for _ in range(3):\n",
    "        representations += copy.deepcopy(sub_representation)\n",
    "    composed_reft_config = ReftConfig(representations=representations)\n",
    "elif intervention_type == \"coeffloreft\":\n",
    "    representations = []\n",
    "    sub_representation = [{\n",
    "                \"layer\": l, \"component\": \"encoder.block.\" + str(l) + \".output\",\n",
    "                \"low_rank_dimension\": low_rank,\n",
    "                \"intervention\": CoeffloreftIntervention(\n",
    "                    embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                    dtype=torch.bfloat16 if fp16 else torch.float32,\n",
    "                    init_orth=True,\n",
    "                )\n",
    "            } for l in layers]\n",
    "    for _ in range(3):\n",
    "        representations += copy.deepcopy(sub_representation)\n",
    "    sub_representation = [{\n",
    "                \"layer\": l, \"component\": \"decoder.block.\" + str(l) + \".output\",\n",
    "                \"low_rank_dimension\": low_rank,\n",
    "                \"intervention\": CoeffloreftIntervention(\n",
    "                    embed_dim=model.config.hidden_size, low_rank_dimension=low_rank,\n",
    "                    dtype=torch.bfloat16 if fp16 else torch.float32, \n",
    "                    init_orth=True,\n",
    "                )\n",
    "            } for l in layers]\n",
    "    for _ in range(3):\n",
    "        representations += copy.deepcopy(sub_representation)\n",
    "    composed_reft_config = ReftConfig(representations=representations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "478192a8-7ace-4c4f-ae8d-67072e9b3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_eval(reft_model):\n",
    "    reft_model.eval()\n",
    "    reft_model.model.eval()\n",
    "    reft_model.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14ff12af-9862-4981-9171-2e7cd3736dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_batch_size = 64\n",
    "force_flexible = [\"negative\",\"positive\"]\n",
    "force_words_ids = [tokenizer(force_flexible, add_special_tokens=True).input_ids]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c72e68c-b83a-4c5c-a39d-84329edc5bd9",
   "metadata": {},
   "source": [
    "#### Yelp Test\n",
    "\n",
    "In this test we test on the Yelp dataset to see if the amazon_classifier can improve on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d700398-5eed-4fd2-94a2-d685562dcc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "trainable intervention params: 295,200 || trainable model params: 0\n",
      "model params: 60,492,288 || trainable%: 0.48799608968336594\n"
     ]
    }
   ],
   "source": [
    "reft_composed = get_reft_model(model, composed_reft_config, set_device=False)\n",
    "reft_composed.set_device(device)\n",
    "print(reft_composed.get_device())\n",
    "reft_composed.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "744a4bb2-c0d1-4fcd-b0b5-14ac77a0b25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = datasets.load_from_disk(\"validation/yelp_classify_data\") # amazon classifier    \n",
    "if testing: train_dataset = train_dataset.select(range(max_train_examples))\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89e70329-01d0-4365-acd6-f0fc98507439",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set_eval(reft_yelp_lm)\n",
    "set_eval(reft_amazon_lm)\n",
    "set_eval(reft_yelp_classifier)\n",
    "set_eval(reft_amazon_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b0e1310-ff2f-4a4d-814f-7b42f401150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in layers:\n",
    "    # set_lm(reft_yelp_classifier, l, 1.0, 0)\n",
    "    reft_composed = set_lm(reft_yelp_lm, reft_composed, l, 0.3, 0) # 1.0\n",
    "    reft_composed = set_lm(reft_amazon_lm, reft_composed, l, -0.3, 1) # -1.0\n",
    "    # reft_composed = set_lm(reft_yelp_lm, reft_composed, l, -1.0, 1) # -1.0\n",
    "    reft_composed = set_lm(reft_amazon_classifier, reft_composed, l, 1.0, 2) # 1.0\n",
    "set_eval(reft_composed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fdf26a87-35b9-4ca0-b48b-466ea0d16ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[2841, 1], [1465, 1]]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "force_words_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eebbb240-4113-43c7-9bb2-09ecf3371d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008e3e6d99764e9bb3621151afad5115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/625 [00:00<?, ?it/s]/nlp/scr/peterwz/miniconda3/envs/peterwz-comp/lib/python3.11/site-packages/transformers/generation/utils.py:1256: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "100%|| 625/625 [01:26<00:00,  7.23it/s, Correct=9218.0, Accuracy=0.922]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.9218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reft_train_dataset = make_all_positions_unsupervised_data_module(tokenizer, model, train_dataset, num_interventions=3 * num_interventions, nonstop=False)\n",
    "reft_train_dataset, data_collator = reft_train_dataset[\"train_dataset\"], reft_train_dataset[\"data_collator\"]\n",
    "eval(reft_composed, 3 * num_interventions, reft_train_dataset, data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "017f21f8-d747-493a-810a-4cbff8304c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81618e6ebe0e4ebdae526b9ee580b09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 625/625 [01:16<00:00,  8.15it/s, Correct=9205.0, Accuracy=0.92]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.9205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reft_train_dataset = make_all_positions_unsupervised_data_module(tokenizer, model, train_dataset, num_interventions=num_interventions, nonstop=False)\n",
    "reft_train_dataset, data_collator = reft_train_dataset[\"train_dataset\"], reft_train_dataset[\"data_collator\"]\n",
    "eval(reft_amazon_classifier, num_interventions, train_dataset, data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c436cf4f-5fec-4622-a044-79523267dbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 625/625 [01:17<00:00,  8.11it/s, Correct=9294.0, Accuracy=0.929]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.9294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reft_train_dataset = make_all_positions_unsupervised_data_module(tokenizer, model, train_dataset, num_interventions=num_interventions, nonstop=False)\n",
    "reft_train_dataset, data_collator = reft_train_dataset[\"train_dataset\"], reft_train_dataset[\"data_collator\"]\n",
    "eval(reft_yelp_classifier, num_interventions, train_dataset, data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b41ed1-d899-4dc9-af8c-3309db5f21e8",
   "metadata": {},
   "source": [
    "#### Amazon Test\n",
    "\n",
    "In this test we use the amazon dataset to see if yelp classifier can improve on it with the help of the lm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e75845ab-d7a4-44f7-958a-896e22db1fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "trainable intervention params: 295,200 || trainable model params: 0\n",
      "model params: 60,492,288 || trainable%: 0.48799608968336594\n"
     ]
    }
   ],
   "source": [
    "reft_composed = get_reft_model(model, composed_reft_config, set_device=False)\n",
    "reft_composed.set_device(device)\n",
    "print(reft_composed.get_device())\n",
    "reft_composed.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8ee6a71-c6bf-46fb-99bb-1d04cf4f717b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = datasets.load_from_disk(\"validation/amazon_classify_data\") # amazon classifier\n",
    "if testing: train_dataset = train_dataset.select(range(max_train_examples))\n",
    "len(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5005e360-57a6-4889-b0af-61ac81b31e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in layers:\n",
    "    reft_composed = set_lm(reft_amazon_lm, reft_composed, l, 0.3, 0) # 1.0\n",
    "    reft_composed = set_lm(reft_yelp_lm, reft_composed, l, -0.3, 1) # -1.0\n",
    "    reft_composed = set_lm(reft_yelp_classifier, reft_composed, l, 1.0, 2) # 1.0\n",
    "set_eval(reft_composed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c1aecb3-2712-48c4-9283-c90c880d443d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef4900aff34452d82103ff253162d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 625/625 [01:16<00:00,  8.18it/s, Correct=8814.0, Accuracy=0.881]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.8814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reft_train_dataset = make_all_positions_unsupervised_data_module(tokenizer, model, train_dataset, num_interventions=3 * num_interventions, nonstop=False)\n",
    "reft_train_dataset, data_collator = reft_train_dataset[\"train_dataset\"], reft_train_dataset[\"data_collator\"]\n",
    "eval(reft_composed, 3 * num_interventions, reft_train_dataset, data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26448e37-331b-46ed-9650-16dfe7238e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968bcdfbdd144da18bb33ce1728de2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 625/625 [01:06<00:00,  9.34it/s, Correct=8952.0, Accuracy=0.895]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.8952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reft_train_dataset = make_all_positions_unsupervised_data_module(tokenizer, model, train_dataset, num_interventions=num_interventions, nonstop=False)\n",
    "reft_train_dataset, data_collator = reft_train_dataset[\"train_dataset\"], reft_train_dataset[\"data_collator\"]\n",
    "eval(reft_amazon_classifier, num_interventions, train_dataset, data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d04e620-86c6-44e2-85b2-b98f5b259e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 625/625 [01:07<00:00,  9.24it/s, Correct=8818.0, Accuracy=0.882]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.8818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reft_train_dataset = make_all_positions_unsupervised_data_module(tokenizer, model, train_dataset, num_interventions=num_interventions, nonstop=False)\n",
    "reft_train_dataset, data_collator = reft_train_dataset[\"train_dataset\"], reft_train_dataset[\"data_collator\"]\n",
    "eval(reft_yelp_classifier, num_interventions, train_dataset, data_collator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798153cc-a117-40ca-812a-24d48127312b",
   "metadata": {},
   "source": [
    "You can see that for t5-small, on Yelp test dataset, we did observe gains from the lm diffs during the domain transfer. However, we did not observe the similar phenomenon for t5-base, or t5-small on the Amazon test. \n",
    "\n",
    "You can modify the lambda (currently set as 0.3) as much as you want. However, we can now conclude that ReFT's performance on domain transfering (without orthogonal restrictions) is just like IA-3 in the [LoRA operators](https://arxiv.org/pdf/2306.14870) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5d88bd-245b-414f-b7d3-7f14f9070e13",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Why did this happen? ReFT and IA-3 are pretty similar in many ways. They are all related to manipulating a steering vector and add it to the representation. However, LoRA has a larger effect due to its richer representations.\n",
    "\n",
    "However, if we orthogonalize only across amazon_lm and amazon_classify tasks, and yelp_lm and yelp_classify tasks, we might segregate these 2 tasks into more different subspaces, which would cause better composability. If, however, we orthogonalize amazon_lm tasks and yelp_lm tasks, it would not reach a similar effect.\n",
    "\n",
    "The ultimate goal is to develop some architecture of interventions that can (1) segregate different tasks as much as possible (such as via orthogonality), and (2) still get something for similar tasks (such as yelp_lm and amazon_lm), so that they can compose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e370c-9ce5-4b06-81ab-f50bf530c2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
